Jan 13 17:42:24 host systemd[1]: Starting k3s.service - Lightweight Kubernetes...
Jan 13 17:42:24 host k3s[7262]: time="2026-01-13T17:42:24+01:00" level=info msg="Acquiring lock file /var/lib/rancher/k3s/data/.lock"
Jan 13 17:42:24 host k3s[7262]: time="2026-01-13T17:42:24+01:00" level=info msg="Preparing data dir /var/lib/rancher/k3s/data/ffd82c56f7aadb3263c7708547a6027705a0eb9a638534465002ba309cae9990"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="Starting k3s v1.34.3+k3s1 (48ffa7b6)"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="Configuring sqlite3 database connection pooling: maxIdleConns=2, maxOpenConns=0, connMaxLifetime=0s"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="Configuring database table schema and indexes, this may take a moment..."
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="Database tables and indexes are up to date"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="Kine available at unix://kine.sock"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="Datastore connection validated successfully, proceeding with bootstrap data generation"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="generated self-signed CA certificate CN=k3s-client-ca@1768322548: notBefore=2026-01-13 16:42:28.0555517 +0000 UTC notAfter=2036-01-11 16:42:28.0555517 +0000 UTC"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="certificate CN=system:admin,O=system:masters signed by CN=k3s-client-ca@1768322548: notBefore=2026-01-13 16:42:28 +0000 UTC notAfter=2027-01-13 16:42:28 +0000 UTC"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="certificate CN=system:k3s-supervisor,O=system:masters signed by CN=k3s-client-ca@1768322548: notBefore=2026-01-13 16:42:28 +0000 UTC notAfter=2027-01-13 16:42:28 +0000 UTC"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="certificate CN=system:kube-controller-manager signed by CN=k3s-client-ca@1768322548: notBefore=2026-01-13 16:42:28 +0000 UTC notAfter=2027-01-13 16:42:28 +0000 UTC"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="certificate CN=system:kube-scheduler signed by CN=k3s-client-ca@1768322548: notBefore=2026-01-13 16:42:28 +0000 UTC notAfter=2027-01-13 16:42:28 +0000 UTC"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="certificate CN=system:apiserver,O=system:masters signed by CN=k3s-client-ca@1768322548: notBefore=2026-01-13 16:42:28 +0000 UTC notAfter=2027-01-13 16:42:28 +0000 UTC"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="certificate CN=k3s-cloud-controller-manager signed by CN=k3s-client-ca@1768322548: notBefore=2026-01-13 16:42:28 +0000 UTC notAfter=2027-01-13 16:42:28 +0000 UTC"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="generated self-signed CA certificate CN=k3s-server-ca@1768322548: notBefore=2026-01-13 16:42:28.059709545 +0000 UTC notAfter=2036-01-11 16:42:28.059709545 +0000 UTC"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="certificate CN=kube-apiserver signed by CN=k3s-server-ca@1768322548: notBefore=2026-01-13 16:42:28 +0000 UTC notAfter=2027-01-13 16:42:28 +0000 UTC"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="certificate CN=kube-scheduler signed by CN=k3s-server-ca@1768322548: notBefore=2026-01-13 16:42:28 +0000 UTC notAfter=2027-01-13 16:42:28 +0000 UTC"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="certificate CN=kube-controller-manager signed by CN=k3s-server-ca@1768322548: notBefore=2026-01-13 16:42:28 +0000 UTC notAfter=2027-01-13 16:42:28 +0000 UTC"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="generated self-signed CA certificate CN=k3s-request-header-ca@1768322548: notBefore=2026-01-13 16:42:28.061616082 +0000 UTC notAfter=2036-01-11 16:42:28.061616082 +0000 UTC"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="certificate CN=system:auth-proxy signed by CN=k3s-request-header-ca@1768322548: notBefore=2026-01-13 16:42:28 +0000 UTC notAfter=2027-01-13 16:42:28 +0000 UTC"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="generated self-signed CA certificate CN=etcd-server-ca@1768322548: notBefore=2026-01-13 16:42:28.062269155 +0000 UTC notAfter=2036-01-11 16:42:28.062269155 +0000 UTC"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="certificate CN=etcd-client signed by CN=etcd-server-ca@1768322548: notBefore=2026-01-13 16:42:28 +0000 UTC notAfter=2027-01-13 16:42:28 +0000 UTC"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="generated self-signed CA certificate CN=etcd-peer-ca@1768322548: notBefore=2026-01-13 16:42:28.063269679 +0000 UTC notAfter=2036-01-11 16:42:28.063269679 +0000 UTC"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="certificate CN=etcd-peer signed by CN=etcd-peer-ca@1768322548: notBefore=2026-01-13 16:42:28 +0000 UTC notAfter=2027-01-13 16:42:28 +0000 UTC"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="certificate CN=etcd-server signed by CN=etcd-server-ca@1768322548: notBefore=2026-01-13 16:42:28 +0000 UTC notAfter=2027-01-13 16:42:28 +0000 UTC"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="certificate CN=k3s,O=k3s signed by CN=k3s-server-ca@1768322548: notBefore=2026-01-13 16:42:28 +0000 UTC notAfter=2027-01-13 16:42:28 +0000 UTC"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=warning msg="dynamiclistener [::]:6443: no cached certificate available for preload - deferring certificate load until storage initialization or first client request"
Jan 13 17:42:28 host k3s[7262]: time="2026-01-13T17:42:28+01:00" level=info msg="Active TLS secret / (ver=) (count 11): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.0.233:192.168.0.233 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-host:host listener.cattle.io/cn-k3s-server.local:k3s-server.local listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/fingerprint:SHA1=B2D6FF50FE9BE7DC32FEDADD73AA9B4AECF2032D]"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Password verified locally for node host"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="certificate CN=host signed by CN=k3s-server-ca@1768322548: notBefore=2026-01-13 16:42:28 +0000 UTC notAfter=2027-01-13 16:42:29 +0000 UTC"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="certificate CN=system:node:host,O=system:nodes signed by CN=k3s-client-ca@1768322548: notBefore=2026-01-13 16:42:28 +0000 UTC notAfter=2027-01-13 16:42:29 +0000 UTC"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="certificate CN=system:kube-proxy signed by CN=k3s-client-ca@1768322548: notBefore=2026-01-13 16:42:28 +0000 UTC notAfter=2027-01-13 16:42:29 +0000 UTC"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="certificate CN=system:k3s-controller signed by CN=k3s-client-ca@1768322548: notBefore=2026-01-13 16:42:28 +0000 UTC notAfter=2027-01-13 16:42:29 +0000 UTC"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Module overlay was already loaded"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Module nf_conntrack was already loaded"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Module br_netfilter was already loaded"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=warning msg="Failed to load kernel module nft-expr-counter with modprobe"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_close_wait' to 3600"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_max' to 131072"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_established' to 86400"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Connecting to proxy" url="wss://127.0.0.1:6443/v1-k3s/connect"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Creating k3s-cert-monitor event broadcaster"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Saving cluster bootstrap data to datastore"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Handling backend connection request [host]"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Connected to proxy" url="wss://127.0.0.1:6443/v1-k3s/connect"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Remotedialer connected to proxy" url="wss://127.0.0.1:6443/v1-k3s/connect"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Connection to etcd is ready"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="ETCD server is now running"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Polling for API server readiness: GET /readyz failed: the server is currently unable to handle the request"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Running kube-apiserver --advertise-port=6443 --allow-privileged=true --anonymous-auth=false --api-audiences=https://kubernetes.default.svc.cluster.local,k3s --authorization-mode=Node,RBAC --bind-address=127.0.0.1 --cert-dir=/var/lib/rancher/k3s/server/tls/temporary-certs --client-ca-file=/var/lib/rancher/k3s/server/tls/client-ca.crt --egress-selector-config-file=/var/lib/rancher/k3s/server/etc/egress-selector-config.yaml --enable-admission-plugins=NodeRestriction --enable-aggregator-routing=true --enable-bootstrap-token-auth=true --etcd-servers=unix://kine.sock --kubelet-certificate-authority=/var/lib/rancher/k3s/server/tls/server-ca.crt --kubelet-client-certificate=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.crt --kubelet-client-key=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --profiling=false --proxy-client-cert-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt --proxy-client-key-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.key --requestheader-allowed-names=system:auth-proxy --requestheader-client-ca-file=/var/lib/rancher/k3s/server/tls/request-header-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6444 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/rancher/k3s/server/tls/service.key --service-account-signing-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --tls-cert-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-private-key-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Running kube-scheduler --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --bind-address=127.0.0.1 --kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --leader-elect=false --profiling=false --secure-port=10259 --tls-cert-file=/var/lib/rancher/k3s/server/tls/kube-scheduler/kube-scheduler.crt --tls-private-key-file=/var/lib/rancher/k3s/server/tls/kube-scheduler/kube-scheduler.key"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Running kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --bind-address=127.0.0.1 --cluster-cidr=10.42.0.0/16 --cluster-signing-kube-apiserver-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kube-apiserver-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kubelet-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-serving-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-kubelet-serving-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --cluster-signing-legacy-unknown-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-legacy-unknown-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --configure-cloud-routes=false --controllers=*,tokencleaner,-service,-route,-cloud-node-lifecycle --kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --leader-elect=false --profiling=false --root-ca-file=/var/lib/rancher/k3s/server/tls/server-ca.crt --secure-port=10257 --service-account-private-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --tls-cert-file=/var/lib/rancher/k3s/server/tls/kube-controller-manager/kube-controller-manager.crt --tls-private-key-file=/var/lib/rancher/k3s/server/tls/kube-controller-manager/kube-controller-manager.key --use-service-account-credentials=true"
Jan 13 17:42:29 host k3s[7262]: I0113 17:42:29.867925    7262 options.go:305] unable to set WatchListClient feature gate, err: cannot override default for feature "WatchListClient": gates already added to a flag set
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Running cloud-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --bind-address=127.0.0.1 --cloud-config=/var/lib/rancher/k3s/server/etc/cloud-config.yaml --cloud-provider=k3s --cluster-cidr=10.42.0.0/16 --configure-cloud-routes=false --controllers=*,-route,-service --kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --leader-elect=false --leader-elect-resource-name=k3s-cloud-controller-manager --node-status-update-frequency=1m0s --profiling=false"
Jan 13 17:42:29 host k3s[7262]: I0113 17:42:29.868362    7262 options.go:263] external host was not specified, using 192.168.0.233
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Server node token is available at /var/lib/rancher/k3s/server/token"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="To join server node to cluster: k3s server -s https://192.168.0.233:6443 -t ${SERVER_NODE_TOKEN}"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Agent node token is available at /var/lib/rancher/k3s/server/agent-token"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="To join agent node to cluster: k3s agent -s https://192.168.0.233:6443 -t ${AGENT_NODE_TOKEN}"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Wrote kubeconfig /etc/rancher/k3s/k3s.yaml"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Run: k3s kubectl"
Jan 13 17:42:29 host k3s[7262]: I0113 17:42:29.877062    7262 server.go:158] Version: v1.34.3+k3s1
Jan 13 17:42:29 host k3s[7262]: I0113 17:42:29.877131    7262 server.go:160] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Running cri-dockerd --cni-bin-dir=/var/lib/rancher/k3s/data/cni --cni-conf-dir=/var/lib/rancher/k3s/agent/etc/cni/net.d --container-runtime-endpoint=unix:///run/k3s/cri-dockerd/cri-dockerd.sock --cri-dockerd-root-directory=/var/lib/rancher/k3s/agent/cri-dockerd --network-plugin=cni --pod-infra-container-image=rancher/mirrored-pause:3.6 --streaming-bind-addr=127.0.0.1:10010"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="cri-dockerd version v0.3.19-k3s3 (HEAD)"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Starting cri-dockerd v0.3.19-k3s3 (HEAD)"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Start docker client with request timeout 0s"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=error msg="Sending HTTP/1.1 503 response to 127.0.0.1:40768: runtime core not ready"
Jan 13 17:42:29 host k3s[7262]: time="2026-01-13T17:42:29+01:00" level=info msg="Hairpin mode is set to none"
Jan 13 17:42:30 host k3s[7262]: time="2026-01-13T17:42:30+01:00" level=info msg="Loaded network plugin cni"
Jan 13 17:42:30 host k3s[7262]: time="2026-01-13T17:42:30+01:00" level=info msg="Docker cri networking managed by network plugin cni"
Jan 13 17:42:30 host k3s[7262]: time="2026-01-13T17:42:30+01:00" level=info msg="Setting cgroupDriver systemd"
Jan 13 17:42:30 host k3s[7262]: time="2026-01-13T17:42:30+01:00" level=info msg="Docker cri received runtime config network_config:{}"
Jan 13 17:42:30 host k3s[7262]: time="2026-01-13T17:42:30+01:00" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jan 13 17:42:30 host k3s[7262]: time="2026-01-13T17:42:30+01:00" level=info msg="Start cri-dockerd grpc backend"
Jan 13 17:42:30 host k3s[7262]: time="2026-01-13T17:42:30+01:00" level=info msg="Running kube-proxy --cluster-cidr=10.42.0.0/16 --conntrack-max-per-core=0 --conntrack-tcp-timeout-close-wait=0s --conntrack-tcp-timeout-established=0s --healthz-bind-address=127.0.0.1 --hostname-override=host --kubeconfig=/var/lib/rancher/k3s/agent/kubeproxy.kubeconfig --proxy-mode=iptables"
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.165660    7262 shared_informer.go:349] "Waiting for caches to sync" controller="node_authorizer"
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.170285    7262 shared_informer.go:349] "Waiting for caches to sync" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.173896    7262 plugins.go:157] Loaded 14 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,PodTopologyLabels,MutatingAdmissionPolicy,MutatingAdmissionWebhook.
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.173947    7262 plugins.go:160] Loaded 13 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.174623    7262 instance.go:239] Using reconciler: lease
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.181821    7262 handler.go:285] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.182019    7262 genericapiserver.go:784] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.188742    7262 cidrallocator.go:197] starting ServiceCIDR Allocator Controller
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.258438+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.281778+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":1,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.299218    7262 handler.go:285] Adding GroupVersion  v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.299438    7262 apis.go:112] API group "internal.apiserver.k8s.io" is not enabled, skipping.
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.306852+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":2,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.338061+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":3,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.365099+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":4,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.380857    7262 apis.go:112] API group "storagemigration.k8s.io" is not enabled, skipping.
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.390884+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":5,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.421963+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":6,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.449940+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":7,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.473842+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":8,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.491002    7262 handler.go:285] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.491509    7262 genericapiserver.go:784] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.491517    7262 genericapiserver.go:784] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.493260    7262 handler.go:285] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.493353    7262 genericapiserver.go:784] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.496937    7262 handler.go:285] Adding GroupVersion autoscaling v2 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.497428    7262 handler.go:285] Adding GroupVersion autoscaling v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.497440    7262 genericapiserver.go:784] Skipping API autoscaling/v2beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.497443    7262 genericapiserver.go:784] Skipping API autoscaling/v2beta2 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.498612    7262 handler.go:285] Adding GroupVersion batch v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.498646    7262 genericapiserver.go:784] Skipping API batch/v1beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.499841+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":9,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.502006    7262 handler.go:285] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.502051    7262 genericapiserver.go:784] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.502056    7262 genericapiserver.go:784] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.502382    7262 handler.go:285] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.502425    7262 genericapiserver.go:784] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.502430    7262 genericapiserver.go:784] Skipping API coordination.k8s.io/v1alpha2 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.502909    7262 handler.go:285] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.502917    7262 genericapiserver.go:784] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.510268    7262 handler.go:285] Adding GroupVersion networking.k8s.io v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.510282    7262 genericapiserver.go:784] Skipping API networking.k8s.io/v1beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.510966    7262 handler.go:285] Adding GroupVersion node.k8s.io v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.510975    7262 genericapiserver.go:784] Skipping API node.k8s.io/v1beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.510978    7262 genericapiserver.go:784] Skipping API node.k8s.io/v1alpha1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.511957    7262 handler.go:285] Adding GroupVersion policy v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.511971    7262 genericapiserver.go:784] Skipping API policy/v1beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.516802    7262 handler.go:285] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.516881    7262 genericapiserver.go:784] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.516887    7262 genericapiserver.go:784] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.517153    7262 handler.go:285] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.517244    7262 genericapiserver.go:784] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.517249    7262 genericapiserver.go:784] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.522951    7262 handler.go:285] Adding GroupVersion storage.k8s.io v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.523079    7262 genericapiserver.go:784] Skipping API storage.k8s.io/v1beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.523084    7262 genericapiserver.go:784] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.525225+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":10,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.527131    7262 handler.go:285] Adding GroupVersion flowcontrol.apiserver.k8s.io v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.527223    7262 genericapiserver.go:784] Skipping API flowcontrol.apiserver.k8s.io/v1beta3 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.527229    7262 genericapiserver.go:784] Skipping API flowcontrol.apiserver.k8s.io/v1beta2 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.527231    7262 genericapiserver.go:784] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.531819    7262 handler.go:285] Adding GroupVersion apps v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.532012    7262 genericapiserver.go:784] Skipping API apps/v1beta2 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.532024    7262 genericapiserver.go:784] Skipping API apps/v1beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.533696    7262 handler.go:285] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.533787    7262 genericapiserver.go:784] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.533793    7262 genericapiserver.go:784] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.534477    7262 handler.go:285] Adding GroupVersion events.k8s.io v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.534623    7262 genericapiserver.go:784] Skipping API events.k8s.io/v1beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.534667    7262 genericapiserver.go:784] Skipping API resource.k8s.io/v1beta2 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.542333    7262 handler.go:285] Adding GroupVersion resource.k8s.io v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.542359    7262 genericapiserver.go:784] Skipping API resource.k8s.io/v1beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.542364    7262 genericapiserver.go:784] Skipping API resource.k8s.io/v1alpha3 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: I0113 17:42:30.549083    7262 handler.go:285] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
Jan 13 17:42:30 host k3s[7262]: W0113 17:42:30.549228    7262 genericapiserver.go:784] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.551622+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":11,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.583036+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":12,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.645860+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":13,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.674898+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":14,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.713958+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":15,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.743981+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":16,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.769155+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":17,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.803430+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":18,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.833003+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":19,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.859450+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":20,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.898080+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":21,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.931256+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":22,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.958020+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":23,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:30 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:30.986923+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":24,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.012682+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":25,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.036105+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":26,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.060136+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":27,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: time="2026-01-13T17:42:31+01:00" level=info msg="cri-dockerd is now running"
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.099592+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":28,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: time="2026-01-13T17:42:31+01:00" level=info msg="Running kubelet --cloud-provider=external --config-dir=/var/lib/rancher/k3s/agent/etc/kubelet.conf.d --hostname-override=host --kubeconfig=/var/lib/rancher/k3s/agent/kubelet.kubeconfig --node-ip=192.168.0.233 --node-labels= --read-only-port=0"
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.133483+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":29,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.175255+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":30,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.207259+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":31,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.234052+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":32,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.259203+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":33,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.285036+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":34,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.315122+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":35,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.339404+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":36,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.349785    7262 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.349908    7262 secure_serving.go:211] Serving securely on 127.0.0.1:6444
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.349990    7262 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350000    7262 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350083    7262 system_namespaces_controller.go:66] Starting system namespaces controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350017    7262 tlsconfig.go:243] "Starting DynamicServingCertificateController"
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350179    7262 apf_controller.go:377] Starting API Priority and Fairness config controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350209    7262 apiservice_controller.go:100] Starting APIServiceRegistrationController
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350216    7262 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350255    7262 aggregator.go:169] waiting for initial CRD sync...
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350357    7262 crdregistration_controller.go:114] Starting crd-autoregister controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350370    7262 shared_informer.go:349] "Waiting for caches to sync" controller="crd-autoregister"
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350393    7262 controller.go:80] Starting OpenAPI V3 AggregationController
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350436    7262 controller.go:78] Starting OpenAPI AggregationController
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350452    7262 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt::/var/lib/rancher/k3s/server/tls/client-auth-proxy.key"
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350472    7262 local_available_controller.go:156] Starting LocalAvailability controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350474    7262 gc_controller.go:78] Starting apiserver lease garbage collector
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350493    7262 remote_available_controller.go:425] Starting RemoteAvailability controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350501    7262 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350504    7262 customresource_discovery_controller.go:294] Starting DiscoveryController
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350650    7262 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350667    7262 shared_informer.go:349] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350798    7262 controller.go:119] Starting legacy_token_tracking_controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350807    7262 shared_informer.go:349] "Waiting for caches to sync" controller="configmaps"
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350831    7262 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350903    7262 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.351413    7262 controller.go:90] Starting OpenAPI V3 controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.351453    7262 naming_controller.go:299] Starting NamingConditionController
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.351477    7262 establishing_controller.go:81] Starting EstablishingController
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.351508    7262 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.350480    7262 cache.go:32] Waiting for caches to sync for LocalAvailability controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.351576    7262 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.351597    7262 crd_finalizer.go:269] Starting CRDFinalizer
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.351689    7262 repairip.go:210] Starting ipallocator-repair-controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.351714    7262 shared_informer.go:349] "Waiting for caches to sync" controller="ipallocator-repair-controller"
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.352955    7262 controller.go:142] Starting OpenAPI controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.358344    7262 default_servicecidr_controller.go:111] Starting kubernetes-service-cidr-controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.359041    7262 shared_informer.go:349] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.364180+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":37,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.390924+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":38,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.418430+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":39,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.445047+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":40,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.452170    7262 cache.go:39] Caches are synced for LocalAvailability controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.452981    7262 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.453221    7262 cache.go:39] Caches are synced for APIServiceRegistrationController controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.453777    7262 shared_informer.go:356] "Caches are synced" controller="configmaps"
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.454033    7262 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.453824    7262 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.453720    7262 apf_controller.go:382] Running API Priority and Fairness config worker
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.454097    7262 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.454176    7262 aggregator.go:171] initial CRD sync complete...
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.454194    7262 autoregister_controller.go:144] Starting autoregister controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.454203    7262 cache.go:32] Waiting for caches to sync for autoregister controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.454215    7262 cache.go:39] Caches are synced for autoregister controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.453744    7262 cache.go:39] Caches are synced for RemoteAvailability controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.454059    7262 handler_discovery.go:451] Starting ResourceDiscoveryManager
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.459988    7262 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.460111    7262 default_servicecidr_controller.go:166] Creating default ServiceCIDR with CIDRs: [10.43.0.0/16]
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.466711    7262 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.470846    7262 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.470896    7262 policy_source.go:240] refreshing policies
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.470936+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":41,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.477105    7262 controller.go:667] quota admission added evaluator for: namespaces
Jan 13 17:42:31 host k3s[7262]: E0113 17:42:31.477902    7262 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.497118+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":42,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.523019+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":43,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: E0113 17:42:31.527410    7262 controller.go:95] Unable to perform initial Kubernetes service initialization: namespaces "default" not found
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.549095+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":44,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.583041+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":45,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.610900+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":46,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.618447    7262 default_servicecidr_controller.go:228] Setting default ServiceCIDR condition Ready to True
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.633784+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":47,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.658266    7262 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.43.0.0/16
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.660061+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":48,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.679051    7262 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.681183    7262 default_servicecidr_controller.go:228] Setting default ServiceCIDR condition Ready to True
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.692724+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":49,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.697923    7262 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.43.0.0/16
Jan 13 17:42:31 host k3s[7262]: I0113 17:42:31.717638    7262 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.720461+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":50,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.748401+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":51,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.774838+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":52,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.801016+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":53,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.825748+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":54,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.853016+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":55,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: time="2026-01-13T17:42:31+01:00" level=info msg="Polling for API server readiness: GET /readyz failed: unknown"
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.879930+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":56,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.902949+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":57,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.932211+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":58,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.959557+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":59,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:31 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:31.984030+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":60,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.009221+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":61,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.035267+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":62,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.062111+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":63,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.086890+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":64,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.114339+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":65,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.140376+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":66,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.165826+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":67,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.189130+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":68,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.216487+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":69,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.240410+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":70,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.268042+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":71,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.301693+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":72,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.328660+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":73,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.355093+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":74,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.386264+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":75,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: I0113 17:42:32.390976    7262 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
Jan 13 17:42:32 host k3s[7262]: I0113 17:42:32.403535    7262 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
Jan 13 17:42:32 host k3s[7262]: I0113 17:42:32.403929    7262 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.410733+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":76,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.437966+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":77,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.461504+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":78,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.488504+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":79,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.513044+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":80,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.539463+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":81,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.566860+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":82,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.595155+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":83,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.619878+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":84,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.646182+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":85,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.673549+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":86,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.703001+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":87,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.729306+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":88,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.757014+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":89,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.783055+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":90,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.810321+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":91,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.835907+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":92,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.859274+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":93,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.883488+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":94,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.908552+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":95,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.934373+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":96,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.959026+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":97,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:32 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:32.985190+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":98,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:33 host k3s[7262]: {"level":"warn","ts":"2026-01-13T17:42:33.014368+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002446780/kine.sock","method":"/etcdserverpb.KV/Range","attempt":99,"error":"rpc error: code = Canceled desc = grpc: the client connection is closing"}
Jan 13 17:42:33 host k3s[7262]: time="2026-01-13T17:42:33+01:00" level=info msg="Polling for API server readiness: GET /readyz failed: an error on the server (\"[+]ping ok\\n[+]log ok\\n[+]etcd ok\\n[+]etcd-readiness ok\\n[+]informer-sync ok\\n[+]poststarthook/start-apiserver-admission-initializer ok\\n[+]poststarthook/generic-apiserver-start-informers ok\\n[+]poststarthook/priority-and-fairness-config-consumer ok\\n[+]poststarthook/priority-and-fairness-filter ok\\n[+]poststarthook/storage-object-count-tracker-hook ok\\n[+]poststarthook/start-apiextensions-informers ok\\n[+]poststarthook/start-apiextensions-controllers ok\\n[+]poststarthook/crd-informer-synced ok\\n[+]poststarthook/start-system-namespaces-controller ok\\n[+]poststarthook/start-cluster-authentication-info-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok\\n[+]poststarthook/start-legacy-token-tracking-controller ok\\n[+]poststarthook/start-service-ip-repair-controllers ok\\n[-]poststarthook/rbac/bootstrap-roles failed: reason withheld\\n[+]poststarthook/scheduling/bootstrap-system-priority-classes ok\\n[+]poststarthook/priority-and-fairness-config-producer ok\\n[+]poststarthook/bootstrap-controller ok\\n[+]poststarthook/start-kubernetes-service-cidr-controller ok\\n[+]poststarthook/aggregator-reload-proxy-client-cert ok\\n[+]poststarthook/start-kube-aggregator-informers ok\\n[+]poststarthook/apiservice-status-local-available-controller ok\\n[+]poststarthook/apiservice-status-remote-available-controller ok\\n[+]poststarthook/apiservice-registration-controller ok\\n[+]poststarthook/apiservice-discovery-controller ok\\n[+]poststarthook/kube-apiserver-autoregistration ok\\n[+]autoregister-completion ok\\n[+]poststarthook/apiservice-openapi-controller ok\\n[+]poststarthook/apiservice-openapiv3-controller ok\\n[+]shutdown ok\\nreadyz check failed\") has prevented the request from succeeding"
Jan 13 17:42:34 host k3s[7262]: I0113 17:42:34.294797    7262 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
Jan 13 17:42:34 host k3s[7262]: I0113 17:42:34.421442    7262 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
Jan 13 17:42:34 host k3s[7262]: I0113 17:42:34.661018    7262 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.43.0.1"}
Jan 13 17:42:34 host k3s[7262]: W0113 17:42:34.688076    7262 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.0.233]
Jan 13 17:42:34 host k3s[7262]: I0113 17:42:34.692385    7262 controller.go:667] quota admission added evaluator for: endpoints
Jan 13 17:42:34 host k3s[7262]: I0113 17:42:34.706425    7262 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
Jan 13 17:42:35 host k3s[7262]: time="2026-01-13T17:42:35+01:00" level=info msg="Kube API server is now running"
Jan 13 17:42:35 host k3s[7262]: time="2026-01-13T17:42:35+01:00" level=info msg="k3s is up and running"
Jan 13 17:42:35 host k3s[7262]: time="2026-01-13T17:42:35+01:00" level=info msg="Waiting for untainted node"
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.868519    7262 event.go:389] "Event occurred" object="host" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="CertificateExpirationOK" message="Node and Certificate Authority certificates managed by k3s are OK"
Jan 13 17:42:35 host systemd[1]: Started k3s.service - Lightweight Kubernetes.
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.894040    7262 server.go:525] "Kubelet version" kubeletVersion="v1.34.3+k3s1"
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.894738    7262 server.go:527] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.894782    7262 watchdog_linux.go:95] "Systemd watchdog is not enabled"
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.894791    7262 watchdog_linux.go:137] "Systemd watchdog is not enabled or the interval is invalid, so health checking will not be started."
Jan 13 17:42:35 host k3s[7262]: time="2026-01-13T17:42:35+01:00" level=info msg="Waiting for cloud-controller-manager privileges to become available"
Jan 13 17:42:35 host k3s[7262]: time="2026-01-13T17:42:35+01:00" level=info msg="Creating k3s-supervisor event broadcaster"
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.907457    7262 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/agent/client-ca.crt"
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.925891    7262 server.go:1419] "Using cgroup driver setting received from the CRI runtime" cgroupDriver="systemd"
Jan 13 17:42:35 host k3s[7262]: time="2026-01-13T17:42:35+01:00" level=info msg="Creating embedded CRD addons.k3s.cattle.io"
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.952475    7262 server.go:777] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  Defaulting to /"
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.952850    7262 server.go:838] "NoSwap is set due to memorySwapBehavior not specified" memorySwapBehavior="" FailSwapOn=false
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.953251    7262 swap_util.go:115] "Swap is on" /proc/swaps contents=<
Jan 13 17:42:35 host k3s[7262]:         Filename                                Type                Size                Used                Priority
Jan 13 17:42:35 host k3s[7262]:         /swap.img                               file                4194300                0                -2
Jan 13 17:42:35 host k3s[7262]:  >
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.954050    7262 container_manager_linux.go:270] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.954392    7262 container_manager_linux.go:275] "Creating Container Manager object based on Node Config" nodeConfig={"NodeName":"host","RuntimeCgroupsName":"","SystemCgroupsName":"","KubeletCgroupsName":"","KubeletOOMScoreAdj":-999,"ContainerRuntime":"","CgroupsPerQOS":true,"CgroupRoot":"/","CgroupDriver":"systemd","KubeletRootDir":"/var/lib/kubelet","ProtectKernelDefaults":false,"KubeReservedCgroupName":"","SystemReservedCgroupName":"","ReservedSystemCPUs":{},"EnforceNodeAllocatable":{"pods":{}},"KubeReserved":null,"SystemReserved":null,"HardEvictionThresholds":[{"Signal":"imagefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null},{"Signal":"nodefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null}],"QOSReserved":{},"CPUManagerPolicy":"none","CPUManagerPolicyOptions":null,"TopologyManagerScope":"container","CPUManagerReconcilePeriod":10000000000,"MemoryManagerPolicy":"None","MemoryManagerReservedMemory":null,"PodPidsLimit":-1,"EnforceCPULimits":true,"CPUCFSQuotaPeriod":100000000,"TopologyManagerPolicy":"none","TopologyManagerPolicyOptions":null,"CgroupVersion":2}
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.954949    7262 topology_manager.go:138] "Creating topology manager with none policy"
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.955021    7262 container_manager_linux.go:306] "Creating device plugin manager"
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.955207    7262 container_manager_linux.go:315] "Creating Dynamic Resource Allocation (DRA) manager"
Jan 13 17:42:35 host k3s[7262]: time="2026-01-13T17:42:35+01:00" level=info msg="Creating embedded CRD etcdsnapshotfiles.k3s.cattle.io"
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.973402    7262 state_mem.go:36] "Initialized new in-memory state store"
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.973674    7262 kubelet.go:475] "Attempting to sync node with API server"
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.973685    7262 kubelet.go:376] "Adding static pod path" path="/var/lib/rancher/k3s/agent/pod-manifests"
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.973697    7262 kubelet.go:387] "Adding apiserver pod source"
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.973705    7262 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.982069    7262 kuberuntime_manager.go:291] "Container runtime initialized" containerRuntime="docker" version="29.1.4" apiVersion="v1"
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.983377    7262 kubelet.go:940] "Not starting ClusterTrustBundle informer because we are in static kubelet mode or the ClusterTrustBundleProjection featuregate is disabled"
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.983973    7262 kubelet.go:964] "Not starting PodCertificateRequest manager because we are in static kubelet mode or the PodCertificateProjection feature gate is disabled"
Jan 13 17:42:35 host k3s[7262]: W0113 17:42:35.984192    7262 probe.go:272] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.989405    7262 server.go:1257] "Started kubelet"
Jan 13 17:42:35 host k3s[7262]: I0113 17:42:35.995800    7262 server.go:180] "Starting to listen" address="0.0.0.0" port=10250
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:35.996391    7262 ratelimit.go:56] "Setting rate limiting for endpoint" service="podresources" qps=100 burstTokens=10
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:35.996456    7262 server_v1.go:49] "podresources" method="list" useActivePods=true
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:35.999011    7262 server.go:310] "Adding debug handlers to kubelet server"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.006745    7262 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.009877    7262 server.go:249] "Starting to serve the podresources API" endpoint="unix:/var/lib/kubelet/pod-resources/kubelet.sock"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.014301    7262 dynamic_serving_content.go:135] "Starting controller" name="kubelet-server-cert-files::/var/lib/rancher/k3s/agent/serving-kubelet.crt::/var/lib/rancher/k3s/agent/serving-kubelet.key"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.035438    7262 volume_manager.go:313] "Starting Kubelet Volume Manager"
Jan 13 17:42:36 host k3s[7262]: E0113 17:42:36.048277    7262 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"host\" not found"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.050496    7262 reconciler.go:29] "Reconciler: start to sync state"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.058925    7262 factory.go:223] Registration of the systemd container factory successfully
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.059199    7262 factory.go:221] Registration of the crio container factory failed: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Creating embedded CRD helmchartconfigs.helm.cattle.io"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.070085    7262 desired_state_of_world_populator.go:146] "Desired state populator starts to run"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.089924    7262 handler.go:285] Adding GroupVersion k3s.cattle.io v1 to ResourceManager
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.090033    7262 factory.go:223] Registration of the containerd container factory successfully
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.135202    7262 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
Jan 13 17:42:36 host k3s[7262]: E0113 17:42:36.151256    7262 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"host\" not found"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Creating embedded CRD helmcharts.helm.cattle.io"
Jan 13 17:42:36 host k3s[7262]: E0113 17:42:36.165372    7262 nodelease.go:49] "Failed to get node when trying to set owner ref to the node lease" err="nodes \"host\" not found" node="host"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.180646    7262 cpu_manager.go:221] "Starting CPU manager" policy="none"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.180755    7262 cpu_manager.go:222] "Reconciling" reconcilePeriod="10s"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.180785    7262 state_mem.go:36] "Initialized new in-memory state store"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.194016    7262 policy_none.go:49] "None policy: Start"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.194445    7262 memory_manager.go:187] "Starting memorymanager" policy="None"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.194470    7262 state_mem.go:36] "Initializing new in-memory state store"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.203707    7262 policy_none.go:47] "Start"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.235883    7262 shared_informer.go:356] "Caches are synced" controller="node informer cache"
Jan 13 17:42:36 host k3s[7262]: E0113 17:42:36.253489    7262 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"host\" not found"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.257562    7262 handler.go:285] Adding GroupVersion k3s.cattle.io v1 to ResourceManager
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Waiting for CRD helmchartconfigs.helm.cattle.io to become available"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.296164    7262 kubelet_network_linux.go:54] "Initialized iptables rules." protocol="IPv4"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.329314    7262 kubelet_network_linux.go:54] "Initialized iptables rules." protocol="IPv6"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.329342    7262 status_manager.go:244] "Starting to sync pod status with apiserver"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.330580    7262 kubelet.go:2428] "Starting kubelet main sync loop"
Jan 13 17:42:36 host k3s[7262]: E0113 17:42:36.331008    7262 kubelet.go:2452] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
Jan 13 17:42:36 host k3s[7262]: E0113 17:42:36.343091    7262 manager.go:513] "Failed to read data from checkpoint" err="checkpoint is not found" checkpoint="kubelet_internal_checkpoint"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.343328    7262 eviction_manager.go:189] "Eviction manager: starting control loop"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.343346    7262 container_log_manager.go:146] "Initializing container log rotate workers" workers=1 monitorPeriod="10s"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.347858    7262 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Jan 13 17:42:36 host k3s[7262]: E0113 17:42:36.359521    7262 eviction_manager.go:267] "eviction manager: failed to check if we have separate container filesystem. Ignoring." err="no imagefs label for configured runtime"
Jan 13 17:42:36 host k3s[7262]: E0113 17:42:36.360072    7262 eviction_manager.go:292] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"host\" not found"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.387006    7262 handler.go:285] Adding GroupVersion helm.cattle.io v1 to ResourceManager
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.445932    7262 kubelet_node_status.go:75] "Attempting to register node" node="host"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.487997    7262 kubelet_node_status.go:78] "Successfully registered node" node="host"
Jan 13 17:42:36 host k3s[7262]: E0113 17:42:36.488477    7262 kubelet_node_status.go:486] "Error updating node status, will retry" err="error getting node \"host\": node \"host\" not found"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.545379    7262 handler.go:285] Adding GroupVersion helm.cattle.io v1 to ResourceManager
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Annotations and labels have been set successfully on node: host"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Starting flannel with backend vxlan"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Done waiting for CRD helmchartconfigs.helm.cattle.io to become available"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Waiting for CRD helmcharts.helm.cattle.io to become available"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Done waiting for CRD helmcharts.helm.cattle.io to become available"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-37.1.1+up37.1.0.tgz"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-crd-37.1.1+up37.1.0.tgz"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/ccm.yaml"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/coredns.yaml"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/local-storage.yaml"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/rolebindings.yaml"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/runtimes.yaml"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/traefik.yaml"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Starting dynamiclistener CN filter node controller with SANs: [k3s-server.local 127.0.0.1 ::1 localhost host 192.168.0.233 10.43.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local]"
Jan 13 17:42:36 host k3s[7262]: time="2026-01-13T17:42:36+01:00" level=info msg="Tunnel server egress proxy mode: agent"
Jan 13 17:42:36 host k3s[7262]: I0113 17:42:36.982787    7262 apiserver.go:52] "Watching apiserver"
Jan 13 17:42:37 host k3s[7262]: time="2026-01-13T17:42:37+01:00" level=info msg="Starting k3s.cattle.io/v1, Kind=Addon controller"
Jan 13 17:42:37 host k3s[7262]: time="2026-01-13T17:42:37+01:00" level=info msg="Creating deploy event broadcaster"
Jan 13 17:42:37 host k3s[7262]: time="2026-01-13T17:42:37+01:00" level=info msg="Starting /v1, Kind=Node controller"
Jan 13 17:42:37 host k3s[7262]: time="2026-01-13T17:42:37+01:00" level=info msg="Creating helm-controller event broadcaster"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.082121    7262 desired_state_of_world_populator.go:154] "Finished populating initial desired state of world"
Jan 13 17:42:37 host k3s[7262]: time="2026-01-13T17:42:37+01:00" level=info msg="Labels and annotations have been set successfully on node: host"
Jan 13 17:42:37 host k3s[7262]: time="2026-01-13T17:42:37+01:00" level=info msg="Cluster dns configmap has been set successfully"
Jan 13 17:42:37 host k3s[7262]: time="2026-01-13T17:42:37+01:00" level=info msg="Starting rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding controller"
Jan 13 17:42:37 host k3s[7262]: time="2026-01-13T17:42:37+01:00" level=info msg="Starting batch/v1, Kind=Job controller"
Jan 13 17:42:37 host k3s[7262]: time="2026-01-13T17:42:37+01:00" level=info msg="Starting /v1, Kind=ServiceAccount controller"
Jan 13 17:42:37 host k3s[7262]: time="2026-01-13T17:42:37+01:00" level=info msg="Starting /v1, Kind=Secret controller"
Jan 13 17:42:37 host k3s[7262]: time="2026-01-13T17:42:37+01:00" level=info msg="Starting /v1, Kind=ConfigMap controller"
Jan 13 17:42:37 host k3s[7262]: time="2026-01-13T17:42:37+01:00" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChartConfig controller"
Jan 13 17:42:37 host k3s[7262]: time="2026-01-13T17:42:37+01:00" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChart controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.243104    7262 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.0.233"]
Jan 13 17:42:37 host k3s[7262]: E0113 17:42:37.243286    7262 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.277574    7262 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.278489    7262 server_linux.go:132] "Using iptables Proxier"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.299679    7262 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
Jan 13 17:42:37 host k3s[7262]: time="2026-01-13T17:42:37+01:00" level=info msg="Creating new TLS secret for kube-system/k3s-serving (count: 11): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.0.233:192.168.0.233 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-host:host listener.cattle.io/cn-k3s-server.local:k3s-server.local listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/fingerprint:SHA1=B2D6FF50FE9BE7DC32FEDADD73AA9B4AECF2032D]"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.316269    7262 server.go:527] "Version info" version="v1.34.3+k3s1"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.316482    7262 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.320057    7262 config.go:200] "Starting service config controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.320514    7262 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.320550    7262 config.go:106] "Starting endpoint slice config controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.320557    7262 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.320571    7262 config.go:403] "Starting serviceCIDR config controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.320578    7262 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.322455    7262 config.go:309] "Starting node config controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.322695    7262 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.322715    7262 shared_informer.go:356] "Caches are synced" controller="node config"
Jan 13 17:42:37 host k3s[7262]: time="2026-01-13T17:42:37+01:00" level=info msg="Active TLS secret kube-system/k3s-serving (ver=236) (count 11): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.0.233:192.168.0.233 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-host:host listener.cattle.io/cn-k3s-server.local:k3s-server.local listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/fingerprint:SHA1=B2D6FF50FE9BE7DC32FEDADD73AA9B4AECF2032D]"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.422306    7262 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.422683    7262 shared_informer.go:356] "Caches are synced" controller="service config"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.423252    7262 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.565075    7262 controllermanager.go:191] "Starting" version="v1.34.3+k3s1"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.565190    7262 controllermanager.go:193] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.577916    7262 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.577963    7262 shared_informer.go:349] "Waiting for caches to sync" controller="RequestHeaderAuthRequestController"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.578003    7262 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.578013    7262 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.578028    7262 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.578038    7262 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.578205    7262 secure_serving.go:211] Serving securely on 127.0.0.1:10257
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.579279    7262 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/rancher/k3s/server/tls/kube-controller-manager/kube-controller-manager.crt::/var/lib/rancher/k3s/server/tls/kube-controller-manager/kube-controller-manager.key"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.579524    7262 tlsconfig.go:243] "Starting DynamicServingCertificateController"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.604258    7262 controllermanager.go:781] "Started controller" controller="serviceaccount-token-controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.604372    7262 shared_informer.go:349] "Waiting for caches to sync" controller="tokens"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.633169    7262 controller.go:667] quota admission added evaluator for: serviceaccounts
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.667347    7262 controllermanager.go:781] "Started controller" controller="endpointslice-controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.667871    7262 endpointslice_controller.go:281] "Starting endpoint slice controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.667899    7262 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint_slice"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.678305    7262 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.678855    7262 shared_informer.go:356] "Caches are synced" controller="RequestHeaderAuthRequestController"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.678940    7262 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.706038    7262 shared_informer.go:356] "Caches are synced" controller="tokens"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.729479    7262 controllermanager.go:781] "Started controller" controller="endpointslice-mirroring-controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.730025    7262 endpointslicemirroring_controller.go:227] "Starting EndpointSliceMirroring controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.732237    7262 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint_slice_mirroring"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.765972    7262 controllermanager.go:781] "Started controller" controller="pod-garbage-collector-controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.766337    7262 gc_controller.go:99] "Starting GC controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.766589    7262 shared_informer.go:349] "Waiting for caches to sync" controller="GC"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.831508    7262 controllermanager.go:781] "Started controller" controller="serviceaccount-controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.834041    7262 serviceaccounts_controller.go:114] "Starting service account controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.837313    7262 shared_informer.go:349] "Waiting for caches to sync" controller="service account"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.910218    7262 controllermanager.go:781] "Started controller" controller="job-controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.910545    7262 job_controller.go:257] "Starting job controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.910559    7262 shared_informer.go:349] "Waiting for caches to sync" controller="job"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.946382    7262 controllermanager.go:781] "Started controller" controller="cronjob-controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.946849    7262 cronjob_controllerv2.go:145] "Starting cronjob controller v2"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.947071    7262 shared_informer.go:349] "Waiting for caches to sync" controller="cronjob"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.973917    7262 controllermanager.go:781] "Started controller" controller="persistentvolume-expander-controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.973986    7262 controllermanager.go:744] "Warning: controller is disabled" controller="selinux-warning-controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.974117    7262 expand_controller.go:327] "Starting expand controller"
Jan 13 17:42:37 host k3s[7262]: I0113 17:42:37.974133    7262 shared_informer.go:349] "Waiting for caches to sync" controller="expand"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.010601    7262 controllermanager.go:781] "Started controller" controller="endpoints-controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.011195    7262 endpoints_controller.go:188] "Starting endpoint controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.011346    7262 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.094311    7262 controllermanager.go:781] "Started controller" controller="namespace-controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.095016    7262 namespace_controller.go:202] "Starting namespace controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.097274    7262 shared_informer.go:349] "Waiting for caches to sync" controller="namespace"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.118374    7262 controllermanager.go:781] "Started controller" controller="statefulset-controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.118487    7262 controllermanager.go:733] "Controller is disabled by a feature gate" controller="storageversion-garbage-collector-controller" requiredFeatureGates=["APIServerIdentity","StorageVersionAPI"]
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.118528    7262 controllermanager.go:733] "Controller is disabled by a feature gate" controller="device-taint-eviction-controller" requiredFeatureGates=["DynamicResourceAllocation","DRADeviceTaints"]
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.118764    7262 stateful_set.go:169] "Starting stateful set controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.118779    7262 shared_informer.go:349] "Waiting for caches to sync" controller="stateful set"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.139603    7262 controllermanager.go:781] "Started controller" controller="legacy-serviceaccount-token-cleaner-controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.139831    7262 legacy_serviceaccount_token_cleaner.go:103] "Starting legacy service account token cleaner controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.139845    7262 shared_informer.go:349] "Waiting for caches to sync" controller="legacy-service-account-token-cleaner"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.230891    7262 controllermanager.go:781] "Started controller" controller="taint-eviction-controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.234095    7262 taint_eviction.go:282] "Starting" controller="taint-eviction-controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.234213    7262 taint_eviction.go:288] "Sending events to api server"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.234244    7262 shared_informer.go:349] "Waiting for caches to sync" controller="taint-eviction-controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.289858    7262 controllermanager.go:781] "Started controller" controller="deployment-controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.290231    7262 deployment_controller.go:173] "Starting controller" controller="deployment"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.290250    7262 shared_informer.go:349] "Waiting for caches to sync" controller="deployment"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.321364    7262 controllermanager.go:781] "Started controller" controller="replicationcontroller-controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.321555    7262 replica_set.go:243] "Starting controller" name="replicationcontroller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.321571    7262 shared_informer.go:349] "Waiting for caches to sync" controller="ReplicationController"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.464167    7262 controllermanager.go:781] "Started controller" controller="daemonset-controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.464344    7262 daemon_controller.go:310] "Starting daemon sets controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.464737    7262 shared_informer.go:349] "Waiting for caches to sync" controller="daemon sets"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.617964    7262 controllermanager.go:781] "Started controller" controller="ttl-controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.618110    7262 controllermanager.go:739] "Skipping a cloud provider controller" controller="service-lb-controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.618122    7262 controllermanager.go:739] "Skipping a cloud provider controller" controller="node-route-controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.618127    7262 controllermanager.go:739] "Skipping a cloud provider controller" controller="cloud-node-lifecycle-controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.618207    7262 ttl_controller.go:127] "Starting TTL controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.618218    7262 shared_informer.go:349] "Waiting for caches to sync" controller="TTL"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.778445    7262 controllermanager.go:781] "Started controller" controller="persistentvolume-protection-controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.779083    7262 pv_protection_controller.go:81] "Starting PV protection controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.779213    7262 shared_informer.go:349] "Waiting for caches to sync" controller="PV protection"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.963532    7262 controllermanager.go:781] "Started controller" controller="disruption-controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.963955    7262 disruption.go:457] "Sending events to api server."
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.964006    7262 disruption.go:468] "Starting disruption controller"
Jan 13 17:42:38 host k3s[7262]: I0113 17:42:38.964016    7262 shared_informer.go:349] "Waiting for caches to sync" controller="disruption"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.034093    7262 controller.go:667] quota admission added evaluator for: addons.k3s.cattle.io
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.075238    7262 event.go:389] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.147043    7262 controllermanager.go:781] "Started controller" controller="persistentvolumeclaim-protection-controller"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.147125    7262 pvc_protection_controller.go:168] "Starting PVC protection controller"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.147136    7262 shared_informer.go:349] "Waiting for caches to sync" controller="PVC protection"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.195507    7262 event.go:389] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.258220    7262 event.go:389] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.295214    7262 controllermanager.go:781] "Started controller" controller="ttl-after-finished-controller"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.298324    7262 ttlafterfinished_controller.go:112] "Starting TTL after finished controller"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.298371    7262 shared_informer.go:349] "Waiting for caches to sync" controller="TTL after finished"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.487498    7262 controllermanager.go:781] "Started controller" controller="validatingadmissionpolicy-status-controller"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.487826    7262 shared_informer.go:349] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.570069    7262 controller.go:667] quota admission added evaluator for: deployments.apps
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.622478    7262 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.43.0.10"}
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.627232    7262 event.go:389] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.628559    7262 controllermanager.go:781] "Started controller" controller="certificatesigningrequest-signing-controller"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.630047    7262 certificate_controller.go:120] "Starting certificate controller" name="csrsigning-kubelet-serving"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.630172    7262 shared_informer.go:349] "Waiting for caches to sync" controller="certificate-csrsigning-kubelet-serving"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.630236    7262 certificate_controller.go:120] "Starting certificate controller" name="csrsigning-kubelet-client"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.630243    7262 shared_informer.go:349] "Waiting for caches to sync" controller="certificate-csrsigning-kubelet-client"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.630276    7262 certificate_controller.go:120] "Starting certificate controller" name="csrsigning-kube-apiserver-client"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.630283    7262 shared_informer.go:349] "Waiting for caches to sync" controller="certificate-csrsigning-kube-apiserver-client"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.630383    7262 certificate_controller.go:120] "Starting certificate controller" name="csrsigning-legacy-unknown"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.630392    7262 shared_informer.go:349] "Waiting for caches to sync" controller="certificate-csrsigning-legacy-unknown"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.630810    7262 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.630945    7262 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.631009    7262 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.631064    7262 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.662237    7262 event.go:389] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.768020    7262 controllermanager.go:781] "Started controller" controller="persistentvolume-binder-controller"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.768298    7262 pv_controller_base.go:308] "Starting persistent volume controller"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.768319    7262 shared_informer.go:349] "Waiting for caches to sync" controller="persistent volume"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.850907    7262 event.go:389] "Event occurred" object="host" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodePasswordValidationComplete" message="Deferred node password secret validation complete"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.916361    7262 controllermanager.go:781] "Started controller" controller="root-ca-certificate-publisher-controller"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.917096    7262 publisher.go:107] "Starting root CA cert publisher controller"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.917713    7262 shared_informer.go:349] "Waiting for caches to sync" controller="crt configmap"
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.966071    7262 event.go:389] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
Jan 13 17:42:39 host k3s[7262]: I0113 17:42:39.993032    7262 event.go:389] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.182067    7262 kubelet_node_status.go:439] "Fast updating node status as it just became ready"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.192923    7262 event.go:389] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.222969    7262 controllermanager.go:781] "Started controller" controller="garbage-collector-controller"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.223010    7262 garbagecollector.go:144] "Starting controller" controller="garbagecollector"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.223214    7262 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.223234    7262 graph_builder.go:351] "Running" component="GraphBuilder"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.257810    7262 controllermanager.go:781] "Started controller" controller="certificatesigningrequest-approving-controller"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.263753    7262 controllermanager.go:733] "Controller is disabled by a feature gate" controller="podcertificaterequest-cleaner-controller" requiredFeatureGates=["PodCertificateRequest"]
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.263891    7262 certificate_controller.go:120] "Starting certificate controller" name="csrapproving"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.263918    7262 shared_informer.go:349] "Waiting for caches to sync" controller="certificate-csrapproving"
Jan 13 17:42:40 host k3s[7262]: time="2026-01-13T17:42:40+01:00" level=info msg="Adding node OwnerReference to node-password secret host.node-password.k3s"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.318012    7262 event.go:389] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
Jan 13 17:42:40 host k3s[7262]: time="2026-01-13T17:42:40+01:00" level=info msg="Synced coredns NodeHosts entries for host"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.443203    7262 event.go:389] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.475927    7262 controllermanager.go:781] "Started controller" controller="token-cleaner-controller"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.476120    7262 tokencleaner.go:117] "Starting token cleaner controller"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.476134    7262 shared_informer.go:349] "Waiting for caches to sync" controller="token_cleaner"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.476144    7262 shared_informer.go:356] "Caches are synced" controller="token_cleaner"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.493982    7262 event.go:389] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.537227    7262 node_lifecycle_controller.go:419] "Controller will reconcile labels"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.537481    7262 controllermanager.go:781] "Started controller" controller="node-lifecycle-controller"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.538290    7262 node_lifecycle_controller.go:453] "Sending events to api server"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.539179    7262 node_lifecycle_controller.go:464] "Starting node controller"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.541315    7262 shared_informer.go:349] "Waiting for caches to sync" controller="taint"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.587758    7262 event.go:389] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.658239    7262 event.go:389] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.668661    7262 controllermanager.go:781] "Started controller" controller="persistentvolume-attach-detach-controller"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.668964    7262 attach_detach_controller.go:336] "Starting attach detach controller"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.668981    7262 shared_informer.go:349] "Waiting for caches to sync" controller="attach detach"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.749338    7262 handler.go:285] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.752521    7262 event.go:389] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
Jan 13 17:42:40 host k3s[7262]: W0113 17:42:40.771747    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 17:42:40 host k3s[7262]: E0113 17:42:40.771982    7262 controller.go:146] "Unhandled Error" err=<
Jan 13 17:42:40 host k3s[7262]:         Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
Jan 13 17:42:40 host k3s[7262]:         , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
Jan 13 17:42:40 host k3s[7262]:  >
Jan 13 17:42:40 host k3s[7262]: E0113 17:42:40.772045    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: service "metrics-server" not found
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.796102    7262 event.go:389] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.824025    7262 controllermanager.go:781] "Started controller" controller="clusterrole-aggregation-controller"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.824329    7262 clusterroleaggregation_controller.go:194] "Starting ClusterRoleAggregator controller"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.824344    7262 shared_informer.go:349] "Waiting for caches to sync" controller="ClusterRoleAggregator"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.918125    7262 event.go:389] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.968004    7262 controllermanager.go:781] "Started controller" controller="volumeattributesclass-protection-controller"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.968101    7262 vac_protection_controller.go:206] "Starting VAC protection controller"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.968108    7262 shared_informer.go:349] "Waiting for caches to sync" controller="VAC protection"
Jan 13 17:42:40 host k3s[7262]: I0113 17:42:40.976610    7262 event.go:389] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.052098    7262 alloc.go:328] "allocated clusterIPs" service="kube-system/metrics-server" clusterIPs={"IPv4":"10.43.8.230"}
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.054156    7262 event.go:389] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
Jan 13 17:42:41 host k3s[7262]: W0113 17:42:41.078040    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 17:42:41 host k3s[7262]: E0113 17:42:41.078205    7262 controller.go:146] "Unhandled Error" err=<
Jan 13 17:42:41 host k3s[7262]:         Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
Jan 13 17:42:41 host k3s[7262]:         , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
Jan 13 17:42:41 host k3s[7262]:  >
Jan 13 17:42:41 host k3s[7262]: E0113 17:42:41.078280    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.106985    7262 event.go:389] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.258678    7262 event.go:389] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.265062    7262 controllermanager.go:781] "Started controller" controller="horizontal-pod-autoscaler-controller"
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.265256    7262 horizontal.go:205] "Starting HPA controller"
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.265271    7262 shared_informer.go:349] "Waiting for caches to sync" controller="HPA"
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.302153    7262 event.go:389] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.330086    7262 controllermanager.go:781] "Started controller" controller="certificatesigningrequest-cleaner-controller"
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.330126    7262 controllermanager.go:744] "Warning: controller is disabled" controller="bootstrap-signer-controller"
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.330182    7262 cleaner.go:83] "Starting CSR cleaner controller"
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.443746    7262 serving.go:392] Generated self-signed cert in-memory
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.511453    7262 range_allocator.go:112] "No Secondary Service CIDR provided. Skipping filtering out secondary service addresses"
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.515850    7262 controllermanager.go:781] "Started controller" controller="node-ipam-controller"
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.516052    7262 controllermanager.go:733] "Controller is disabled by a feature gate" controller="kube-apiserver-serving-clustertrustbundle-publisher-controller" requiredFeatureGates=["ClusterTrustBundle"]
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.517034    7262 node_ipam_controller.go:141] "Starting ipam controller"
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.520841    7262 shared_informer.go:349] "Waiting for caches to sync" controller="node"
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.683713    7262 controllermanager.go:781] "Started controller" controller="resourceclaim-controller"
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.685035    7262 controller.go:397] "Starting resource claim controller"
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.685398    7262 shared_informer.go:349] "Waiting for caches to sync" controller="resource_claim"
Jan 13 17:42:41 host k3s[7262]: W0113 17:42:41.749165    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 17:42:41 host k3s[7262]: E0113 17:42:41.750306    7262 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService"
Jan 13 17:42:41 host k3s[7262]: W0113 17:42:41.749321    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 17:42:41 host k3s[7262]: E0113 17:42:41.751369    7262 controller.go:102] "Unhandled Error" err=<
Jan 13 17:42:41 host k3s[7262]:         loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
Jan 13 17:42:41 host k3s[7262]:         , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
Jan 13 17:42:41 host k3s[7262]:  >
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.751394    7262 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.752917    7262 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.805713    7262 controllermanager.go:781] "Started controller" controller="service-cidr-controller"
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.806144    7262 controllermanager.go:759] "Warning: skipping controller" controller="storage-version-migrator-controller"
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.807288    7262 servicecidrs_controller.go:137] "Starting" controller="service-cidr-controller"
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.809694    7262 shared_informer.go:349] "Waiting for caches to sync" controller="service-cidr-controller"
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.859416    7262 event.go:389] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
Jan 13 17:42:41 host k3s[7262]: time="2026-01-13T17:42:41+01:00" level=info msg="Tunnel authorizer set Kubelet Port 0.0.0.0:10250"
Jan 13 17:42:41 host k3s[7262]: I0113 17:42:41.901835    7262 event.go:389] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
Jan 13 17:42:42 host k3s[7262]: E0113 17:42:42.138889    7262 resource_quota_controller.go:175] "Unhandled Error" err="initial discovery check failure, continuing and counting on future sync update: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139072    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="deployments.apps"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139101    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="roles.rbac.authorization.k8s.io"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139110    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="endpointslices.discovery.k8s.io"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139120    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="endpoints"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139129    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="jobs.batch"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139139    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="poddisruptionbudgets.policy"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139155    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="addons.k3s.cattle.io"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139165    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="csistoragecapacities.storage.k8s.io"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139213    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="serviceaccounts"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139221    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="statefulsets.apps"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139230    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="cronjobs.batch"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139239    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="helmchartconfigs.helm.cattle.io"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139249    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="podtemplates"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139257    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="rolebindings.rbac.authorization.k8s.io"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139265    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="leases.coordination.k8s.io"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139278    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="networkpolicies.networking.k8s.io"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139290    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="helmcharts.helm.cattle.io"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139297    7262 shared_informer.go:673] "Warning: resync period is smaller than resync check period and the informer has already started. Changing it to the resync check period" resyncPeriod="14h4m29.469883643s" resyncCheckPeriod="15h37m8.603210647s"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139312    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="replicasets.apps"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139319    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="controllerrevisions.apps"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139329    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="horizontalpodautoscalers.autoscaling"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139337    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="resourceclaimtemplates.resource.k8s.io"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139351    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="limitranges"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139368    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="daemonsets.apps"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139376    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="ingresses.networking.k8s.io"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.139385    7262 controllermanager.go:781] "Started controller" controller="resourcequota-controller"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.140806    7262 resource_quota_controller.go:300] "Starting resource quota controller"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.140839    7262 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.142789    7262 resource_quota_monitor.go:308] "QuotaMonitor running"
Jan 13 17:42:42 host k3s[7262]: E0113 17:42:42.160515    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.454891    7262 controllermanager.go:781] "Started controller" controller="replicaset-controller"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.459979    7262 replica_set.go:243] "Starting controller" name="replicaset"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.466668    7262 shared_informer.go:349] "Waiting for caches to sync" controller="ReplicaSet"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.538234    7262 controllermanager.go:781] "Started controller" controller="ephemeral-volume-controller"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.548214    7262 controller.go:173] "Starting ephemeral volume controller"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.548551    7262 shared_informer.go:349] "Waiting for caches to sync" controller="ephemeral"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.551375    7262 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.611789    7262 shared_informer.go:356] "Caches are synced" controller="job"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.630218    7262 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.641868    7262 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.681953    7262 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.682194    7262 shared_informer.go:356] "Caches are synced" controller="HPA"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.682105    7262 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.689141    7262 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"host\" does not exist"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.698159    7262 shared_informer.go:356] "Caches are synced" controller="resource_claim"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.706449    7262 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.707417    7262 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.710134    7262 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.717403    7262 shared_informer.go:356] "Caches are synced" controller="endpoint"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.718172    7262 shared_informer.go:356] "Caches are synced" controller="crt configmap"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.720161    7262 shared_informer.go:356] "Caches are synced" controller="TTL"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.721672    7262 shared_informer.go:356] "Caches are synced" controller="node"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.730795    7262 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.736325    7262 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.745206    7262 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.767891    7262 range_allocator.go:177] "Sending events to api server"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.767982    7262 shared_informer.go:356] "Caches are synced" controller="cronjob"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.768308    7262 shared_informer.go:356] "Caches are synced" controller="VAC protection"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.767950    7262 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.768024    7262 shared_informer.go:356] "Caches are synced" controller="GC"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.769199    7262 range_allocator.go:183] "Starting range CIDR allocator"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.769211    7262 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.769217    7262 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.768193    7262 shared_informer.go:356] "Caches are synced" controller="PVC protection"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.770756    7262 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.770997    7262 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.771863    7262 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.774633    7262 shared_informer.go:356] "Caches are synced" controller="expand"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.783016    7262 shared_informer.go:356] "Caches are synced" controller="PV protection"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.767912    7262 shared_informer.go:356] "Caches are synced" controller="ephemeral"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.771912    7262 shared_informer.go:356] "Caches are synced" controller="persistent volume"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.819074    7262 shared_informer.go:356] "Caches are synced" controller="stateful set"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.834239    7262 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.835060    7262 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.868773    7262 shared_informer.go:356] "Caches are synced" controller="disruption"
Jan 13 17:42:42 host k3s[7262]: time="2026-01-13T17:42:42+01:00" level=info msg="Flannel found PodCIDR assigned for node host"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.886000    7262 range_allocator.go:428] "Set node PodCIDR" node="host" podCIDRs=["10.42.0.0/24"]
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.890425    7262 shared_informer.go:356] "Caches are synced" controller="deployment"
Jan 13 17:42:42 host k3s[7262]: time="2026-01-13T17:42:42+01:00" level=info msg="The interface enp0s3 with ipv4 address 192.168.0.233 will be used by flannel"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.899395    7262 kube.go:139] Waiting 10m0s for node controller to sync
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.903285    7262 kube.go:537] Starting kube subnet manager
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.942201    7262 shared_informer.go:356] "Caches are synced" controller="taint"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.947715    7262 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" zone=""
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.954469    7262 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" node="host"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.954838    7262 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" zone="" newState="Normal"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.969942    7262 shared_informer.go:356] "Caches are synced" controller="daemon sets"
Jan 13 17:42:42 host k3s[7262]: I0113 17:42:42.972565    7262 shared_informer.go:356] "Caches are synced" controller="attach detach"
Jan 13 17:42:43 host k3s[7262]: I0113 17:42:43.043093    7262 shared_informer.go:356] "Caches are synced" controller="resource quota"
Jan 13 17:42:43 host k3s[7262]: I0113 17:42:43.052924    7262 shared_informer.go:356] "Caches are synced" controller="resource quota"
Jan 13 17:42:43 host k3s[7262]: I0113 17:42:43.109574    7262 shared_informer.go:356] "Caches are synced" controller="namespace"
Jan 13 17:42:43 host k3s[7262]: I0113 17:42:43.194987    7262 shared_informer.go:356] "Caches are synced" controller="service account"
Jan 13 17:42:43 host k3s[7262]: I0113 17:42:43.336451    7262 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.43.0.0/16
Jan 13 17:42:43 host k3s[7262]: W0113 17:42:43.475606    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 17:42:43 host k3s[7262]: I0113 17:42:43.484244    7262 controller.go:667] quota admission added evaluator for: replicasets.apps
Jan 13 17:42:43 host k3s[7262]: E0113 17:42:43.479238    7262 controller.go:146] "Unhandled Error" err=<
Jan 13 17:42:43 host k3s[7262]:         Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
Jan 13 17:42:43 host k3s[7262]:         , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
Jan 13 17:42:43 host k3s[7262]:  >
Jan 13 17:42:43 host k3s[7262]: E0113 17:42:43.489218    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
Jan 13 17:42:43 host k3s[7262]: I0113 17:42:43.907374    7262 kube.go:163] Node controller sync successful
Jan 13 17:42:43 host k3s[7262]: I0113 17:42:43.950334    7262 vxlan.go:128] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.027022    7262 shared_informer.go:356] "Caches are synced" controller="garbage collector"
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.028154    7262 garbagecollector.go:154] "Garbage collector: all resource monitors have synced"
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.028163    7262 garbagecollector.go:157] "Proceeding to collect garbage"
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.050473    7262 shared_informer.go:356] "Caches are synced" controller="garbage collector"
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.151666    7262 kube.go:704] List of node(host) annotations: map[string]string{"alpha.kubernetes.io/provided-node-ip":"192.168.0.233", "k3s.io/hostname":"host", "k3s.io/internal-ip":"192.168.0.233", "k3s.io/node-args":"[\"server\",\"--docker\",\"--disable\",\"servicelb\",\"--tls-san\",\"k3s-server.local\"]", "k3s.io/node-config-hash":"RDH46IDAKXYNKOHZGJYMTFSMTSIZYLUN5S223JM7UFZDDMRYSRSQ====", "k3s.io/node-env":"{}", "node.alpha.kubernetes.io/ttl":"0", "volumes.kubernetes.io/controller-managed-attach-detach":"true"}
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.303873    7262 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.43.0.0/16
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.392417    7262 iptables.go:50] Starting flannel in iptables mode...
Jan 13 17:42:44 host k3s[7262]: time="2026-01-13T17:42:44+01:00" level=warning msg="no subnet found for key: FLANNEL_NETWORK in file: /run/flannel/subnet.env"
Jan 13 17:42:44 host k3s[7262]: time="2026-01-13T17:42:44+01:00" level=warning msg="no subnet found for key: FLANNEL_SUBNET in file: /run/flannel/subnet.env"
Jan 13 17:42:44 host k3s[7262]: time="2026-01-13T17:42:44+01:00" level=warning msg="no subnet found for key: FLANNEL_IPV6_NETWORK in file: /run/flannel/subnet.env"
Jan 13 17:42:44 host k3s[7262]: time="2026-01-13T17:42:44+01:00" level=warning msg="no subnet found for key: FLANNEL_IPV6_SUBNET in file: /run/flannel/subnet.env"
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.392515    7262 iptables.go:101] Current network or subnet (10.42.0.0/16, 10.42.0.0/24) is not equal to previous one (0.0.0.0/0, 0.0.0.0/0), trying to recycle old iptables rules
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.440867    7262 kube.go:558] Creating the node lease for IPv4. This is the n.Spec.PodCIDRs: [10.42.0.0/24]
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.444166    7262 event.go:389] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
Jan 13 17:42:44 host k3s[7262]: W0113 17:42:44.493847    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 17:42:44 host k3s[7262]: E0113 17:42:44.493879    7262 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService"
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.493889    7262 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
Jan 13 17:42:44 host k3s[7262]: W0113 17:42:44.494004    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 17:42:44 host k3s[7262]: E0113 17:42:44.494044    7262 controller.go:102] "Unhandled Error" err=<
Jan 13 17:42:44 host k3s[7262]:         loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
Jan 13 17:42:44 host k3s[7262]:         , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
Jan 13 17:42:44 host k3s[7262]:  >
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.495224    7262 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.604695    7262 event.go:389] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.650170    7262 iptables.go:111] Setting up masking rules
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.677076    7262 controller.go:667] quota admission added evaluator for: helmcharts.helm.cattle.io
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.684109    7262 iptables.go:212] Changing default FORWARD chain policy to ACCEPT
Jan 13 17:42:44 host k3s[7262]: time="2026-01-13T17:42:44+01:00" level=info msg="Wrote flannel subnet file to /run/flannel/subnet.env"
Jan 13 17:42:44 host k3s[7262]: time="2026-01-13T17:42:44+01:00" level=info msg="Running flannel backend."
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.714129    7262 vxlan_network.go:68] watching for new subnet leases
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.714150    7262 vxlan_network.go:115] starting vxlan device watcher
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.775693    7262 event.go:389] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.890965    7262 iptables.go:358] bootstrap done
Jan 13 17:42:44 host k3s[7262]: I0113 17:42:44.899174    7262 event.go:389] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
Jan 13 17:42:45 host k3s[7262]: I0113 17:42:45.011002    7262 event.go:389] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
Jan 13 17:42:45 host k3s[7262]: I0113 17:42:45.024798    7262 iptables.go:358] bootstrap done
Jan 13 17:42:45 host k3s[7262]: I0113 17:42:45.100196    7262 controller.go:667] quota admission added evaluator for: jobs.batch
Jan 13 17:42:45 host k3s[7262]: I0113 17:42:45.167788    7262 event.go:389] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
Jan 13 17:42:45 host k3s[7262]: I0113 17:42:45.236033    7262 event.go:389] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
Jan 13 17:42:45 host k3s[7262]: I0113 17:42:45.246373    7262 event.go:389] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
Jan 13 17:42:45 host k3s[7262]: I0113 17:42:45.336142    7262 event.go:389] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
Jan 13 17:42:45 host k3s[7262]: I0113 17:42:45.379078    7262 event.go:389] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
Jan 13 17:42:45 host k3s[7262]: I0113 17:42:45.407938    7262 event.go:389] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
Jan 13 17:42:45 host k3s[7262]: I0113 17:42:45.436047    7262 event.go:389] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
Jan 13 17:42:45 host k3s[7262]: I0113 17:42:45.770718    7262 serving.go:392] Generated self-signed cert in-memory
Jan 13 17:42:45 host k3s[7262]: time="2026-01-13T17:42:45+01:00" level=info msg="Started tunnel to 192.168.0.233:6443"
Jan 13 17:42:45 host k3s[7262]: time="2026-01-13T17:42:45+01:00" level=info msg="Stopped tunnel to 127.0.0.1:6443"
Jan 13 17:42:45 host k3s[7262]: time="2026-01-13T17:42:45+01:00" level=info msg="Connecting to proxy" url="wss://192.168.0.233:6443/v1-k3s/connect"
Jan 13 17:42:45 host k3s[7262]: time="2026-01-13T17:42:45+01:00" level=info msg="Proxy done" err="context canceled" url="wss://127.0.0.1:6443/v1-k3s/connect"
Jan 13 17:42:45 host k3s[7262]: time="2026-01-13T17:42:45+01:00" level=info msg="error in remotedialer server [400]: websocket: close 1006 (abnormal closure): unexpected EOF"
Jan 13 17:42:45 host k3s[7262]: time="2026-01-13T17:42:45+01:00" level=info msg="Handling backend connection request [host]"
Jan 13 17:42:45 host k3s[7262]: time="2026-01-13T17:42:45+01:00" level=info msg="Connected to proxy" url="wss://192.168.0.233:6443/v1-k3s/connect"
Jan 13 17:42:45 host k3s[7262]: time="2026-01-13T17:42:45+01:00" level=info msg="Remotedialer connected to proxy" url="wss://192.168.0.233:6443/v1-k3s/connect"
Jan 13 17:42:46 host k3s[7262]: I0113 17:42:46.980153    7262 kuberuntime_manager.go:1828] "Updating runtime config through cri with podcidr" CIDR="10.42.0.0/24"
Jan 13 17:42:46 host k3s[7262]: time="2026-01-13T17:42:46+01:00" level=info msg="Docker cri received runtime config network_config:{pod_cidr:\"10.42.0.0/24\"}"
Jan 13 17:42:46 host k3s[7262]: I0113 17:42:46.981167    7262 kubelet_network.go:47] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.42.0.0/24"
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.294975    7262 controllermanager.go:160] Version: v1.34.3+k3s1
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.305497    7262 secure_serving.go:211] Serving securely on 127.0.0.1:10258
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.306600    7262 tlsconfig.go:243] "Starting DynamicServingCertificateController"
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.306660    7262 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.306675    7262 shared_informer.go:349] "Waiting for caches to sync" controller="RequestHeaderAuthRequestController"
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.306718    7262 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.306727    7262 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.306738    7262 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.306748    7262 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.334222    7262 controllermanager.go:329] Started "cloud-node-controller"
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.334492    7262 controllermanager.go:329] Started "cloud-node-lifecycle-controller"
Jan 13 17:42:47 host k3s[7262]: W0113 17:42:47.334503    7262 controllermanager.go:306] "service-lb-controller" is disabled
Jan 13 17:42:47 host k3s[7262]: W0113 17:42:47.334510    7262 controllermanager.go:306] "node-route-controller" is disabled
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.335181    7262 node_lifecycle_controller.go:112] Sending events to api server
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.335276    7262 node_controller.go:176] Sending events to api server.
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.336059    7262 node_controller.go:185] Waiting for informer caches to sync
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.406929    7262 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.407199    7262 shared_informer.go:356] "Caches are synced" controller="RequestHeaderAuthRequestController"
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.407233    7262 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.438838    7262 node_controller.go:429] Initializing node host with cloud provider
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.456420    7262 node_controller.go:474] Successfully initialized node host with cloud provider
Jan 13 17:42:47 host k3s[7262]: I0113 17:42:47.457115    7262 event.go:389] "Event occurred" object="host" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="Synced" message="Node synced successfully"
Jan 13 17:42:48 host k3s[7262]: I0113 17:42:48.702097    7262 server.go:173] "Starting Kubernetes Scheduler" version="v1.34.3+k3s1"
Jan 13 17:42:48 host k3s[7262]: I0113 17:42:48.702117    7262 server.go:175] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Jan 13 17:42:48 host k3s[7262]: I0113 17:42:48.713313    7262 secure_serving.go:211] Serving securely on 127.0.0.1:10259
Jan 13 17:42:48 host k3s[7262]: I0113 17:42:48.713378    7262 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
Jan 13 17:42:48 host k3s[7262]: I0113 17:42:48.713387    7262 shared_informer.go:349] "Waiting for caches to sync" controller="RequestHeaderAuthRequestController"
Jan 13 17:42:48 host k3s[7262]: I0113 17:42:48.713400    7262 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/rancher/k3s/server/tls/kube-scheduler/kube-scheduler.crt::/var/lib/rancher/k3s/server/tls/kube-scheduler/kube-scheduler.key"
Jan 13 17:42:48 host k3s[7262]: I0113 17:42:48.714292    7262 tlsconfig.go:243] "Starting DynamicServingCertificateController"
Jan 13 17:42:48 host k3s[7262]: I0113 17:42:48.715011    7262 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
Jan 13 17:42:48 host k3s[7262]: I0113 17:42:48.715170    7262 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
Jan 13 17:42:48 host k3s[7262]: I0113 17:42:48.716497    7262 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
Jan 13 17:42:48 host k3s[7262]: I0113 17:42:48.716816    7262 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
Jan 13 17:42:48 host k3s[7262]: I0113 17:42:48.819096    7262 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
Jan 13 17:42:48 host k3s[7262]: I0113 17:42:48.821181    7262 shared_informer.go:356] "Caches are synced" controller="RequestHeaderAuthRequestController"
Jan 13 17:42:48 host k3s[7262]: I0113 17:42:48.821686    7262 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.054105    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-dir\" (UniqueName: \"kubernetes.io/empty-dir/4a117e67-62a3-4604-9f62-9552f930c9fb-tmp-dir\") pod \"metrics-server-7b9c9c4b9c-pc55q\" (UID: \"4a117e67-62a3-4604-9f62-9552f930c9fb\") " pod="kube-system/metrics-server-7b9c9c4b9c-pc55q"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.054365    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-5tjbd\" (UniqueName: \"kubernetes.io/projected/4a117e67-62a3-4604-9f62-9552f930c9fb-kube-api-access-5tjbd\") pod \"metrics-server-7b9c9c4b9c-pc55q\" (UID: \"4a117e67-62a3-4604-9f62-9552f930c9fb\") " pod="kube-system/metrics-server-7b9c9c4b9c-pc55q"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.054394    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/4763e582-ab41-4c62-8244-6b5c304d36a1-config-volume\") pod \"local-path-provisioner-578895bd58-nv5mr\" (UID: \"4763e582-ab41-4c62-8244-6b5c304d36a1\") " pod="kube-system/local-path-provisioner-578895bd58-nv5mr"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.054421    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-x9v5g\" (UniqueName: \"kubernetes.io/projected/4763e582-ab41-4c62-8244-6b5c304d36a1-kube-api-access-x9v5g\") pod \"local-path-provisioner-578895bd58-nv5mr\" (UID: \"4763e582-ab41-4c62-8244-6b5c304d36a1\") " pod="kube-system/local-path-provisioner-578895bd58-nv5mr"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.156702    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/b37262ee-f031-4d9c-98b0-992ee8fb875d-klipper-config\") pod \"helm-install-traefik-crd-9zrsc\" (UID: \"b37262ee-f031-4d9c-98b0-992ee8fb875d\") " pod="kube-system/helm-install-traefik-crd-9zrsc"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.156897    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-5hlcz\" (UniqueName: \"kubernetes.io/projected/b37262ee-f031-4d9c-98b0-992ee8fb875d-kube-api-access-5hlcz\") pod \"helm-install-traefik-crd-9zrsc\" (UID: \"b37262ee-f031-4d9c-98b0-992ee8fb875d\") " pod="kube-system/helm-install-traefik-crd-9zrsc"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.156930    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-klipper-cache\") pod \"helm-install-traefik-cl86v\" (UID: \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\") " pod="kube-system/helm-install-traefik-cl86v"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.156941    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-content\") pod \"helm-install-traefik-cl86v\" (UID: \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\") " pod="kube-system/helm-install-traefik-cl86v"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.156953    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/b37262ee-f031-4d9c-98b0-992ee8fb875d-klipper-helm\") pod \"helm-install-traefik-crd-9zrsc\" (UID: \"b37262ee-f031-4d9c-98b0-992ee8fb875d\") " pod="kube-system/helm-install-traefik-crd-9zrsc"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.156962    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/bc3abdd1-81d3-4cc5-ba70-9dd62922effb-config-volume\") pod \"coredns-7f496c8d7d-6jpfv\" (UID: \"bc3abdd1-81d3-4cc5-ba70-9dd62922effb\") " pod="kube-system/coredns-7f496c8d7d-6jpfv"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.156997    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/b37262ee-f031-4d9c-98b0-992ee8fb875d-tmp\") pod \"helm-install-traefik-crd-9zrsc\" (UID: \"b37262ee-f031-4d9c-98b0-992ee8fb875d\") " pod="kube-system/helm-install-traefik-crd-9zrsc"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.157006    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/b37262ee-f031-4d9c-98b0-992ee8fb875d-content\") pod \"helm-install-traefik-crd-9zrsc\" (UID: \"b37262ee-f031-4d9c-98b0-992ee8fb875d\") " pod="kube-system/helm-install-traefik-crd-9zrsc"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.157014    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-klipper-helm\") pod \"helm-install-traefik-cl86v\" (UID: \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\") " pod="kube-system/helm-install-traefik-cl86v"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.157025    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-klipper-config\") pod \"helm-install-traefik-cl86v\" (UID: \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\") " pod="kube-system/helm-install-traefik-cl86v"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.157034    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-tmp\") pod \"helm-install-traefik-cl86v\" (UID: \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\") " pod="kube-system/helm-install-traefik-cl86v"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.157043    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/projected/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-values\") pod \"helm-install-traefik-cl86v\" (UID: \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\") " pod="kube-system/helm-install-traefik-cl86v"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.157057    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/b37262ee-f031-4d9c-98b0-992ee8fb875d-klipper-cache\") pod \"helm-install-traefik-crd-9zrsc\" (UID: \"b37262ee-f031-4d9c-98b0-992ee8fb875d\") " pod="kube-system/helm-install-traefik-crd-9zrsc"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.157067    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/projected/b37262ee-f031-4d9c-98b0-992ee8fb875d-values\") pod \"helm-install-traefik-crd-9zrsc\" (UID: \"b37262ee-f031-4d9c-98b0-992ee8fb875d\") " pod="kube-system/helm-install-traefik-crd-9zrsc"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.157079    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mj4bd\" (UniqueName: \"kubernetes.io/projected/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-kube-api-access-mj4bd\") pod \"helm-install-traefik-cl86v\" (UID: \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\") " pod="kube-system/helm-install-traefik-cl86v"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.157087    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"custom-config-volume\" (UniqueName: \"kubernetes.io/configmap/bc3abdd1-81d3-4cc5-ba70-9dd62922effb-custom-config-volume\") pod \"coredns-7f496c8d7d-6jpfv\" (UID: \"bc3abdd1-81d3-4cc5-ba70-9dd62922effb\") " pod="kube-system/coredns-7f496c8d7d-6jpfv"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.157095    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-b2lw6\" (UniqueName: \"kubernetes.io/projected/bc3abdd1-81d3-4cc5-ba70-9dd62922effb-kube-api-access-b2lw6\") pod \"coredns-7f496c8d7d-6jpfv\" (UID: \"bc3abdd1-81d3-4cc5-ba70-9dd62922effb\") " pod="kube-system/coredns-7f496c8d7d-6jpfv"
Jan 13 17:42:49 host k3s[7262]: time="2026-01-13T17:42:49+01:00" level=info msg="Starting network policy controller version v2.6.3-k3s1, built on 2025-12-19T18:40:57Z, go1.24.11"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.163946    7262 network_policy_controller.go:164] Starting network policy controller
Jan 13 17:42:49 host k3s[7262]: time="2026-01-13T17:42:49+01:00" level=info msg="Pulling the image without credentials. Image: rancher/mirrored-pause:3.6"
Jan 13 17:42:49 host k3s[7262]: time="2026-01-13T17:42:49+01:00" level=info msg="Pulling the image without credentials. Image: rancher/mirrored-pause:3.6"
Jan 13 17:42:49 host k3s[7262]: time="2026-01-13T17:42:49+01:00" level=info msg="Pulling the image without credentials. Image: rancher/mirrored-pause:3.6"
Jan 13 17:42:49 host k3s[7262]: time="2026-01-13T17:42:49+01:00" level=info msg="Pulling the image without credentials. Image: rancher/mirrored-pause:3.6"
Jan 13 17:42:49 host k3s[7262]: time="2026-01-13T17:42:49+01:00" level=info msg="Pulling the image without credentials. Image: rancher/mirrored-pause:3.6"
Jan 13 17:42:49 host k3s[7262]: I0113 17:42:49.933492    7262 network_policy_controller.go:179] Starting network policy controller full sync goroutine
Jan 13 17:43:03 host k3s[7262]: time="2026-01-13T17:43:03+01:00" level=info msg="Pulling image rancher/mirrored-pause:3.6: fbe1a72f5dcd: Pulling fs layer "
Jan 13 17:43:03 host k3s[7262]: time="2026-01-13T17:43:03+01:00" level=info msg="Pulling image rancher/mirrored-pause:3.6: fbe1a72f5dcd: Pulling fs layer "
Jan 13 17:43:04 host k3s[7262]: time="2026-01-13T17:43:04+01:00" level=info msg="Stop pulling image rancher/mirrored-pause:3.6: Status: Downloaded newer image for rancher/mirrored-pause:3.6"
Jan 13 17:43:05 host k3s[7262]: time="2026-01-13T17:43:05+01:00" level=info msg="Pulling image rancher/mirrored-pause:3.6: fbe1a72f5dcd: Pull complete "
Jan 13 17:43:05 host k3s[7262]: time="2026-01-13T17:43:05+01:00" level=info msg="Stop pulling image rancher/mirrored-pause:3.6: Status: Downloaded newer image for rancher/mirrored-pause:3.6"
Jan 13 17:43:05 host k3s[7262]: time="2026-01-13T17:43:05+01:00" level=info msg="Stop pulling image rancher/mirrored-pause:3.6: Status: Downloaded newer image for rancher/mirrored-pause:3.6"
Jan 13 17:43:06 host k3s[7262]: time="2026-01-13T17:43:06+01:00" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2e63f8fcaab669e74349849c388fe9db232d519e813d54c9b95746d6613da8f6/resolv.conf as [nameserver 83.139.103.3 nameserver 83.139.121.8]"
Jan 13 17:43:06 host k3s[7262]: I0113 17:43:06.980883    7262 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2e63f8fcaab669e74349849c388fe9db232d519e813d54c9b95746d6613da8f6"
Jan 13 17:43:07 host k3s[7262]: time="2026-01-13T17:43:07+01:00" level=info msg="Stop pulling image rancher/mirrored-pause:3.6: Status: Downloaded newer image for rancher/mirrored-pause:3.6"
Jan 13 17:43:07 host k3s[7262]: time="2026-01-13T17:43:07+01:00" level=info msg="Stop pulling image rancher/mirrored-pause:3.6: Status: Downloaded newer image for rancher/mirrored-pause:3.6"
Jan 13 17:43:07 host k3s[7262]: map[string]interface {}{"cniVersion":"1.0.0", "forceAddress":true, "hairpinMode":true, "ipMasq":false, "ipam":map[string]interface {}{"ranges":[][]map[string]interface {}{[]map[string]interface {}{map[string]interface {}{"subnet":"10.42.0.0/24"}}}, "routes":[]types.Route{types.Route{Dst:net.IPNet{IP:net.IP{0xa, 0x2a, 0x0, 0x0}, Mask:net.IPMask{0xff, 0xff, 0x0, 0x0}}, GW:net.IP(nil), MTU:0, AdvMSS:0, Priority:0, Table:(*int)(nil), Scope:(*int)(nil)}}, "type":"host-local"}, "isDefaultGateway":true, "isGateway":true, "mtu":(*uint)(0xc000180a70), "name":"cbr0", "type":"bridge"}
Jan 13 17:43:07 host k3s[7262]: delegateAdd: netconf sent to delegate plugin:
Jan 13 17:43:08 host k3s[7262]: {"cniVersion":"1.0.0","forceAddress":true,"hairpinMode":true,"ipMasq":false,"ipam":{"ranges":[[{"subnet":"10.42.0.0/24"}]],"routes":[{"dst":"10.42.0.0/16"}],"type":"host-local"},"isDefaultGateway":true,"isGateway":true,"mtu":1450,"name":"cbr0","type":"bridge"}time="2026-01-13T17:43:08+01:00" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3b62d484a92616746d8a922b55ca4f9ce707d549d6af469e9222c4020897c29d/resolv.conf as [nameserver 10.43.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 13 17:43:08 host k3s[7262]: map[string]interface {}{"cniVersion":"1.0.0", "forceAddress":true, "hairpinMode":true, "ipMasq":false, "ipam":map[string]interface {}{"ranges":[][]map[string]interface {}{[]map[string]interface {}{map[string]interface {}{"subnet":"10.42.0.0/24"}}}, "routes":[]types.Route{types.Route{Dst:net.IPNet{IP:net.IP{0xa, 0x2a, 0x0, 0x0}, Mask:net.IPMask{0xff, 0xff, 0x0, 0x0}}, GW:net.IP(nil), MTU:0, AdvMSS:0, Priority:0, Table:(*int)(nil), Scope:(*int)(nil)}}, "type":"host-local"}, "isDefaultGateway":true, "isGateway":true, "mtu":(*uint)(0xc000098bc0), "name":"cbr0", "type":"bridge"}
Jan 13 17:43:08 host k3s[7262]: delegateAdd: netconf sent to delegate plugin:
Jan 13 17:43:09 host k3s[7262]: {"cniVersion":"1.0.0","forceAddress":true,"hairpinMode":true,"ipMasq":false,"ipam":{"ranges":[[{"subnet":"10.42.0.0/24"}]],"routes":[{"dst":"10.42.0.0/16"}],"type":"host-local"},"isDefaultGateway":true,"isGateway":true,"mtu":1450,"name":"cbr0","type":"bridge"}time="2026-01-13T17:43:09+01:00" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/db3321e5d45ae6ee3df5ab777dc12ad9472247b0e3fad68ccd83731244ba6e7f/resolv.conf as [nameserver 10.43.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 13 17:43:09 host k3s[7262]: I0113 17:43:09.247508    7262 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="db3321e5d45ae6ee3df5ab777dc12ad9472247b0e3fad68ccd83731244ba6e7f"
Jan 13 17:43:09 host k3s[7262]: map[string]interface {}{"cniVersion":"1.0.0", "forceAddress":true, "hairpinMode":true, "ipMasq":false, "ipam":map[string]interface {}{"ranges":[][]map[string]interface {}{[]map[string]interface {}{map[string]interface {}{"subnet":"10.42.0.0/24"}}}, "routes":[]types.Route{types.Route{Dst:net.IPNet{IP:net.IP{0xa, 0x2a, 0x0, 0x0}, Mask:net.IPMask{0xff, 0xff, 0x0, 0x0}}, GW:net.IP(nil), MTU:0, AdvMSS:0, Priority:0, Table:(*int)(nil), Scope:(*int)(nil)}}, "type":"host-local"}, "isDefaultGateway":true, "isGateway":true, "mtu":(*uint)(0xc00011abc0), "name":"cbr0", "type":"bridge"}
Jan 13 17:43:09 host k3s[7262]: delegateAdd: netconf sent to delegate plugin:
Jan 13 17:43:09 host k3s[7262]: {"cniVersion":"1.0.0","forceAddress":true,"hairpinMode":true,"ipMasq":false,"ipam":{"ranges":[[{"subnet":"10.42.0.0/24"}]],"routes":[{"dst":"10.42.0.0/16"}],"type":"host-local"},"isDefaultGateway":true,"isGateway":true,"mtu":1450,"name":"cbr0","type":"bridge"}I0113 17:43:09.423615    7262 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3b62d484a92616746d8a922b55ca4f9ce707d549d6af469e9222c4020897c29d"
Jan 13 17:43:09 host k3s[7262]: time="2026-01-13T17:43:09+01:00" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6d5ee5ce45493a3351cca5e79f4a0a16857a6b95a80ec62ad6e548389436935e/resolv.conf as [nameserver 10.43.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 13 17:43:09 host k3s[7262]: map[string]interface {}{"cniVersion":"1.0.0", "forceAddress":true, "hairpinMode":true, "ipMasq":false, "ipam":map[string]interface {}{"ranges":[][]map[string]interface {}{[]map[string]interface {}{map[string]interface {}{"subnet":"10.42.0.0/24"}}}, "routes":[]types.Route{types.Route{Dst:net.IPNet{IP:net.IP{0xa, 0x2a, 0x0, 0x0}, Mask:net.IPMask{0xff, 0xff, 0x0, 0x0}}, GW:net.IP(nil), MTU:0, AdvMSS:0, Priority:0, Table:(*int)(nil), Scope:(*int)(nil)}}, "type":"host-local"}, "isDefaultGateway":true, "isGateway":true, "mtu":(*uint)(0xc000098bc0), "name":"cbr0", "type":"bridge"}
Jan 13 17:43:09 host k3s[7262]: delegateAdd: netconf sent to delegate plugin:
Jan 13 17:43:09 host k3s[7262]: {"cniVersion":"1.0.0","forceAddress":true,"hairpinMode":true,"ipMasq":false,"ipam":{"ranges":[[{"subnet":"10.42.0.0/24"}]],"routes":[{"dst":"10.42.0.0/16"}],"type":"host-local"},"isDefaultGateway":true,"isGateway":true,"mtu":1450,"name":"cbr0","type":"bridge"}time="2026-01-13T17:43:09+01:00" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2b36b3fe1bdd3067483754ee42193629a93b3a9ec0b03c112c8d9d021eb00455/resolv.conf as [nameserver 10.43.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 13 17:43:09 host k3s[7262]: map[string]interface {}{"cniVersion":"1.0.0", "forceAddress":true, "hairpinMode":true, "ipMasq":false, "ipam":map[string]interface {}{"ranges":[][]map[string]interface {}{[]map[string]interface {}{map[string]interface {}{"subnet":"10.42.0.0/24"}}}, "routes":[]types.Route{types.Route{Dst:net.IPNet{IP:net.IP{0xa, 0x2a, 0x0, 0x0}, Mask:net.IPMask{0xff, 0xff, 0x0, 0x0}}, GW:net.IP(nil), MTU:0, AdvMSS:0, Priority:0, Table:(*int)(nil), Scope:(*int)(nil)}}, "type":"host-local"}, "isDefaultGateway":true, "isGateway":true, "mtu":(*uint)(0xc000104bc0), "name":"cbr0", "type":"bridge"}
Jan 13 17:43:09 host k3s[7262]: delegateAdd: netconf sent to delegate plugin:
Jan 13 17:43:13 host k3s[7262]: {"cniVersion":"1.0.0","forceAddress":true,"hairpinMode":true,"ipMasq":false,"ipam":{"ranges":[[{"subnet":"10.42.0.0/24"}]],"routes":[{"dst":"10.42.0.0/16"}],"type":"host-local"},"isDefaultGateway":true,"isGateway":true,"mtu":1450,"name":"cbr0","type":"bridge"}E0113 17:43:13.065225    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 17:43:14 host k3s[7262]: I0113 17:43:14.308316    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 17:43:21 host k3s[7262]: time="2026-01-13T17:43:21+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 3cedb4da7482: Download complete "
Jan 13 17:43:22 host k3s[7262]: time="2026-01-13T17:43:22+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: e273c95755cd: Download complete "
Jan 13 17:43:22 host k3s[7262]: time="2026-01-13T17:43:22+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: bfb59b82a9b6: Pull complete "
Jan 13 17:43:24 host k3s[7262]: time="2026-01-13T17:43:24+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: d411bdd3b6b6: Download complete "
Jan 13 17:43:25 host k3s[7262]: time="2026-01-13T17:43:25+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: d411bdd3b6b6: Download complete "
Jan 13 17:43:31 host k3s[7262]: time="2026-01-13T17:43:31+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 3cedb4da7482: Pull complete "
Jan 13 17:43:31 host k3s[7262]: E0113 17:43:31.454950    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
Jan 13 17:43:32 host k3s[7262]: time="2026-01-13T17:43:32+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: e273c95755cd: Download complete "
Jan 13 17:43:32 host k3s[7262]: time="2026-01-13T17:43:32+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: ddf74a63f7d8: Pull complete "
Jan 13 17:43:34 host k3s[7262]: time="2026-01-13T17:43:34+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: d411bdd3b6b6: Download complete "
Jan 13 17:43:35 host k3s[7262]: time="2026-01-13T17:43:35+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: d411bdd3b6b6: Download complete "
Jan 13 17:43:41 host k3s[7262]: time="2026-01-13T17:43:41+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 3cedb4da7482: Pull complete "
Jan 13 17:43:42 host k3s[7262]: time="2026-01-13T17:43:42+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: dcd36ca5acd6: Downloading [====================>                              ]  1.049MB/2.597MB"
Jan 13 17:43:42 host k3s[7262]: time="2026-01-13T17:43:42+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: ddf74a63f7d8: Pull complete "
Jan 13 17:43:43 host k3s[7262]: E0113 17:43:43.080551    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 17:43:44 host k3s[7262]: I0113 17:43:44.362260    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 17:43:44 host k3s[7262]: time="2026-01-13T17:43:44+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=>                                                 ]  1.049MB/41.29MB"
Jan 13 17:43:44 host k3s[7262]: W0113 17:43:44.496165    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 17:43:44 host k3s[7262]: E0113 17:43:44.497173    7262 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService"
Jan 13 17:43:44 host k3s[7262]: W0113 17:43:44.496249    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 17:43:44 host k3s[7262]: I0113 17:43:44.497413    7262 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
Jan 13 17:43:44 host k3s[7262]: E0113 17:43:44.498103    7262 controller.go:102] "Unhandled Error" err=<
Jan 13 17:43:44 host k3s[7262]:         loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
Jan 13 17:43:44 host k3s[7262]:         , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
Jan 13 17:43:44 host k3s[7262]:  >
Jan 13 17:43:44 host k3s[7262]: I0113 17:43:44.499764    7262 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
Jan 13 17:43:45 host k3s[7262]: time="2026-01-13T17:43:45+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 77cd7e4ae6b6: Downloading [=========================>                         ]  1.049MB/2.041MB"
Jan 13 17:43:51 host k3s[7262]: time="2026-01-13T17:43:51+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 3cedb4da7482: Pull complete "
Jan 13 17:43:52 host k3s[7262]: time="2026-01-13T17:43:52+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: 9824c27679d3: Downloading [=============>                                     ]  1.049MB/3.8MB"
Jan 13 17:43:52 host k3s[7262]: time="2026-01-13T17:43:52+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: ddf74a63f7d8: Pull complete "
Jan 13 17:43:54 host k3s[7262]: time="2026-01-13T17:43:54+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=>                                                 ]  1.049MB/41.29MB"
Jan 13 17:43:55 host k3s[7262]: time="2026-01-13T17:43:55+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=>                                                 ]  1.049MB/41.29MB"
Jan 13 17:44:01 host k3s[7262]: time="2026-01-13T17:44:01+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 3cedb4da7482: Pull complete "
Jan 13 17:44:02 host k3s[7262]: time="2026-01-13T17:44:02+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: 9824c27679d3: Downloading [===========================>                       ]  2.097MB/3.8MB"
Jan 13 17:44:02 host k3s[7262]: time="2026-01-13T17:44:02+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: ddf74a63f7d8: Pull complete "
Jan 13 17:44:04 host k3s[7262]: time="2026-01-13T17:44:04+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==>                                                ]  2.097MB/41.29MB"
Jan 13 17:44:05 host k3s[7262]: time="2026-01-13T17:44:05+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==>                                                ]  2.097MB/41.29MB"
Jan 13 17:44:11 host k3s[7262]: time="2026-01-13T17:44:11+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [==>                                                ]  1.049MB/22.7MB"
Jan 13 17:44:12 host k3s[7262]: time="2026-01-13T17:44:12+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: 9824c27679d3: Downloading [===========================>                       ]  2.097MB/3.8MB"
Jan 13 17:44:12 host k3s[7262]: time="2026-01-13T17:44:12+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: ddf74a63f7d8: Pull complete "
Jan 13 17:44:13 host k3s[7262]: E0113 17:44:13.104424    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 17:44:14 host k3s[7262]: I0113 17:44:14.406999    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 17:44:14 host k3s[7262]: time="2026-01-13T17:44:14+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===>                                               ]  3.146MB/41.29MB"
Jan 13 17:44:15 host k3s[7262]: time="2026-01-13T17:44:15+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===>                                               ]  3.146MB/41.29MB"
Jan 13 17:44:21 host k3s[7262]: time="2026-01-13T17:44:21+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [==>                                                ]  1.049MB/22.7MB"
Jan 13 17:44:22 host k3s[7262]: time="2026-01-13T17:44:22+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: 9824c27679d3: Downloading [=========================================>         ]  3.146MB/3.8MB"
Jan 13 17:44:23 host k3s[7262]: time="2026-01-13T17:44:23+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [==>                                                ]  1.049MB/21.69MB"
Jan 13 17:44:24 host k3s[7262]: time="2026-01-13T17:44:24+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 014e56e61396: Downloading [=============>                                     ]  1.049MB/3.859MB"
Jan 13 17:44:25 host k3s[7262]: time="2026-01-13T17:44:25+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=====>                                             ]  4.194MB/41.29MB"
Jan 13 17:44:31 host k3s[7262]: time="2026-01-13T17:44:31+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [====>                                              ]  2.097MB/22.7MB"
Jan 13 17:44:31 host k3s[7262]: E0113 17:44:31.455235    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
Jan 13 17:44:32 host k3s[7262]: time="2026-01-13T17:44:32+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: c993b855ee4a: Downloading [==============>                                    ]  4.194MB/14.73MB"
Jan 13 17:44:32 host k3s[7262]: time="2026-01-13T17:44:32+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [==>                                                ]  1.049MB/21.69MB"
Jan 13 17:44:34 host k3s[7262]: time="2026-01-13T17:44:34+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: b36647633376: Downloading [========>                                          ]  3.146MB/18.34MB"
Jan 13 17:44:35 host k3s[7262]: time="2026-01-13T17:44:35+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=====>                                             ]  4.194MB/41.29MB"
Jan 13 17:44:41 host k3s[7262]: time="2026-01-13T17:44:41+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [====>                                              ]  2.097MB/22.7MB"
Jan 13 17:44:42 host k3s[7262]: time="2026-01-13T17:44:42+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: c993b855ee4a: Downloading [=================>                                 ]  5.243MB/14.73MB"
Jan 13 17:44:42 host k3s[7262]: time="2026-01-13T17:44:42+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [==>                                                ]  1.049MB/21.69MB"
Jan 13 17:44:43 host k3s[7262]: E0113 17:44:43.125650    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 17:44:44 host k3s[7262]: time="2026-01-13T17:44:44+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 014e56e61396: Downloading [===========================>                       ]  2.097MB/3.859MB"
Jan 13 17:44:44 host k3s[7262]: I0113 17:44:44.460040    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 17:44:45 host k3s[7262]: time="2026-01-13T17:44:45+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: b36647633376: Downloading [===========>                                       ]  4.194MB/18.34MB"
Jan 13 17:44:51 host k3s[7262]: time="2026-01-13T17:44:51+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [======>                                            ]  3.146MB/22.7MB"
Jan 13 17:44:52 host k3s[7262]: time="2026-01-13T17:44:52+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: c993b855ee4a: Downloading [=================>                                 ]  5.243MB/14.73MB"
Jan 13 17:44:52 host k3s[7262]: time="2026-01-13T17:44:52+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [====>                                              ]  2.097MB/21.69MB"
Jan 13 17:44:54 host k3s[7262]: time="2026-01-13T17:44:54+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 014e56e61396: Downloading [===========================>                       ]  2.097MB/3.859MB"
Jan 13 17:44:55 host k3s[7262]: time="2026-01-13T17:44:55+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [======>                                            ]  5.243MB/41.29MB"
Jan 13 17:45:01 host k3s[7262]: time="2026-01-13T17:45:01+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [======>                                            ]  3.146MB/22.7MB"
Jan 13 17:45:02 host k3s[7262]: time="2026-01-13T17:45:02+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: c993b855ee4a: Downloading [=====================>                             ]  6.291MB/14.73MB"
Jan 13 17:45:02 host k3s[7262]: time="2026-01-13T17:45:02+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [====>                                              ]  2.097MB/21.69MB"
Jan 13 17:45:04 host k3s[7262]: time="2026-01-13T17:45:04+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 014e56e61396: Downloading [========================================>          ]  3.146MB/3.859MB"
Jan 13 17:45:05 host k3s[7262]: time="2026-01-13T17:45:05+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: b36647633376: Downloading [==============>                                    ]  5.243MB/18.34MB"
Jan 13 17:45:11 host k3s[7262]: time="2026-01-13T17:45:11+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [=========>                                         ]  4.194MB/22.7MB"
Jan 13 17:45:12 host k3s[7262]: time="2026-01-13T17:45:12+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: c993b855ee4a: Downloading [=====================>                             ]  6.291MB/14.73MB"
Jan 13 17:45:12 host k3s[7262]: time="2026-01-13T17:45:12+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [=======>                                           ]  3.146MB/21.69MB"
Jan 13 17:45:13 host k3s[7262]: E0113 17:45:13.151139    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 17:45:14 host k3s[7262]: time="2026-01-13T17:45:14+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 4f4fb700ef54: Extracting 1 s"
Jan 13 17:45:14 host k3s[7262]: I0113 17:45:14.484549    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 17:45:15 host k3s[7262]: time="2026-01-13T17:45:15+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: b36647633376: Downloading [==============>                                    ]  5.243MB/18.34MB"
Jan 13 17:45:21 host k3s[7262]: time="2026-01-13T17:45:21+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [=========>                                         ]  4.194MB/22.7MB"
Jan 13 17:45:22 host k3s[7262]: time="2026-01-13T17:45:22+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: c993b855ee4a: Downloading [========================>                          ]   7.34MB/14.73MB"
Jan 13 17:45:22 host k3s[7262]: time="2026-01-13T17:45:22+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [=======>                                           ]  3.146MB/21.69MB"
Jan 13 17:45:24 host k3s[7262]: time="2026-01-13T17:45:24+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [========>                                          ]   7.34MB/41.29MB"
Jan 13 17:45:25 host k3s[7262]: time="2026-01-13T17:45:25+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [========>                                          ]   7.34MB/41.29MB"
Jan 13 17:45:31 host k3s[7262]: time="2026-01-13T17:45:31+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [===========>                                       ]  5.243MB/22.7MB"
Jan 13 17:45:31 host k3s[7262]: E0113 17:45:31.454850    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
Jan 13 17:45:32 host k3s[7262]: time="2026-01-13T17:45:32+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: c993b855ee4a: Downloading [========================>                          ]   7.34MB/14.73MB"
Jan 13 17:45:32 host k3s[7262]: time="2026-01-13T17:45:32+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [=========>                                         ]  4.194MB/21.69MB"
Jan 13 17:45:34 host k3s[7262]: time="2026-01-13T17:45:34+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [========>                                          ]   7.34MB/41.29MB"
Jan 13 17:45:35 host k3s[7262]: time="2026-01-13T17:45:35+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [========>                                          ]   7.34MB/41.29MB"
Jan 13 17:45:41 host k3s[7262]: time="2026-01-13T17:45:41+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [===========>                                       ]  5.243MB/22.7MB"
Jan 13 17:45:42 host k3s[7262]: time="2026-01-13T17:45:42+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: c993b855ee4a: Downloading [============================>                      ]  8.389MB/14.73MB"
Jan 13 17:45:42 host k3s[7262]: time="2026-01-13T17:45:42+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [=========>                                         ]  4.194MB/21.69MB"
Jan 13 17:45:43 host k3s[7262]: E0113 17:45:43.168980    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 17:45:44 host k3s[7262]: time="2026-01-13T17:45:44+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==========>                                        ]  8.389MB/41.29MB"
Jan 13 17:45:44 host k3s[7262]: W0113 17:45:44.497912    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 17:45:44 host k3s[7262]: E0113 17:45:44.498082    7262 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService"
Jan 13 17:45:44 host k3s[7262]: I0113 17:45:44.498095    7262 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
Jan 13 17:45:44 host k3s[7262]: W0113 17:45:44.500321    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 17:45:44 host k3s[7262]: E0113 17:45:44.500427    7262 controller.go:102] "Unhandled Error" err=<
Jan 13 17:45:44 host k3s[7262]:         loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
Jan 13 17:45:44 host k3s[7262]:         , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
Jan 13 17:45:44 host k3s[7262]:  >
Jan 13 17:45:44 host k3s[7262]: I0113 17:45:44.500437    7262 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
Jan 13 17:45:44 host k3s[7262]: I0113 17:45:44.502339    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 17:45:45 host k3s[7262]: time="2026-01-13T17:45:45+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==========>                                        ]  8.389MB/41.29MB"
Jan 13 17:45:51 host k3s[7262]: time="2026-01-13T17:45:51+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [=============>                                     ]  6.291MB/22.7MB"
Jan 13 17:45:52 host k3s[7262]: time="2026-01-13T17:45:52+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: c993b855ee4a: Downloading [============================>                      ]  8.389MB/14.73MB"
Jan 13 17:45:52 host k3s[7262]: time="2026-01-13T17:45:52+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [============>                                      ]  5.243MB/21.69MB"
Jan 13 17:45:54 host k3s[7262]: time="2026-01-13T17:45:54+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: b36647633376: Downloading [======================>                            ]  8.389MB/18.34MB"
Jan 13 17:45:55 host k3s[7262]: time="2026-01-13T17:45:55+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==========>                                        ]  8.389MB/41.29MB"
Jan 13 17:46:01 host k3s[7262]: time="2026-01-13T17:46:01+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [=============>                                     ]  6.291MB/22.7MB"
Jan 13 17:46:02 host k3s[7262]: time="2026-01-13T17:46:02+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: c993b855ee4a: Downloading [================================>                  ]  9.437MB/14.73MB"
Jan 13 17:46:02 host k3s[7262]: time="2026-01-13T17:46:02+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [============>                                      ]  5.243MB/21.69MB"
Jan 13 17:46:04 host k3s[7262]: time="2026-01-13T17:46:04+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===========>                                       ]  9.437MB/41.29MB"
Jan 13 17:46:05 host k3s[7262]: time="2026-01-13T17:46:05+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===========>                                       ]  9.437MB/41.29MB"
Jan 13 17:46:11 host k3s[7262]: time="2026-01-13T17:46:11+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [=============>                                     ]  6.291MB/22.7MB"
Jan 13 17:46:12 host k3s[7262]: time="2026-01-13T17:46:12+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: c993b855ee4a: Downloading [================================>                  ]  9.437MB/14.73MB"
Jan 13 17:46:12 host k3s[7262]: time="2026-01-13T17:46:12+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [==============>                                    ]  6.291MB/21.69MB"
Jan 13 17:46:13 host k3s[7262]: E0113 17:46:13.173863    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 17:46:14 host k3s[7262]: time="2026-01-13T17:46:14+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: b36647633376: Downloading [=========================>                         ]  9.437MB/18.34MB"
Jan 13 17:46:14 host k3s[7262]: I0113 17:46:14.514699    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 17:46:15 host k3s[7262]: time="2026-01-13T17:46:15+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===========>                                       ]  9.437MB/41.29MB"
Jan 13 17:46:21 host k3s[7262]: time="2026-01-13T17:46:21+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [================>                                  ]   7.34MB/22.7MB"
Jan 13 17:46:22 host k3s[7262]: time="2026-01-13T17:46:22+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: c993b855ee4a: Downloading [===================================>               ]  10.49MB/14.73MB"
Jan 13 17:46:22 host k3s[7262]: time="2026-01-13T17:46:22+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [==============>                                    ]  6.291MB/21.69MB"
Jan 13 17:46:24 host k3s[7262]: time="2026-01-13T17:46:24+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: b36647633376: Downloading [=========================>                         ]  9.437MB/18.34MB"
Jan 13 17:46:25 host k3s[7262]: time="2026-01-13T17:46:25+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: b36647633376: Downloading [=========================>                         ]  9.437MB/18.34MB"
Jan 13 17:46:31 host k3s[7262]: time="2026-01-13T17:46:31+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [==================>                                ]  8.389MB/22.7MB"
Jan 13 17:46:31 host k3s[7262]: E0113 17:46:31.455369    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
Jan 13 17:46:32 host k3s[7262]: time="2026-01-13T17:46:32+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: c993b855ee4a: Downloading [===================================>               ]  10.49MB/14.73MB"
Jan 13 17:46:32 host k3s[7262]: time="2026-01-13T17:46:32+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [================>                                  ]   7.34MB/21.69MB"
Jan 13 17:46:34 host k3s[7262]: time="2026-01-13T17:46:34+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=============>                                     ]  11.53MB/41.29MB"
Jan 13 17:46:35 host k3s[7262]: time="2026-01-13T17:46:35+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: b36647633376: Downloading [============================>                      ]  10.49MB/18.34MB"
Jan 13 17:46:41 host k3s[7262]: time="2026-01-13T17:46:41+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [==================>                                ]  8.389MB/22.7MB"
Jan 13 17:46:42 host k3s[7262]: time="2026-01-13T17:46:42+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: c993b855ee4a: Downloading [=======================================>           ]  11.53MB/14.73MB"
Jan 13 17:46:42 host k3s[7262]: time="2026-01-13T17:46:42+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [================>                                  ]   7.34MB/21.69MB"
Jan 13 17:46:43 host k3s[7262]: E0113 17:46:43.196560    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 17:46:44 host k3s[7262]: time="2026-01-13T17:46:44+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=============>                                     ]  11.53MB/41.29MB"
Jan 13 17:46:44 host k3s[7262]: I0113 17:46:44.548008    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 17:46:45 host k3s[7262]: time="2026-01-13T17:46:45+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: b36647633376: Downloading [============================>                      ]  10.49MB/18.34MB"
Jan 13 17:46:51 host k3s[7262]: time="2026-01-13T17:46:51+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [====================>                              ]  9.437MB/22.7MB"
Jan 13 17:46:52 host k3s[7262]: time="2026-01-13T17:46:52+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: c993b855ee4a: Downloading [=======================================>           ]  11.53MB/14.73MB"
Jan 13 17:46:52 host k3s[7262]: time="2026-01-13T17:46:52+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [===================>                               ]  8.389MB/21.69MB"
Jan 13 17:46:54 host k3s[7262]: time="2026-01-13T17:46:54+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===============>                                   ]  12.58MB/41.29MB"
Jan 13 17:46:55 host k3s[7262]: time="2026-01-13T17:46:55+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===============>                                   ]  12.58MB/41.29MB"
Jan 13 17:47:01 host k3s[7262]: time="2026-01-13T17:47:01+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [=======================>                           ]  10.49MB/22.7MB"
Jan 13 17:47:02 host k3s[7262]: time="2026-01-13T17:47:02+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: c993b855ee4a: Downloading [==========================================>        ]  12.58MB/14.73MB"
Jan 13 17:47:02 host k3s[7262]: time="2026-01-13T17:47:02+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [=====================>                             ]  9.437MB/21.69MB"
Jan 13 17:47:04 host k3s[7262]: time="2026-01-13T17:47:04+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [================>                                  ]  13.63MB/41.29MB"
Jan 13 17:47:05 host k3s[7262]: time="2026-01-13T17:47:05+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [================>                                  ]  13.63MB/41.29MB"
Jan 13 17:47:11 host k3s[7262]: time="2026-01-13T17:47:11+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [=======================>                           ]  10.49MB/22.7MB"
Jan 13 17:47:12 host k3s[7262]: time="2026-01-13T17:47:12+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: c993b855ee4a: Downloading [==============================================>    ]  13.63MB/14.73MB"
Jan 13 17:47:12 host k3s[7262]: time="2026-01-13T17:47:12+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [=====================>                             ]  9.437MB/21.69MB"
Jan 13 17:47:13 host k3s[7262]: E0113 17:47:13.312237    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 17:47:14 host k3s[7262]: time="2026-01-13T17:47:14+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [================>                                  ]  13.63MB/41.29MB"
Jan 13 17:47:14 host k3s[7262]: I0113 17:47:14.580444    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 17:47:15 host k3s[7262]: time="2026-01-13T17:47:15+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [================>                                  ]  13.63MB/41.29MB"
Jan 13 17:47:21 host k3s[7262]: time="2026-01-13T17:47:21+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [=========================>                         ]  11.53MB/22.7MB"
Jan 13 17:47:22 host k3s[7262]: time="2026-01-13T17:47:22+01:00" level=info msg="Pulling image rancher/local-path-provisioner:v0.0.32: c993b855ee4a: Extracting 1 s"
Jan 13 17:47:22 host k3s[7262]: time="2026-01-13T17:47:22+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [========================>                          ]  10.49MB/21.69MB"
Jan 13 17:47:24 host k3s[7262]: time="2026-01-13T17:47:24+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: b36647633376: Downloading [=====================================>             ]  13.63MB/18.34MB"
Jan 13 17:47:25 host k3s[7262]: time="2026-01-13T17:47:25+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=================>                                 ]  14.68MB/41.29MB"
Jan 13 17:47:26 host k3s[7262]: time="2026-01-13T17:47:26+01:00" level=info msg="Stop pulling image rancher/local-path-provisioner:v0.0.32: Status: Downloaded newer image for rancher/local-path-provisioner:v0.0.32"
Jan 13 17:47:31 host k3s[7262]: time="2026-01-13T17:47:31+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [=========================>                         ]  11.53MB/22.7MB"
Jan 13 17:47:31 host k3s[7262]: I0113 17:47:31.323423    7262 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/local-path-provisioner-578895bd58-nv5mr" podStartSLOduration=29.480479335 podStartE2EDuration="4m47.323409803s" podCreationTimestamp="2026-01-13 17:42:44 +0100 CET" firstStartedPulling="2026-01-13 17:43:08.903060419 +0100 CET m=+41.057050833" lastFinishedPulling="2026-01-13 17:47:26.745990907 +0100 CET m=+298.899981301" observedRunningTime="2026-01-13 17:47:31.32291517 +0100 CET m=+303.476905592" watchObservedRunningTime="2026-01-13 17:47:31.323409803 +0100 CET m=+303.477400182"
Jan 13 17:47:31 host k3s[7262]: W0113 17:47:31.413423    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 17:47:31 host k3s[7262]: E0113 17:47:31.413820    7262 controller.go:146] "Unhandled Error" err=<
Jan 13 17:47:31 host k3s[7262]:         Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
Jan 13 17:47:31 host k3s[7262]:         , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
Jan 13 17:47:31 host k3s[7262]:  >
Jan 13 17:47:31 host k3s[7262]: E0113 17:47:31.414306    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
Jan 13 17:47:31 host k3s[7262]: E0113 17:47:31.454673    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
Jan 13 17:47:32 host k3s[7262]: W0113 17:47:32.414814    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 17:47:32 host k3s[7262]: E0113 17:47:32.414996    7262 controller.go:102] "Unhandled Error" err=<
Jan 13 17:47:32 host k3s[7262]:         loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
Jan 13 17:47:32 host k3s[7262]:         , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
Jan 13 17:47:32 host k3s[7262]:  >
Jan 13 17:47:32 host k3s[7262]: W0113 17:47:32.415336    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 17:47:32 host k3s[7262]: E0113 17:47:32.415366    7262 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService"
Jan 13 17:47:32 host k3s[7262]: I0113 17:47:32.416714    7262 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
Jan 13 17:47:32 host k3s[7262]: I0113 17:47:32.415025    7262 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
Jan 13 17:47:32 host k3s[7262]: time="2026-01-13T17:47:32+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [========================>                          ]  10.49MB/21.69MB"
Jan 13 17:47:34 host k3s[7262]: time="2026-01-13T17:47:34+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: b36647633376: Downloading [=====================================>             ]  13.63MB/18.34MB"
Jan 13 17:47:35 host k3s[7262]: time="2026-01-13T17:47:35+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=================>                                 ]  14.68MB/41.29MB"
Jan 13 17:47:41 host k3s[7262]: time="2026-01-13T17:47:41+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [===========================>                       ]  12.58MB/22.7MB"
Jan 13 17:47:42 host k3s[7262]: time="2026-01-13T17:47:42+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [========================>                          ]  10.49MB/21.69MB"
Jan 13 17:47:43 host k3s[7262]: E0113 17:47:43.455527    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 17:47:44 host k3s[7262]: time="2026-01-13T17:47:44+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: b36647633376: Downloading [========================================>          ]  14.68MB/18.34MB"
Jan 13 17:47:44 host k3s[7262]: I0113 17:47:44.811720    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 17:47:45 host k3s[7262]: time="2026-01-13T17:47:45+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: b36647633376: Downloading [========================================>          ]  14.68MB/18.34MB"
Jan 13 17:47:51 host k3s[7262]: time="2026-01-13T17:47:51+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [==============================>                    ]  13.63MB/22.7MB"
Jan 13 17:47:52 host k3s[7262]: time="2026-01-13T17:47:52+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [==========================>                        ]  11.53MB/21.69MB"
Jan 13 17:47:54 host k3s[7262]: time="2026-01-13T17:47:54+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===================>                               ]  15.73MB/41.29MB"
Jan 13 17:47:55 host k3s[7262]: time="2026-01-13T17:47:55+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===================>                               ]  15.73MB/41.29MB"
Jan 13 17:48:01 host k3s[7262]: time="2026-01-13T17:48:01+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [==============================>                    ]  13.63MB/22.7MB"
Jan 13 17:48:02 host k3s[7262]: time="2026-01-13T17:48:02+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [==========================>                        ]  11.53MB/21.69MB"
Jan 13 17:48:04 host k3s[7262]: time="2026-01-13T17:48:04+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [====================>                              ]  16.78MB/41.29MB"
Jan 13 17:48:05 host k3s[7262]: time="2026-01-13T17:48:05+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [====================>                              ]  16.78MB/41.29MB"
Jan 13 17:48:07 host k3s[7262]: E0113 17:48:07.465124    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.076s"
Jan 13 17:48:11 host k3s[7262]: time="2026-01-13T17:48:11+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [================================>                  ]  14.68MB/22.7MB"
Jan 13 17:48:12 host k3s[7262]: time="2026-01-13T17:48:12+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [=============================>                     ]  12.58MB/21.69MB"
Jan 13 17:48:13 host k3s[7262]: E0113 17:48:13.470280    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 17:48:14 host k3s[7262]: time="2026-01-13T17:48:14+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: b36647633376: Downloading [==========================================>        ]  15.73MB/18.34MB"
Jan 13 17:48:14 host k3s[7262]: I0113 17:48:14.833353    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 17:48:15 host k3s[7262]: time="2026-01-13T17:48:15+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [====================>                              ]  16.78MB/41.29MB"
Jan 13 17:48:21 host k3s[7262]: time="2026-01-13T17:48:21+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [==================================>                ]  15.73MB/22.7MB"
Jan 13 17:48:23 host k3s[7262]: time="2026-01-13T17:48:22+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [=============================>                     ]  12.58MB/21.69MB"
Jan 13 17:48:24 host k3s[7262]: time="2026-01-13T17:48:24+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=====================>                             ]  17.83MB/41.29MB"
Jan 13 17:48:25 host k3s[7262]: time="2026-01-13T17:48:25+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: b36647633376: Downloading [=============================================>     ]  16.78MB/18.34MB"
Jan 13 17:48:31 host k3s[7262]: time="2026-01-13T17:48:31+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [==================================>                ]  15.73MB/22.7MB"
Jan 13 17:48:31 host k3s[7262]: E0113 17:48:31.457184    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
Jan 13 17:48:32 host k3s[7262]: W0113 17:48:32.417801    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 17:48:32 host k3s[7262]: E0113 17:48:32.420173    7262 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService"
Jan 13 17:48:32 host k3s[7262]: I0113 17:48:32.420187    7262 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
Jan 13 17:48:32 host k3s[7262]: W0113 17:48:32.421615    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 17:48:32 host k3s[7262]: E0113 17:48:32.421721    7262 controller.go:102] "Unhandled Error" err=<
Jan 13 17:48:32 host k3s[7262]:         loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
Jan 13 17:48:32 host k3s[7262]:         , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
Jan 13 17:48:32 host k3s[7262]:  >
Jan 13 17:48:32 host k3s[7262]: I0113 17:48:32.421729    7262 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
Jan 13 17:48:32 host k3s[7262]: time="2026-01-13T17:48:32+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [===============================>                   ]  13.63MB/21.69MB"
Jan 13 17:48:34 host k3s[7262]: time="2026-01-13T17:48:34+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=====================>                             ]  17.83MB/41.29MB"
Jan 13 17:48:35 host k3s[7262]: time="2026-01-13T17:48:35+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: b36647633376: Downloading [================================================>  ]  17.83MB/18.34MB"
Jan 13 17:48:41 host k3s[7262]: time="2026-01-13T17:48:41+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [==================================>                ]  15.73MB/22.7MB"
Jan 13 17:48:42 host k3s[7262]: time="2026-01-13T17:48:42+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [===============================>                   ]  13.63MB/21.69MB"
Jan 13 17:48:43 host k3s[7262]: E0113 17:48:43.484968    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 17:48:44 host k3s[7262]: time="2026-01-13T17:48:44+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [======================>                            ]  18.87MB/41.29MB"
Jan 13 17:48:44 host k3s[7262]: I0113 17:48:44.876280    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 17:48:45 host k3s[7262]: time="2026-01-13T17:48:45+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [======================>                            ]  18.87MB/41.29MB"
Jan 13 17:48:51 host k3s[7262]: time="2026-01-13T17:48:51+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [====================================>              ]  16.78MB/22.7MB"
Jan 13 17:48:52 host k3s[7262]: time="2026-01-13T17:48:52+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [=================================>                 ]  14.68MB/21.69MB"
Jan 13 17:48:54 host k3s[7262]: time="2026-01-13T17:48:54+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [========================>                          ]  19.92MB/41.29MB"
Jan 13 17:48:55 host k3s[7262]: time="2026-01-13T17:48:55+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [========================>                          ]  19.92MB/41.29MB"
Jan 13 17:49:01 host k3s[7262]: time="2026-01-13T17:49:01+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [====================================>              ]  16.78MB/22.7MB"
Jan 13 17:49:02 host k3s[7262]: time="2026-01-13T17:49:02+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [=================================>                 ]  14.68MB/21.69MB"
Jan 13 17:49:04 host k3s[7262]: time="2026-01-13T17:49:04+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [========================>                          ]  19.92MB/41.29MB"
Jan 13 17:49:05 host k3s[7262]: time="2026-01-13T17:49:05+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [========================>                          ]  19.92MB/41.29MB"
Jan 13 17:49:11 host k3s[7262]: time="2026-01-13T17:49:11+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [=======================================>           ]  17.83MB/22.7MB"
Jan 13 17:49:12 host k3s[7262]: time="2026-01-13T17:49:12+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [=================================>                 ]  14.68MB/21.69MB"
Jan 13 17:49:13 host k3s[7262]: E0113 17:49:13.496120    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 17:49:14 host k3s[7262]: time="2026-01-13T17:49:14+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=========================>                         ]  20.97MB/41.29MB"
Jan 13 17:49:14 host k3s[7262]: I0113 17:49:14.895620    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 17:49:15 host k3s[7262]: time="2026-01-13T17:49:15+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=========================>                         ]  20.97MB/41.29MB"
Jan 13 17:49:21 host k3s[7262]: time="2026-01-13T17:49:21+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [=======================================>           ]  17.83MB/22.7MB"
Jan 13 17:49:22 host k3s[7262]: time="2026-01-13T17:49:22+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [====================================>              ]  15.73MB/21.69MB"
Jan 13 17:49:24 host k3s[7262]: time="2026-01-13T17:49:24+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=========================>                         ]  20.97MB/41.29MB"
Jan 13 17:49:25 host k3s[7262]: time="2026-01-13T17:49:25+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=========================>                         ]  20.97MB/41.29MB"
Jan 13 17:49:31 host k3s[7262]: time="2026-01-13T17:49:31+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [=========================================>         ]  18.87MB/22.7MB"
Jan 13 17:49:31 host k3s[7262]: E0113 17:49:31.454692    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
Jan 13 17:49:32 host k3s[7262]: time="2026-01-13T17:49:32+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [======================================>            ]  16.78MB/21.69MB"
Jan 13 17:49:34 host k3s[7262]: time="2026-01-13T17:49:34+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==========================>                        ]  22.02MB/41.29MB"
Jan 13 17:49:35 host k3s[7262]: time="2026-01-13T17:49:35+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==========================>                        ]  22.02MB/41.29MB"
Jan 13 17:49:41 host k3s[7262]: time="2026-01-13T17:49:41+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [=========================================>         ]  18.87MB/22.7MB"
Jan 13 17:49:42 host k3s[7262]: time="2026-01-13T17:49:42+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [======================================>            ]  16.78MB/21.69MB"
Jan 13 17:49:43 host k3s[7262]: E0113 17:49:43.508152    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 17:49:44 host k3s[7262]: time="2026-01-13T17:49:44+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==========================>                        ]  22.02MB/41.29MB"
Jan 13 17:49:44 host k3s[7262]: I0113 17:49:44.923138    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 17:49:45 host k3s[7262]: time="2026-01-13T17:49:45+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==========================>                        ]  22.02MB/41.29MB"
Jan 13 17:49:51 host k3s[7262]: time="2026-01-13T17:49:51+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [===========================================>       ]  19.92MB/22.7MB"
Jan 13 17:49:52 host k3s[7262]: time="2026-01-13T17:49:52+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [=========================================>         ]  17.83MB/21.69MB"
Jan 13 17:49:54 host k3s[7262]: time="2026-01-13T17:49:54+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===========================>                       ]  23.07MB/41.29MB"
Jan 13 17:49:55 host k3s[7262]: time="2026-01-13T17:49:55+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===========================>                       ]  23.07MB/41.29MB"
Jan 13 17:50:01 host k3s[7262]: time="2026-01-13T17:50:01+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [===========================================>       ]  19.92MB/22.7MB"
Jan 13 17:50:02 host k3s[7262]: time="2026-01-13T17:50:02+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [=========================================>         ]  17.83MB/21.69MB"
Jan 13 17:50:04 host k3s[7262]: time="2026-01-13T17:50:04+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===========================>                       ]  23.07MB/41.29MB"
Jan 13 17:50:05 host k3s[7262]: time="2026-01-13T17:50:05+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===========================>                       ]  23.07MB/41.29MB"
Jan 13 17:50:11 host k3s[7262]: time="2026-01-13T17:50:11+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [==============================================>    ]  20.97MB/22.7MB"
Jan 13 17:50:12 host k3s[7262]: time="2026-01-13T17:50:12+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [===========================================>       ]  18.87MB/21.69MB"
Jan 13 17:50:13 host k3s[7262]: E0113 17:50:13.623080    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 17:50:14 host k3s[7262]: time="2026-01-13T17:50:14+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===========================>                       ]  23.07MB/41.29MB"
Jan 13 17:50:15 host k3s[7262]: time="2026-01-13T17:50:15+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===========================>                       ]  23.07MB/41.29MB"
Jan 13 17:50:15 host k3s[7262]: I0113 17:50:15.049369    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 17:50:21 host k3s[7262]: time="2026-01-13T17:50:21+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Downloading [==============================================>    ]  20.97MB/22.7MB"
Jan 13 17:50:22 host k3s[7262]: time="2026-01-13T17:50:22+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [===========================================>       ]  18.87MB/21.69MB"
Jan 13 17:50:24 host k3s[7262]: time="2026-01-13T17:50:24+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=============================>                     ]  24.12MB/41.29MB"
Jan 13 17:50:25 host k3s[7262]: time="2026-01-13T17:50:25+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=============================>                     ]  24.12MB/41.29MB"
Jan 13 17:50:31 host k3s[7262]: time="2026-01-13T17:50:31+01:00" level=info msg="Pulling image rancher/mirrored-coredns-coredns:1.13.1: 80579ede8985: Extracting 2 s"
Jan 13 17:50:31 host k3s[7262]: E0113 17:50:31.454514    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
Jan 13 17:50:32 host k3s[7262]: W0113 17:50:32.421551    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 17:50:32 host k3s[7262]: E0113 17:50:32.421614    7262 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService"
Jan 13 17:50:32 host k3s[7262]: I0113 17:50:32.421633    7262 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
Jan 13 17:50:32 host k3s[7262]: W0113 17:50:32.422497    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 17:50:32 host k3s[7262]: E0113 17:50:32.422656    7262 controller.go:102] "Unhandled Error" err=<
Jan 13 17:50:32 host k3s[7262]:         loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
Jan 13 17:50:32 host k3s[7262]:         , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
Jan 13 17:50:32 host k3s[7262]:  >
Jan 13 17:50:32 host k3s[7262]: I0113 17:50:32.422670    7262 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
Jan 13 17:50:32 host k3s[7262]: time="2026-01-13T17:50:32+01:00" level=info msg="Stop pulling image rancher/mirrored-coredns-coredns:1.13.1: Status: Downloaded newer image for rancher/mirrored-coredns-coredns:1.13.1"
Jan 13 17:50:32 host k3s[7262]: time="2026-01-13T17:50:32+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [===========================================>       ]  18.87MB/21.69MB"
Jan 13 17:50:34 host k3s[7262]: time="2026-01-13T17:50:34+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=============================>                     ]  24.12MB/41.29MB"
Jan 13 17:50:35 host k3s[7262]: time="2026-01-13T17:50:35+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=============================>                     ]  24.12MB/41.29MB"
Jan 13 17:50:36 host k3s[7262]: I0113 17:50:36.253039    7262 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-7f496c8d7d-6jpfv" podStartSLOduration=26.855743121 podStartE2EDuration="7m52.253016465s" podCreationTimestamp="2026-01-13 17:42:44 +0100 CET" firstStartedPulling="2026-01-13 17:43:07.957109024 +0100 CET m=+40.111099414" lastFinishedPulling="2026-01-13 17:50:33.354382366 +0100 CET m=+485.508372758" observedRunningTime="2026-01-13 17:50:36.242583185 +0100 CET m=+488.396573658" watchObservedRunningTime="2026-01-13 17:50:36.253016465 +0100 CET m=+488.407006887"
Jan 13 17:50:42 host k3s[7262]: time="2026-01-13T17:50:42+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [=============================================>     ]  19.92MB/21.69MB"
Jan 13 17:50:43 host k3s[7262]: E0113 17:50:43.650292    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 17:50:44 host k3s[7262]: time="2026-01-13T17:50:44+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==============================>                    ]  25.17MB/41.29MB"
Jan 13 17:50:45 host k3s[7262]: time="2026-01-13T17:50:45+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==============================>                    ]  25.17MB/41.29MB"
Jan 13 17:50:45 host k3s[7262]: I0113 17:50:45.091323    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 17:50:52 host k3s[7262]: time="2026-01-13T17:50:52+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [================================================>  ]  20.97MB/21.69MB"
Jan 13 17:50:54 host k3s[7262]: time="2026-01-13T17:50:54+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==============================>                    ]  25.17MB/41.29MB"
Jan 13 17:50:55 host k3s[7262]: time="2026-01-13T17:50:55+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==============================>                    ]  25.17MB/41.29MB"
Jan 13 17:51:02 host k3s[7262]: time="2026-01-13T17:51:02+01:00" level=info msg="Pulling image rancher/mirrored-metrics-server:v0.8.0: 6b6c881bc207: Downloading [================================================>  ]  20.97MB/21.69MB"
Jan 13 17:51:04 host k3s[7262]: time="2026-01-13T17:51:04+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===============================>                   ]  26.21MB/41.29MB"
Jan 13 17:51:05 host k3s[7262]: time="2026-01-13T17:51:05+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===============================>                   ]  26.21MB/41.29MB"
Jan 13 17:51:12 host k3s[7262]: time="2026-01-13T17:51:12+01:00" level=info msg="Stop pulling image rancher/mirrored-metrics-server:v0.8.0: Status: Downloaded newer image for rancher/mirrored-metrics-server:v0.8.0"
Jan 13 17:51:13 host k3s[7262]: E0113 17:51:13.770954    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 17:51:14 host k3s[7262]: time="2026-01-13T17:51:14+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===============================>                   ]  26.21MB/41.29MB"
Jan 13 17:51:15 host k3s[7262]: time="2026-01-13T17:51:15+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===============================>                   ]  26.21MB/41.29MB"
Jan 13 17:51:15 host k3s[7262]: I0113 17:51:15.124345    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 17:51:17 host k3s[7262]: I0113 17:51:17.126855    7262 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/metrics-server-7b9c9c4b9c-pc55q" podStartSLOduration=29.797856169 podStartE2EDuration="8m33.126832781s" podCreationTimestamp="2026-01-13 17:42:44 +0100 CET" firstStartedPulling="2026-01-13 17:43:09.516100308 +0100 CET m=+41.670090700" lastFinishedPulling="2026-01-13 17:51:12.845076932 +0100 CET m=+524.999067312" observedRunningTime="2026-01-13 17:51:17.126177018 +0100 CET m=+529.280167459" watchObservedRunningTime="2026-01-13 17:51:17.126832781 +0100 CET m=+529.280823201"
Jan 13 17:51:24 host k3s[7262]: time="2026-01-13T17:51:24+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===============================>                   ]  26.21MB/41.29MB"
Jan 13 17:51:25 host k3s[7262]: time="2026-01-13T17:51:25+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===============================>                   ]  26.21MB/41.29MB"
Jan 13 17:51:31 host k3s[7262]: E0113 17:51:31.459714    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
Jan 13 17:51:33 host k3s[7262]: I0113 17:51:33.760387    7262 handler.go:285] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
Jan 13 17:51:34 host k3s[7262]: time="2026-01-13T17:51:34+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=================================>                 ]  27.26MB/41.29MB"
Jan 13 17:51:35 host k3s[7262]: time="2026-01-13T17:51:35+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=================================>                 ]  27.26MB/41.29MB"
Jan 13 17:51:44 host k3s[7262]: time="2026-01-13T17:51:44+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=================================>                 ]  27.26MB/41.29MB"
Jan 13 17:51:45 host k3s[7262]: time="2026-01-13T17:51:45+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=================================>                 ]  27.26MB/41.29MB"
Jan 13 17:51:54 host k3s[7262]: time="2026-01-13T17:51:54+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==================================>                ]  28.31MB/41.29MB"
Jan 13 17:51:55 host k3s[7262]: time="2026-01-13T17:51:55+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==================================>                ]  28.31MB/41.29MB"
Jan 13 17:52:04 host k3s[7262]: time="2026-01-13T17:52:04+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==================================>                ]  28.31MB/41.29MB"
Jan 13 17:52:05 host k3s[7262]: time="2026-01-13T17:52:05+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==================================>                ]  28.31MB/41.29MB"
Jan 13 17:52:14 host k3s[7262]: time="2026-01-13T17:52:14+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===================================>               ]  29.36MB/41.29MB"
Jan 13 17:52:15 host k3s[7262]: time="2026-01-13T17:52:15+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===================================>               ]  29.36MB/41.29MB"
Jan 13 17:52:24 host k3s[7262]: time="2026-01-13T17:52:24+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===================================>               ]  29.36MB/41.29MB"
Jan 13 17:52:25 host k3s[7262]: time="2026-01-13T17:52:25+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===================================>               ]  29.36MB/41.29MB"
Jan 13 17:52:31 host k3s[7262]: I0113 17:52:31.380007    7262 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.43.0.0/16
Jan 13 17:52:34 host k3s[7262]: time="2026-01-13T17:52:34+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===================================>               ]  29.36MB/41.29MB"
Jan 13 17:52:35 host k3s[7262]: time="2026-01-13T17:52:35+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===================================>               ]  29.36MB/41.29MB"
Jan 13 17:52:44 host k3s[7262]: time="2026-01-13T17:52:44+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [====================================>              ]  30.41MB/41.29MB"
Jan 13 17:52:45 host k3s[7262]: time="2026-01-13T17:52:45+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [====================================>              ]  30.41MB/41.29MB"
Jan 13 17:52:54 host k3s[7262]: time="2026-01-13T17:52:54+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [====================================>              ]  30.41MB/41.29MB"
Jan 13 17:52:55 host k3s[7262]: time="2026-01-13T17:52:55+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [====================================>              ]  30.41MB/41.29MB"
Jan 13 17:53:04 host k3s[7262]: time="2026-01-13T17:53:04+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [======================================>            ]  31.46MB/41.29MB"
Jan 13 17:53:05 host k3s[7262]: time="2026-01-13T17:53:05+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [======================================>            ]  31.46MB/41.29MB"
Jan 13 17:53:14 host k3s[7262]: time="2026-01-13T17:53:14+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=======================================>           ]  32.51MB/41.29MB"
Jan 13 17:53:15 host k3s[7262]: time="2026-01-13T17:53:15+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=======================================>           ]  32.51MB/41.29MB"
Jan 13 17:53:24 host k3s[7262]: time="2026-01-13T17:53:24+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=======================================>           ]  32.51MB/41.29MB"
Jan 13 17:53:25 host k3s[7262]: time="2026-01-13T17:53:25+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=======================================>           ]  32.51MB/41.29MB"
Jan 13 17:53:34 host k3s[7262]: time="2026-01-13T17:53:34+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [========================================>          ]  33.55MB/41.29MB"
Jan 13 17:53:35 host k3s[7262]: time="2026-01-13T17:53:35+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [========================================>          ]  33.55MB/41.29MB"
Jan 13 17:53:44 host k3s[7262]: time="2026-01-13T17:53:44+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [========================================>          ]  33.55MB/41.29MB"
Jan 13 17:53:45 host k3s[7262]: time="2026-01-13T17:53:45+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [========================================>          ]  33.55MB/41.29MB"
Jan 13 17:53:54 host k3s[7262]: time="2026-01-13T17:53:54+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=========================================>         ]   34.6MB/41.29MB"
Jan 13 17:53:55 host k3s[7262]: time="2026-01-13T17:53:55+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=========================================>         ]   34.6MB/41.29MB"
Jan 13 17:54:04 host k3s[7262]: time="2026-01-13T17:54:04+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=========================================>         ]   34.6MB/41.29MB"
Jan 13 17:54:05 host k3s[7262]: time="2026-01-13T17:54:05+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=========================================>         ]   34.6MB/41.29MB"
Jan 13 17:54:14 host k3s[7262]: time="2026-01-13T17:54:14+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===========================================>       ]  35.65MB/41.29MB"
Jan 13 17:54:15 host k3s[7262]: time="2026-01-13T17:54:15+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [===========================================>       ]  35.65MB/41.29MB"
Jan 13 17:54:24 host k3s[7262]: time="2026-01-13T17:54:24+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [============================================>      ]   36.7MB/41.29MB"
Jan 13 17:54:25 host k3s[7262]: time="2026-01-13T17:54:25+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [============================================>      ]   36.7MB/41.29MB"
Jan 13 17:54:34 host k3s[7262]: time="2026-01-13T17:54:34+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [============================================>      ]   36.7MB/41.29MB"
Jan 13 17:54:35 host k3s[7262]: time="2026-01-13T17:54:35+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [============================================>      ]   36.7MB/41.29MB"
Jan 13 17:54:44 host k3s[7262]: time="2026-01-13T17:54:44+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=============================================>     ]  37.75MB/41.29MB"
Jan 13 17:54:45 host k3s[7262]: time="2026-01-13T17:54:45+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=============================================>     ]  37.75MB/41.29MB"
Jan 13 17:54:54 host k3s[7262]: time="2026-01-13T17:54:54+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=============================================>     ]  37.75MB/41.29MB"
Jan 13 17:54:55 host k3s[7262]: time="2026-01-13T17:54:55+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=============================================>     ]  37.75MB/41.29MB"
Jan 13 17:55:04 host k3s[7262]: time="2026-01-13T17:55:04+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=============================================>     ]  37.75MB/41.29MB"
Jan 13 17:55:05 host k3s[7262]: time="2026-01-13T17:55:05+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=============================================>     ]  37.75MB/41.29MB"
Jan 13 17:55:14 host k3s[7262]: time="2026-01-13T17:55:14+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==============================================>    ]   38.8MB/41.29MB"
Jan 13 17:55:15 host k3s[7262]: time="2026-01-13T17:55:15+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==============================================>    ]   38.8MB/41.29MB"
Jan 13 17:55:24 host k3s[7262]: time="2026-01-13T17:55:24+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==============================================>    ]   38.8MB/41.29MB"
Jan 13 17:55:25 host k3s[7262]: time="2026-01-13T17:55:25+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [==============================================>    ]   38.8MB/41.29MB"
Jan 13 17:55:34 host k3s[7262]: time="2026-01-13T17:55:34+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [================================================>  ]  39.85MB/41.29MB"
Jan 13 17:55:35 host k3s[7262]: time="2026-01-13T17:55:35+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [================================================>  ]  39.85MB/41.29MB"
Jan 13 17:55:44 host k3s[7262]: time="2026-01-13T17:55:44+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=================================================> ]  40.89MB/41.29MB"
Jan 13 17:55:45 host k3s[7262]: time="2026-01-13T17:55:45+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Downloading [=================================================> ]  40.89MB/41.29MB"
Jan 13 17:55:54 host k3s[7262]: time="2026-01-13T17:55:54+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Extracting 4 s"
Jan 13 17:55:55 host k3s[7262]: time="2026-01-13T17:55:55+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Extracting 5 s"
Jan 13 17:55:55 host k3s[7262]: E0113 17:55:55.462535    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.085s"
Jan 13 17:55:57 host k3s[7262]: E0113 17:55:57.881756    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.506s"
Jan 13 17:55:59 host k3s[7262]: E0113 17:55:59.422341    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.089s"
Jan 13 17:56:04 host k3s[7262]: time="2026-01-13T17:56:04+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: 539220d5997b: Extracting 14 s"
Jan 13 17:56:05 host k3s[7262]: time="2026-01-13T17:56:05+01:00" level=info msg="Pulling image rancher/klipper-helm:v0.9.12-build20251215: b36647633376: Extracting 1 s"
Jan 13 17:56:10 host k3s[7262]: time="2026-01-13T17:56:10+01:00" level=info msg="Stop pulling image rancher/klipper-helm:v0.9.12-build20251215: Status: Downloaded newer image for rancher/klipper-helm:v0.9.12-build20251215"
Jan 13 17:56:10 host k3s[7262]: time="2026-01-13T17:56:10+01:00" level=info msg="Stop pulling image rancher/klipper-helm:v0.9.12-build20251215: Status: Downloaded newer image for rancher/klipper-helm:v0.9.12-build20251215"
Jan 13 17:56:13 host k3s[7262]: E0113 17:56:13.997318    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.63s"
Jan 13 17:56:15 host k3s[7262]: E0113 17:56:15.339938    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.008s"
Jan 13 17:56:17 host k3s[7262]: I0113 17:56:17.131350    7262 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/helm-install-traefik-cl86v" podStartSLOduration=31.140741498 podStartE2EDuration="13m32.131320025s" podCreationTimestamp="2026-01-13 17:42:45 +0100 CET" firstStartedPulling="2026-01-13 17:43:10.043357623 +0100 CET m=+42.197348016" lastFinishedPulling="2026-01-13 17:56:11.033936129 +0100 CET m=+823.187926543" observedRunningTime="2026-01-13 17:56:17.117430889 +0100 CET m=+829.271421312" watchObservedRunningTime="2026-01-13 17:56:17.131320025 +0100 CET m=+829.285310446"
Jan 13 17:56:17 host k3s[7262]: E0113 17:56:17.643660    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.312s"
Jan 13 17:56:18 host k3s[7262]: I0113 17:56:18.208728    7262 event.go:389] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
Jan 13 17:56:18 host k3s[7262]: I0113 17:56:18.451097    7262 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/helm-install-traefik-crd-9zrsc" podStartSLOduration=31.961932917 podStartE2EDuration="13m33.451070812s" podCreationTimestamp="2026-01-13 17:42:45 +0100 CET" firstStartedPulling="2026-01-13 17:43:09.815521912 +0100 CET m=+41.969512303" lastFinishedPulling="2026-01-13 17:56:11.304659809 +0100 CET m=+823.458650198" observedRunningTime="2026-01-13 17:56:18.423265832 +0100 CET m=+830.577256242" watchObservedRunningTime="2026-01-13 17:56:18.451070812 +0100 CET m=+830.605061233"
Jan 13 17:56:19 host k3s[7262]: I0113 17:56:19.588246    7262 event.go:389] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
Jan 13 17:56:20 host k3s[7262]: I0113 17:56:20.522654    7262 scope.go:117] "RemoveContainer" containerID="5e4fe20047c3eded99e384ff383eda16d3bf38af9f763f0d76014a402616da02"
Jan 13 17:56:22 host k3s[7262]: I0113 17:56:22.476883    7262 event.go:389] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
Jan 13 17:56:24 host k3s[7262]: E0113 17:56:24.167437    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.769s"
Jan 13 17:56:24 host k3s[7262]: I0113 17:56:24.312999    7262 handler.go:285] Adding GroupVersion hub.traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:24 host k3s[7262]: I0113 17:56:24.484626    7262 handler.go:285] Adding GroupVersion hub.traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:24 host k3s[7262]: I0113 17:56:24.677058    7262 handler.go:285] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:24 host k3s[7262]: I0113 17:56:24.840900    7262 handler.go:285] Adding GroupVersion hub.traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:25 host k3s[7262]: I0113 17:56:25.367158    7262 handler.go:285] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:25 host k3s[7262]: I0113 17:56:25.480017    7262 handler.go:285] Adding GroupVersion hub.traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:25 host k3s[7262]: I0113 17:56:25.740083    7262 handler.go:285] Adding GroupVersion hub.traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:25 host k3s[7262]: I0113 17:56:25.923812    7262 handler.go:285] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:26 host k3s[7262]: I0113 17:56:26.113812    7262 handler.go:285] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:26 host k3s[7262]: I0113 17:56:26.178104    7262 handler.go:285] Adding GroupVersion hub.traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:26 host k3s[7262]: I0113 17:56:26.601231    7262 handler.go:285] Adding GroupVersion gateway.networking.k8s.io v1 to ResourceManager
Jan 13 17:56:26 host k3s[7262]: I0113 17:56:26.608888    7262 handler.go:285] Adding GroupVersion gateway.networking.k8s.io v1beta1 to ResourceManager
Jan 13 17:56:26 host k3s[7262]: I0113 17:56:26.816719    7262 handler.go:285] Adding GroupVersion hub.traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:26 host k3s[7262]: I0113 17:56:26.930938    7262 handler.go:285] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:27 host k3s[7262]: I0113 17:56:27.080772    7262 handler.go:285] Adding GroupVersion gateway.networking.k8s.io v1 to ResourceManager
Jan 13 17:56:27 host k3s[7262]: I0113 17:56:27.081157    7262 handler.go:285] Adding GroupVersion gateway.networking.k8s.io v1beta1 to ResourceManager
Jan 13 17:56:27 host k3s[7262]: I0113 17:56:27.133327    7262 handler.go:285] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:27 host k3s[7262]: I0113 17:56:27.221494    7262 handler.go:285] Adding GroupVersion hub.traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:27 host k3s[7262]: I0113 17:56:27.282641    7262 handler.go:285] Adding GroupVersion gateway.networking.k8s.io v1beta1 to ResourceManager
Jan 13 17:56:27 host k3s[7262]: I0113 17:56:27.334688    7262 handler.go:285] Adding GroupVersion hub.traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:27 host k3s[7262]: I0113 17:56:27.681941    7262 handler.go:285] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:28 host k3s[7262]: I0113 17:56:28.124755    7262 handler.go:285] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:28 host k3s[7262]: I0113 17:56:28.143593    7262 scope.go:117] "RemoveContainer" containerID="7dddcda999a36d21239fd3f1ee912b1609dca9bc21fe7df19c2593d85e02e3c5"
Jan 13 17:56:28 host k3s[7262]: E0113 17:56:28.145585    7262 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm\" with CrashLoopBackOff: \"back-off 10s restarting failed container=helm pod=helm-install-traefik-cl86v_kube-system(56a80668-4ed3-48cd-9b3d-3c00d69a1d30)\"" pod="kube-system/helm-install-traefik-cl86v" podUID="56a80668-4ed3-48cd-9b3d-3c00d69a1d30"
Jan 13 17:56:28 host k3s[7262]: I0113 17:56:28.473430    7262 handler.go:285] Adding GroupVersion hub.traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:28 host k3s[7262]: I0113 17:56:28.691637    7262 handler.go:285] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:28 host k3s[7262]: I0113 17:56:28.729386    7262 handler.go:285] Adding GroupVersion hub.traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:28 host k3s[7262]: I0113 17:56:28.955081    7262 handler.go:285] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:29 host k3s[7262]: I0113 17:56:29.009730    7262 handler.go:285] Adding GroupVersion hub.traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:29 host k3s[7262]: I0113 17:56:29.055904    7262 handler.go:285] Adding GroupVersion hub.traefik.io v1alpha1 to ResourceManager
Jan 13 17:56:29 host k3s[7262]: I0113 17:56:29.109570    7262 handler.go:285] Adding GroupVersion gateway.networking.k8s.io v1 to ResourceManager
Jan 13 17:56:29 host k3s[7262]: I0113 17:56:29.330911    7262 handler.go:285] Adding GroupVersion gateway.networking.k8s.io v1 to ResourceManager
Jan 13 17:56:29 host k3s[7262]: I0113 17:56:29.331080    7262 handler.go:285] Adding GroupVersion gateway.networking.k8s.io v1beta1 to ResourceManager
Jan 13 17:56:30 host k3s[7262]: I0113 17:56:30.301219    7262 scope.go:117] "RemoveContainer" containerID="5e4fe20047c3eded99e384ff383eda16d3bf38af9f763f0d76014a402616da02"
Jan 13 17:56:30 host k3s[7262]: I0113 17:56:30.302062    7262 scope.go:117] "RemoveContainer" containerID="7dddcda999a36d21239fd3f1ee912b1609dca9bc21fe7df19c2593d85e02e3c5"
Jan 13 17:56:30 host k3s[7262]: E0113 17:56:30.304383    7262 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm\" with CrashLoopBackOff: \"back-off 10s restarting failed container=helm pod=helm-install-traefik-cl86v_kube-system(56a80668-4ed3-48cd-9b3d-3c00d69a1d30)\"" pod="kube-system/helm-install-traefik-cl86v" podUID="56a80668-4ed3-48cd-9b3d-3c00d69a1d30"
Jan 13 17:56:31 host k3s[7262]: I0113 17:56:31.918948    7262 event.go:389] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
Jan 13 17:56:32 host k3s[7262]: I0113 17:56:32.547226    7262 scope.go:117] "RemoveContainer" containerID="7dddcda999a36d21239fd3f1ee912b1609dca9bc21fe7df19c2593d85e02e3c5"
Jan 13 17:56:32 host k3s[7262]: E0113 17:56:32.551261    7262 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm\" with CrashLoopBackOff: \"back-off 10s restarting failed container=helm pod=helm-install-traefik-cl86v_kube-system(56a80668-4ed3-48cd-9b3d-3c00d69a1d30)\"" pod="kube-system/helm-install-traefik-cl86v" podUID="56a80668-4ed3-48cd-9b3d-3c00d69a1d30"
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.104543    7262 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-5hlcz\" (UniqueName: \"kubernetes.io/projected/b37262ee-f031-4d9c-98b0-992ee8fb875d-kube-api-access-5hlcz\") pod \"b37262ee-f031-4d9c-98b0-992ee8fb875d\" (UID: \"b37262ee-f031-4d9c-98b0-992ee8fb875d\") "
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.105183    7262 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/b37262ee-f031-4d9c-98b0-992ee8fb875d-content\") pod \"b37262ee-f031-4d9c-98b0-992ee8fb875d\" (UID: \"b37262ee-f031-4d9c-98b0-992ee8fb875d\") "
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.105420    7262 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/b37262ee-f031-4d9c-98b0-992ee8fb875d-klipper-config\") pod \"b37262ee-f031-4d9c-98b0-992ee8fb875d\" (UID: \"b37262ee-f031-4d9c-98b0-992ee8fb875d\") "
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.105460    7262 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/b37262ee-f031-4d9c-98b0-992ee8fb875d-klipper-helm\") pod \"b37262ee-f031-4d9c-98b0-992ee8fb875d\" (UID: \"b37262ee-f031-4d9c-98b0-992ee8fb875d\") "
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.105485    7262 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/b37262ee-f031-4d9c-98b0-992ee8fb875d-tmp\") pod \"b37262ee-f031-4d9c-98b0-992ee8fb875d\" (UID: \"b37262ee-f031-4d9c-98b0-992ee8fb875d\") "
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.105518    7262 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/b37262ee-f031-4d9c-98b0-992ee8fb875d-klipper-cache\") pod \"b37262ee-f031-4d9c-98b0-992ee8fb875d\" (UID: \"b37262ee-f031-4d9c-98b0-992ee8fb875d\") "
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.110964    7262 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/projected/b37262ee-f031-4d9c-98b0-992ee8fb875d-values\") pod \"b37262ee-f031-4d9c-98b0-992ee8fb875d\" (UID: \"b37262ee-f031-4d9c-98b0-992ee8fb875d\") "
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.120118    7262 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/b37262ee-f031-4d9c-98b0-992ee8fb875d-content" (OuterVolumeSpecName: "content") pod "b37262ee-f031-4d9c-98b0-992ee8fb875d" (UID: "b37262ee-f031-4d9c-98b0-992ee8fb875d"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGIDValue ""
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.139765    7262 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/b37262ee-f031-4d9c-98b0-992ee8fb875d-klipper-helm" (OuterVolumeSpecName: "klipper-helm") pod "b37262ee-f031-4d9c-98b0-992ee8fb875d" (UID: "b37262ee-f031-4d9c-98b0-992ee8fb875d"). InnerVolumeSpecName "klipper-helm". PluginName "kubernetes.io/empty-dir", VolumeGIDValue ""
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.141879    7262 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/b37262ee-f031-4d9c-98b0-992ee8fb875d-values" (OuterVolumeSpecName: "values") pod "b37262ee-f031-4d9c-98b0-992ee8fb875d" (UID: "b37262ee-f031-4d9c-98b0-992ee8fb875d"). InnerVolumeSpecName "values". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.144866    7262 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/b37262ee-f031-4d9c-98b0-992ee8fb875d-tmp" (OuterVolumeSpecName: "tmp") pod "b37262ee-f031-4d9c-98b0-992ee8fb875d" (UID: "b37262ee-f031-4d9c-98b0-992ee8fb875d"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGIDValue ""
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.181266    7262 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/b37262ee-f031-4d9c-98b0-992ee8fb875d-klipper-config" (OuterVolumeSpecName: "klipper-config") pod "b37262ee-f031-4d9c-98b0-992ee8fb875d" (UID: "b37262ee-f031-4d9c-98b0-992ee8fb875d"). InnerVolumeSpecName "klipper-config". PluginName "kubernetes.io/empty-dir", VolumeGIDValue ""
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.176884    7262 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/b37262ee-f031-4d9c-98b0-992ee8fb875d-kube-api-access-5hlcz" (OuterVolumeSpecName: "kube-api-access-5hlcz") pod "b37262ee-f031-4d9c-98b0-992ee8fb875d" (UID: "b37262ee-f031-4d9c-98b0-992ee8fb875d"). InnerVolumeSpecName "kube-api-access-5hlcz". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.181536    7262 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/b37262ee-f031-4d9c-98b0-992ee8fb875d-klipper-cache" (OuterVolumeSpecName: "klipper-cache") pod "b37262ee-f031-4d9c-98b0-992ee8fb875d" (UID: "b37262ee-f031-4d9c-98b0-992ee8fb875d"). InnerVolumeSpecName "klipper-cache". PluginName "kubernetes.io/empty-dir", VolumeGIDValue ""
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.212859    7262 reconciler_common.go:299] "Volume detached for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/b37262ee-f031-4d9c-98b0-992ee8fb875d-klipper-cache\") on node \"host\" DevicePath \"\""
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.215898    7262 reconciler_common.go:299] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/projected/b37262ee-f031-4d9c-98b0-992ee8fb875d-values\") on node \"host\" DevicePath \"\""
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.216122    7262 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-5hlcz\" (UniqueName: \"kubernetes.io/projected/b37262ee-f031-4d9c-98b0-992ee8fb875d-kube-api-access-5hlcz\") on node \"host\" DevicePath \"\""
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.216141    7262 reconciler_common.go:299] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/b37262ee-f031-4d9c-98b0-992ee8fb875d-content\") on node \"host\" DevicePath \"\""
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.216153    7262 reconciler_common.go:299] "Volume detached for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/b37262ee-f031-4d9c-98b0-992ee8fb875d-klipper-config\") on node \"host\" DevicePath \"\""
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.216165    7262 reconciler_common.go:299] "Volume detached for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/b37262ee-f031-4d9c-98b0-992ee8fb875d-klipper-helm\") on node \"host\" DevicePath \"\""
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.216269    7262 reconciler_common.go:299] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/b37262ee-f031-4d9c-98b0-992ee8fb875d-tmp\") on node \"host\" DevicePath \"\""
Jan 13 17:56:34 host k3s[7262]: I0113 17:56:34.628791    7262 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="6d5ee5ce45493a3351cca5e79f4a0a16857a6b95a80ec62ad6e548389436935e"
Jan 13 17:56:35 host k3s[7262]: I0113 17:56:35.218073    7262 event.go:389] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
Jan 13 17:56:35 host k3s[7262]: I0113 17:56:35.721769    7262 event.go:389] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.042504    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="tlsoptions.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.044593    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="apiportals.hub.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.045557    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="aiservices.hub.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.045595    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="ingressroutes.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.045840    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="apiplans.hub.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.045866    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="tlsstores.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.045886    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="serverstransporttcps.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.045916    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="ingressroutetcps.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.045937    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="apiratelimits.hub.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.045950    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="apibundles.hub.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.045967    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="grpcroutes.gateway.networking.k8s.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.045987    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="apis.hub.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.046013    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="serverstransports.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.047580    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="httproutes.gateway.networking.k8s.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.047642    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="apiauths.hub.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.048381    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="apiversions.hub.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.048461    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="referencegrants.gateway.networking.k8s.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.048768    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="managedsubscriptions.hub.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.048911    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="apiportalauths.hub.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.049009    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="gateways.gateway.networking.k8s.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.049081    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="managedapplications.hub.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.049160    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="middlewaretcps.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.049270    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="apicatalogitems.hub.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.049405    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="traefikservices.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.049508    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="ingressrouteudps.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.049950    7262 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="middlewares.traefik.io"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.050452    7262 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
Jan 13 17:56:44 host k3s[7262]: I0113 17:56:44.599037    7262 scope.go:117] "RemoveContainer" containerID="7dddcda999a36d21239fd3f1ee912b1609dca9bc21fe7df19c2593d85e02e3c5"
Jan 13 17:56:45 host k3s[7262]: I0113 17:56:45.392601    7262 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
Jan 13 17:56:46 host k3s[7262]: I0113 17:56:46.058728    7262 shared_informer.go:356] "Caches are synced" controller="resource quota"
Jan 13 17:56:46 host k3s[7262]: I0113 17:56:46.294581    7262 shared_informer.go:356] "Caches are synced" controller="garbage collector"
Jan 13 17:56:48 host k3s[7262]: I0113 17:56:48.687261    7262 event.go:389] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
Jan 13 17:56:49 host k3s[7262]: I0113 17:56:49.165178    7262 alloc.go:328] "allocated clusterIPs" service="kube-system/traefik" clusterIPs={"IPv4":"10.43.130.254"}
Jan 13 17:56:49 host k3s[7262]: I0113 17:56:49.452805    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"data\" (UniqueName: \"kubernetes.io/empty-dir/2dd86d17-5995-4e0a-bb11-2f8901130dcf-data\") pod \"traefik-6f5f87584-6slgz\" (UID: \"2dd86d17-5995-4e0a-bb11-2f8901130dcf\") " pod="kube-system/traefik-6f5f87584-6slgz"
Jan 13 17:56:49 host k3s[7262]: I0113 17:56:49.453527    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/2dd86d17-5995-4e0a-bb11-2f8901130dcf-tmp\") pod \"traefik-6f5f87584-6slgz\" (UID: \"2dd86d17-5995-4e0a-bb11-2f8901130dcf\") " pod="kube-system/traefik-6f5f87584-6slgz"
Jan 13 17:56:49 host k3s[7262]: I0113 17:56:49.453562    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-x4974\" (UniqueName: \"kubernetes.io/projected/2dd86d17-5995-4e0a-bb11-2f8901130dcf-kube-api-access-x4974\") pod \"traefik-6f5f87584-6slgz\" (UID: \"2dd86d17-5995-4e0a-bb11-2f8901130dcf\") " pod="kube-system/traefik-6f5f87584-6slgz"
Jan 13 17:56:51 host k3s[7262]: time="2026-01-13T17:56:51+01:00" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2a83dfb105db65141308be759c8320e5dc4debf62c5c3ba30ee8487d03cbe016/resolv.conf as [nameserver 10.43.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 13 17:56:51 host k3s[7262]: map[string]interface {}{"cniVersion":"1.0.0", "forceAddress":true, "hairpinMode":true, "ipMasq":false, "ipam":map[string]interface {}{"ranges":[][]map[string]interface {}{[]map[string]interface {}{map[string]interface {}{"subnet":"10.42.0.0/24"}}}, "routes":[]types.Route{types.Route{Dst:net.IPNet{IP:net.IP{0xa, 0x2a, 0x0, 0x0}, Mask:net.IPMask{0xff, 0xff, 0x0, 0x0}}, GW:net.IP(nil), MTU:0, AdvMSS:0, Priority:0, Table:(*int)(nil), Scope:(*int)(nil)}}, "type":"host-local"}, "isDefaultGateway":true, "isGateway":true, "mtu":(*uint)(0xc0000a0bc0), "name":"cbr0", "type":"bridge"}
Jan 13 17:56:51 host k3s[7262]: delegateAdd: netconf sent to delegate plugin:
Jan 13 17:56:51 host k3s[7262]: {"cniVersion":"1.0.0","forceAddress":true,"hairpinMode":true,"ipMasq":false,"ipam":{"ranges":[[{"subnet":"10.42.0.0/24"}]],"routes":[{"dst":"10.42.0.0/16"}],"type":"host-local"},"isDefaultGateway":true,"isGateway":true,"mtu":1450,"name":"cbr0","type":"bridge"}I0113 17:56:51.587304    7262 scope.go:117] "RemoveContainer" containerID="7dddcda999a36d21239fd3f1ee912b1609dca9bc21fe7df19c2593d85e02e3c5"
Jan 13 17:56:52 host k3s[7262]: I0113 17:56:52.738036    7262 event.go:389] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.136654    7262 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-mj4bd\" (UniqueName: \"kubernetes.io/projected/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-kube-api-access-mj4bd\") pod \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\" (UID: \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\") "
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.136730    7262 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-tmp\") pod \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\" (UID: \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\") "
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.136756    7262 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/projected/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-values\") pod \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\" (UID: \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\") "
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.136782    7262 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-klipper-cache\") pod \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\" (UID: \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\") "
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.136804    7262 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-klipper-helm\") pod \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\" (UID: \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\") "
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.136826    7262 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-content\") pod \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\" (UID: \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\") "
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.136844    7262 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-klipper-config\") pod \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\" (UID: \"56a80668-4ed3-48cd-9b3d-3c00d69a1d30\") "
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.144926    7262 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-kube-api-access-mj4bd" (OuterVolumeSpecName: "kube-api-access-mj4bd") pod "56a80668-4ed3-48cd-9b3d-3c00d69a1d30" (UID: "56a80668-4ed3-48cd-9b3d-3c00d69a1d30"). InnerVolumeSpecName "kube-api-access-mj4bd". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.161308    7262 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-klipper-config" (OuterVolumeSpecName: "klipper-config") pod "56a80668-4ed3-48cd-9b3d-3c00d69a1d30" (UID: "56a80668-4ed3-48cd-9b3d-3c00d69a1d30"). InnerVolumeSpecName "klipper-config". PluginName "kubernetes.io/empty-dir", VolumeGIDValue ""
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.175957    7262 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-values" (OuterVolumeSpecName: "values") pod "56a80668-4ed3-48cd-9b3d-3c00d69a1d30" (UID: "56a80668-4ed3-48cd-9b3d-3c00d69a1d30"). InnerVolumeSpecName "values". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.178064    7262 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-content" (OuterVolumeSpecName: "content") pod "56a80668-4ed3-48cd-9b3d-3c00d69a1d30" (UID: "56a80668-4ed3-48cd-9b3d-3c00d69a1d30"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGIDValue ""
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.178533    7262 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-klipper-cache" (OuterVolumeSpecName: "klipper-cache") pod "56a80668-4ed3-48cd-9b3d-3c00d69a1d30" (UID: "56a80668-4ed3-48cd-9b3d-3c00d69a1d30"). InnerVolumeSpecName "klipper-cache". PluginName "kubernetes.io/empty-dir", VolumeGIDValue ""
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.197753    7262 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-klipper-helm" (OuterVolumeSpecName: "klipper-helm") pod "56a80668-4ed3-48cd-9b3d-3c00d69a1d30" (UID: "56a80668-4ed3-48cd-9b3d-3c00d69a1d30"). InnerVolumeSpecName "klipper-helm". PluginName "kubernetes.io/empty-dir", VolumeGIDValue ""
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.237473    7262 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-tmp" (OuterVolumeSpecName: "tmp") pod "56a80668-4ed3-48cd-9b3d-3c00d69a1d30" (UID: "56a80668-4ed3-48cd-9b3d-3c00d69a1d30"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGIDValue ""
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.237954    7262 reconciler_common.go:299] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-content\") on node \"host\" DevicePath \"\""
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.238007    7262 reconciler_common.go:299] "Volume detached for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-klipper-config\") on node \"host\" DevicePath \"\""
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.240039    7262 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-mj4bd\" (UniqueName: \"kubernetes.io/projected/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-kube-api-access-mj4bd\") on node \"host\" DevicePath \"\""
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.240181    7262 reconciler_common.go:299] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-tmp\") on node \"host\" DevicePath \"\""
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.240198    7262 reconciler_common.go:299] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/projected/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-values\") on node \"host\" DevicePath \"\""
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.240210    7262 reconciler_common.go:299] "Volume detached for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-klipper-cache\") on node \"host\" DevicePath \"\""
Jan 13 17:56:54 host k3s[7262]: I0113 17:56:54.240221    7262 reconciler_common.go:299] "Volume detached for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/56a80668-4ed3-48cd-9b3d-3c00d69a1d30-klipper-helm\") on node \"host\" DevicePath \"\""
Jan 13 17:56:55 host k3s[7262]: I0113 17:56:55.065482    7262 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2b36b3fe1bdd3067483754ee42193629a93b3a9ec0b03c112c8d9d021eb00455"
Jan 13 17:56:55 host k3s[7262]: I0113 17:56:55.298877    7262 event.go:389] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
Jan 13 17:56:55 host k3s[7262]: I0113 17:56:55.438160    7262 event.go:389] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
Jan 13 17:57:06 host k3s[7262]: time="2026-01-13T17:57:06+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: 10704978db0b: Download complete "
Jan 13 17:57:16 host k3s[7262]: time="2026-01-13T17:57:16+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: 10704978db0b: Download complete "
Jan 13 17:57:26 host k3s[7262]: time="2026-01-13T17:57:26+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: 10704978db0b: Download complete "
Jan 13 17:57:36 host k3s[7262]: time="2026-01-13T17:57:36+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: 04439cdf28f8: Pull complete "
Jan 13 17:57:46 host k3s[7262]: E0113 17:57:46.000935    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.669s"
Jan 13 17:57:46 host k3s[7262]: time="2026-01-13T17:57:46+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: 04439cdf28f8: Pull complete "
Jan 13 17:57:55 host k3s[7262]: E0113 17:57:55.783279    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.452s"
Jan 13 17:57:56 host k3s[7262]: time="2026-01-13T17:57:56+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: 04439cdf28f8: Pull complete "
Jan 13 17:57:57 host k3s[7262]: E0113 17:57:57.455310    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.122s"
Jan 13 17:58:06 host k3s[7262]: time="2026-01-13T17:58:06+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: 04439cdf28f8: Pull complete "
Jan 13 17:58:16 host k3s[7262]: time="2026-01-13T17:58:16+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: 07cae4063a61: Download complete "
Jan 13 17:58:26 host k3s[7262]: time="2026-01-13T17:58:26+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: 07cae4063a61: Download complete "
Jan 13 17:58:36 host k3s[7262]: time="2026-01-13T17:58:36+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=>                                                 ]  1.049MB/45.48MB"
Jan 13 17:58:46 host k3s[7262]: time="2026-01-13T17:58:46+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=>                                                 ]  1.049MB/45.48MB"
Jan 13 17:58:56 host k3s[7262]: time="2026-01-13T17:58:56+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [==>                                                ]  2.097MB/45.48MB"
Jan 13 17:59:06 host k3s[7262]: time="2026-01-13T17:59:06+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [==>                                                ]  2.097MB/45.48MB"
Jan 13 17:59:16 host k3s[7262]: time="2026-01-13T17:59:16+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===>                                               ]  3.146MB/45.48MB"
Jan 13 17:59:26 host k3s[7262]: time="2026-01-13T17:59:26+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===>                                               ]  3.146MB/45.48MB"
Jan 13 17:59:36 host k3s[7262]: time="2026-01-13T17:59:36+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [====>                                              ]  4.194MB/45.48MB"
Jan 13 17:59:46 host k3s[7262]: time="2026-01-13T17:59:46+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=====>                                             ]  5.243MB/45.48MB"
Jan 13 17:59:55 host k3s[7262]: E0113 17:59:55.873505    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.492s"
Jan 13 17:59:56 host k3s[7262]: time="2026-01-13T17:59:56+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=====>                                             ]  5.243MB/45.48MB"
Jan 13 18:00:06 host k3s[7262]: time="2026-01-13T18:00:06+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [======>                                            ]  6.291MB/45.48MB"
Jan 13 18:00:16 host k3s[7262]: time="2026-01-13T18:00:16+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [======>                                            ]  6.291MB/45.48MB"
Jan 13 18:00:26 host k3s[7262]: time="2026-01-13T18:00:26+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [========>                                          ]   7.34MB/45.48MB"
Jan 13 18:00:36 host k3s[7262]: time="2026-01-13T18:00:36+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=========>                                         ]  8.389MB/45.48MB"
Jan 13 18:00:46 host k3s[7262]: time="2026-01-13T18:00:46+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=========>                                         ]  8.389MB/45.48MB"
Jan 13 18:00:56 host k3s[7262]: time="2026-01-13T18:00:56+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [==========>                                        ]  9.437MB/45.48MB"
Jan 13 18:00:57 host k3s[7262]: E0113 18:00:57.932096    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.593s"
Jan 13 18:01:06 host k3s[7262]: time="2026-01-13T18:01:06+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [==========>                                        ]  9.437MB/45.48MB"
Jan 13 18:01:16 host k3s[7262]: time="2026-01-13T18:01:16+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [==========>                                        ]  9.437MB/45.48MB"
Jan 13 18:01:26 host k3s[7262]: time="2026-01-13T18:01:26+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===========>                                       ]  10.49MB/45.48MB"
Jan 13 18:01:36 host k3s[7262]: time="2026-01-13T18:01:36+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===========>                                       ]  10.49MB/45.48MB"
Jan 13 18:01:46 host k3s[7262]: time="2026-01-13T18:01:46+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [============>                                      ]  11.53MB/45.48MB"
Jan 13 18:01:56 host k3s[7262]: time="2026-01-13T18:01:56+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=============>                                     ]  12.58MB/45.48MB"
Jan 13 18:02:06 host k3s[7262]: time="2026-01-13T18:02:06+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=============>                                     ]  12.58MB/45.48MB"
Jan 13 18:02:16 host k3s[7262]: time="2026-01-13T18:02:16+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=============>                                     ]  12.58MB/45.48MB"
Jan 13 18:02:26 host k3s[7262]: time="2026-01-13T18:02:26+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [==============>                                    ]  13.63MB/45.48MB"
Jan 13 18:02:30 host k3s[7262]: time="2026-01-13T18:02:30+01:00" level=info msg="COMPACT compactRev=0 targetCompactRev=56 currentRev=1056"
Jan 13 18:02:30 host k3s[7262]: time="2026-01-13T18:02:30+01:00" level=info msg="COMPACT deleted 6 rows from 56 revisions in 4.74664ms - compacted to 56/1056"
Jan 13 18:02:30 host k3s[7262]: time="2026-01-13T18:02:30+01:00" level=info msg="COMPACT compacted from 0 to 56 in 1 transactions over 8ms"
Jan 13 18:02:31 host k3s[7262]: I0113 18:02:31.391113    7262 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.43.0.0/16
Jan 13 18:02:36 host k3s[7262]: time="2026-01-13T18:02:36+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [==============>                                    ]  13.63MB/45.48MB"
Jan 13 18:02:46 host k3s[7262]: time="2026-01-13T18:02:46+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [================>                                  ]  14.68MB/45.48MB"
Jan 13 18:02:56 host k3s[7262]: time="2026-01-13T18:02:56+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=================>                                 ]  15.73MB/45.48MB"
Jan 13 18:03:06 host k3s[7262]: time="2026-01-13T18:03:06+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=================>                                 ]  15.73MB/45.48MB"
Jan 13 18:03:16 host k3s[7262]: time="2026-01-13T18:03:16+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=================>                                 ]  15.73MB/45.48MB"
Jan 13 18:03:19 host k3s[7262]: E0113 18:03:19.483077    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.151s"
Jan 13 18:03:26 host k3s[7262]: time="2026-01-13T18:03:26+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [==================>                                ]  16.78MB/45.48MB"
Jan 13 18:03:36 host k3s[7262]: time="2026-01-13T18:03:36+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===================>                               ]  17.83MB/45.48MB"
Jan 13 18:03:46 host k3s[7262]: time="2026-01-13T18:03:46+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===================>                               ]  17.83MB/45.48MB"
Jan 13 18:03:56 host k3s[7262]: time="2026-01-13T18:03:56+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===================>                               ]  17.83MB/45.48MB"
Jan 13 18:04:06 host k3s[7262]: time="2026-01-13T18:04:06+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [====================>                              ]  18.87MB/45.48MB"
Jan 13 18:04:10 host k3s[7262]: E0113 18:04:10.009006    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.677s"
Jan 13 18:04:16 host k3s[7262]: time="2026-01-13T18:04:16+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [====================>                              ]  18.87MB/45.48MB"
Jan 13 18:04:26 host k3s[7262]: time="2026-01-13T18:04:26+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=====================>                             ]  19.92MB/45.48MB"
Jan 13 18:04:36 host k3s[7262]: time="2026-01-13T18:04:36+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=======================>                           ]  20.97MB/45.48MB"
Jan 13 18:04:46 host k3s[7262]: time="2026-01-13T18:04:46+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=======================>                           ]  20.97MB/45.48MB"
Jan 13 18:04:56 host k3s[7262]: time="2026-01-13T18:04:56+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [========================>                          ]  22.02MB/45.48MB"
Jan 13 18:05:06 host k3s[7262]: time="2026-01-13T18:05:06+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=========================>                         ]  23.07MB/45.48MB"
Jan 13 18:05:16 host k3s[7262]: time="2026-01-13T18:05:16+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=========================>                         ]  23.07MB/45.48MB"
Jan 13 18:05:26 host k3s[7262]: time="2026-01-13T18:05:26+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [==========================>                        ]  24.12MB/45.48MB"
Jan 13 18:05:36 host k3s[7262]: time="2026-01-13T18:05:36+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===========================>                       ]  25.17MB/45.48MB"
Jan 13 18:05:46 host k3s[7262]: time="2026-01-13T18:05:46+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===========================>                       ]  25.17MB/45.48MB"
Jan 13 18:05:56 host k3s[7262]: time="2026-01-13T18:05:56+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [============================>                      ]  26.21MB/45.48MB"
Jan 13 18:06:06 host k3s[7262]: time="2026-01-13T18:06:06+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [============================>                      ]  26.21MB/45.48MB"
Jan 13 18:06:17 host k3s[7262]: time="2026-01-13T18:06:17+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=============================>                     ]  27.26MB/45.48MB"
Jan 13 18:06:23 host k3s[7262]: E0113 18:06:23.903899    7262 health_controller.go:212] Metrics Controller heartbeat missed
Jan 13 18:06:26 host k3s[7262]: time="2026-01-13T18:06:26+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:06:23.799739325 +0100 CET m=+1435.953729795) (total time: 2.208555979s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=2.208555979s
Jan 13 18:06:26 host k3s[7262]: time="2026-01-13T18:06:26+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:06:23.944293262 +0100 CET m=+1436.098283695) (total time: 2.229864078s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=2.229864078s
Jan 13 18:06:27 host k3s[7262]: time="2026-01-13T18:06:27+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=============================>                     ]  27.26MB/45.48MB"
Jan 13 18:06:29 host k3s[7262]: E0113 18:06:29.016185    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="12.685s"
Jan 13 18:06:37 host k3s[7262]: time="2026-01-13T18:06:37+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:06:36.050000782 +0100 CET m=+1448.203991199) (total time: 1.104805345s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=1.104805345s
Jan 13 18:06:37 host k3s[7262]: time="2026-01-13T18:06:37+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:06:36.12214992 +0100 CET m=+1448.276140327) (total time: 1.502121705s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=1.502121705s
Jan 13 18:06:37 host k3s[7262]: time="2026-01-13T18:06:37+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:06:36.119031603 +0100 CET m=+1448.273022025) (total time: 1.060808889s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=1.060808889s
Jan 13 18:06:37 host k3s[7262]: time="2026-01-13T18:06:37+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:06:35.846357967 +0100 CET m=+1448.000348404) (total time: 1.454441005s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=1.454441005s
Jan 13 18:06:37 host k3s[7262]: time="2026-01-13T18:06:37+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===============================>                   ]  28.31MB/45.48MB"
Jan 13 18:06:37 host k3s[7262]: time="2026-01-13T18:06:37+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:06:35.680476363 +0100 CET m=+1447.834466777) (total time: 1.921041349s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=1.921041349s
Jan 13 18:06:38 host k3s[7262]: time="2026-01-13T18:06:38+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:06:36.251277152 +0100 CET m=+1448.405267560) (total time: 1.851246531s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? ESCAPE '^' AND kv.id > ? ORDER BY kv.id ASC LIMIT 500" duration=1.851246531s
Jan 13 18:06:39 host k3s[7262]: E0113 18:06:39.259526    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="8.653s"
Jan 13 18:06:47 host k3s[7262]: time="2026-01-13T18:06:47+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===============================>                   ]  28.31MB/45.48MB"
Jan 13 18:06:47 host k3s[7262]: E0113 18:06:47.912303    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.578s"
Jan 13 18:06:50 host k3s[7262]: E0113 18:06:50.664979    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.218s"
Jan 13 18:06:52 host k3s[7262]: E0113 18:06:52.469874    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.804s"
Jan 13 18:06:56 host k3s[7262]: time="2026-01-13T18:06:56+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===============================>                   ]  28.31MB/45.48MB"
Jan 13 18:07:06 host k3s[7262]: time="2026-01-13T18:07:06+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [================================>                  ]  29.36MB/45.48MB"
Jan 13 18:07:07 host k3s[7262]: E0113 18:07:07.495842    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="3.159s"
Jan 13 18:07:13 host k3s[7262]: time="2026-01-13T18:07:13+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:07:12.226606009 +0100 CET m=+1484.380596442) (total time: 1.727200286s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? ESCAPE '^' AND kv.id > ? ORDER BY kv.id ASC LIMIT 500" duration=1.727200286s
Jan 13 18:07:14 host k3s[7262]: time="2026-01-13T18:07:14+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:07:12.181791897 +0100 CET m=+1484.335782351) (total time: 1.879591872s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 10001" duration=1.879591872s
Jan 13 18:07:15 host k3s[7262]: E0113 18:07:15.584495    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="3.651s"
Jan 13 18:07:16 host k3s[7262]: time="2026-01-13T18:07:16+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [================================>                  ]  29.36MB/45.48MB"
Jan 13 18:07:16 host k3s[7262]: E0113 18:07:16.978994    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.391s"
Jan 13 18:07:27 host k3s[7262]: time="2026-01-13T18:07:27+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [================================>                  ]  29.36MB/45.48MB"
Jan 13 18:07:27 host k3s[7262]: E0113 18:07:27.961808    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.629s"
Jan 13 18:07:31 host k3s[7262]: time="2026-01-13T18:07:31+01:00" level=info msg="COMPACT compactRev=56 targetCompactRev=157 currentRev=1157"
Jan 13 18:07:31 host k3s[7262]: time="2026-01-13T18:07:31+01:00" level=info msg="COMPACT deleted 7 rows from 101 revisions in 15.13514ms - compacted to 157/1157"
Jan 13 18:07:31 host k3s[7262]: time="2026-01-13T18:07:31+01:00" level=info msg="COMPACT compacted from 56 to 157 in 1 transactions over 23ms"
Jan 13 18:07:34 host k3s[7262]: time="2026-01-13T18:07:34+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:07:31.892214331 +0100 CET m=+1504.046204741) (total time: 2.120929304s): PRAGMA wal_checkpoint(FULL)" duration=2.120929304s
Jan 13 18:07:34 host k3s[7262]: time="2026-01-13T18:07:34+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:07:32.052936312 +0100 CET m=+1504.206926731) (total time: 2.136275618s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? ESCAPE '^' AND kv.id > ? ORDER BY kv.id ASC LIMIT 500" duration=2.136275618s
Jan 13 18:07:36 host k3s[7262]: W0113 18:07:36.808676    7262 controller.go:139] slow openapi aggregation of "middlewaretcps.traefik.io": 2.436476833s
Jan 13 18:07:36 host k3s[7262]: time="2026-01-13T18:07:36+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [================================>                  ]  29.36MB/45.48MB"
Jan 13 18:07:39 host k3s[7262]: W0113 18:07:39.010854    7262 controller.go:139] slow openapi aggregation of "serverstransporttcps.traefik.io": 1.608949189s
Jan 13 18:07:40 host k3s[7262]: W0113 18:07:40.555528    7262 controller.go:139] slow openapi aggregation of "apiversions.hub.traefik.io": 1.544095219s
Jan 13 18:07:42 host k3s[7262]: W0113 18:07:42.345966    7262 controller.go:139] slow openapi aggregation of "ingressrouteudps.traefik.io": 1.110305283s
Jan 13 18:07:42 host k3s[7262]: E0113 18:07:42.594251    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="12.262s"
Jan 13 18:07:47 host k3s[7262]: time="2026-01-13T18:07:47+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=================================>                 ]  30.41MB/45.48MB"
Jan 13 18:07:50 host k3s[7262]: time="2026-01-13T18:07:50+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:07:47.382335403 +0100 CET m=+1519.536325832) (total time: 3.113755034s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? ESCAPE '^' AND kv.id > ? ORDER BY kv.id ASC LIMIT 500" duration=3.113755034s
Jan 13 18:07:50 host k3s[7262]: time="2026-01-13T18:07:50+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:07:47.295936356 +0100 CET m=+1519.449926792) (total time: 3.201691661s): INSERT INTO kine(name, created, deleted, create_revision, prev_revision, lease, value, old_value) values(?, ?, ?, ?, ?, ?, ?, ?)" duration=3.201691661s
Jan 13 18:07:51 host k3s[7262]: time="2026-01-13T18:07:51+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:07:50.636242968 +0100 CET m=+1522.790233449) (total time: 1.126770813s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=1.126770813s
Jan 13 18:07:52 host k3s[7262]: time="2026-01-13T18:07:52+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:07:50.575651215 +0100 CET m=+1522.729641654) (total time: 1.915089218s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=1.915089218s
Jan 13 18:07:54 host k3s[7262]: I0113 18:07:54.113657    7262 stats.go:143] "Error getting keys" err="Timeout: Too large resource version: 1165, current: 1164"
Jan 13 18:07:54 host k3s[7262]: time="2026-01-13T18:07:54+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:07:53.282671864 +0100 CET m=+1525.436662289) (total time: 1.024245972s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? ESCAPE '^' AND kv.id > ? ORDER BY kv.id ASC LIMIT 500" duration=1.024245972s
Jan 13 18:07:56 host k3s[7262]: time="2026-01-13T18:07:56+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=================================>                 ]  30.41MB/45.48MB"
Jan 13 18:08:03 host k3s[7262]: time="2026-01-13T18:08:03+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:08:01.855664571 +0100 CET m=+1534.009655068) (total time: 1.437256477s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=1.437256477s
Jan 13 18:08:03 host k3s[7262]: time="2026-01-13T18:08:03+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:08:01.950055349 +0100 CET m=+1534.104045799) (total time: 1.859333813s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=1.859333813s
Jan 13 18:08:05 host k3s[7262]: time="2026-01-13T18:08:05+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:08:03.788508258 +0100 CET m=+1535.942498684) (total time: 1.287253635s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 10001" duration=1.287253635s
Jan 13 18:08:06 host k3s[7262]: E0113 18:08:06.303727    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="21.962s"
Jan 13 18:08:06 host k3s[7262]: time="2026-01-13T18:08:06+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=================================>                 ]  30.41MB/45.48MB"
Jan 13 18:08:16 host k3s[7262]: E0113 18:08:16.014734    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="9.67s"
Jan 13 18:08:17 host k3s[7262]: time="2026-01-13T18:08:17+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=================================>                 ]  30.41MB/45.48MB"
Jan 13 18:08:18 host k3s[7262]: E0113 18:08:18.729640    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.697s"
Jan 13 18:08:18 host k3s[7262]: W0113 18:08:18.784764    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 18:08:18 host k3s[7262]: E0113 18:08:18.785050    7262 controller.go:146] "Unhandled Error" err=<
Jan 13 18:08:18 host k3s[7262]:         Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
Jan 13 18:08:18 host k3s[7262]:         , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
Jan 13 18:08:18 host k3s[7262]:  >
Jan 13 18:08:18 host k3s[7262]: E0113 18:08:18.785136    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
Jan 13 18:08:18 host k3s[7262]: I0113 18:08:18.785189    7262 handler.go:285] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
Jan 13 18:08:20 host k3s[7262]: I0113 18:08:20.158242    7262 handler.go:285] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
Jan 13 18:08:26 host k3s[7262]: time="2026-01-13T18:08:26+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=================================>                 ]  30.41MB/45.48MB"
Jan 13 18:08:28 host k3s[7262]: time="2026-01-13T18:08:28+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:08:26.682922525 +0100 CET m=+1558.836912943) (total time: 1.940838187s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=1.940838187s
Jan 13 18:08:28 host k3s[7262]: time="2026-01-13T18:08:28+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:08:26.875320684 +0100 CET m=+1559.029311129) (total time: 1.891799764s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 10001" duration=1.891799764s
Jan 13 18:08:28 host k3s[7262]: time="2026-01-13T18:08:28+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:08:26.526362957 +0100 CET m=+1558.680353390) (total time: 2.256225783s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 10001" duration=2.256225783s
Jan 13 18:08:28 host k3s[7262]: time="2026-01-13T18:08:28+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:08:26.897718004 +0100 CET m=+1559.051708439) (total time: 1.983784426s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=1.983784426s
Jan 13 18:08:29 host k3s[7262]: E0113 18:08:29.229221    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="3.668s"
Jan 13 18:08:37 host k3s[7262]: time="2026-01-13T18:08:37+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:08:36.050301924 +0100 CET m=+1568.204292417) (total time: 1.656818483s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? ESCAPE '^' AND kv.id > ? ORDER BY kv.id ASC LIMIT 500" duration=1.656818483s
Jan 13 18:08:38 host k3s[7262]: time="2026-01-13T18:08:38+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=================================>                 ]  30.41MB/45.48MB"
Jan 13 18:08:38 host k3s[7262]: E0113 18:08:38.492619    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="9.263s"
Jan 13 18:08:43 host k3s[7262]: time="2026-01-13T18:08:43+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:08:40.882837561 +0100 CET m=+1573.036828005) (total time: 2.521097707s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? ESCAPE '^' AND kv.id > ? ORDER BY kv.id ASC LIMIT 500" duration=2.521097707s
Jan 13 18:08:47 host k3s[7262]: time="2026-01-13T18:08:47+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=================================>                 ]  30.41MB/45.48MB"
Jan 13 18:08:48 host k3s[7262]: E0113 18:08:48.276322    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="9.783s"
Jan 13 18:08:48 host k3s[7262]: W0113 18:08:48.505882    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 18:08:48 host k3s[7262]: E0113 18:08:48.506068    7262 controller.go:146] "Unhandled Error" err=<
Jan 13 18:08:48 host k3s[7262]:         Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
Jan 13 18:08:48 host k3s[7262]:         , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
Jan 13 18:08:48 host k3s[7262]:  >
Jan 13 18:08:48 host k3s[7262]: E0113 18:08:48.506176    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
Jan 13 18:08:48 host k3s[7262]: I0113 18:08:48.506230    7262 handler.go:285] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
Jan 13 18:08:56 host k3s[7262]: time="2026-01-13T18:08:56+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=================================>                 ]  30.41MB/45.48MB"
Jan 13 18:08:58 host k3s[7262]: E0113 18:08:58.214807    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="5.883s"
Jan 13 18:08:59 host k3s[7262]: I0113 18:08:59.007659    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 18:08:59 host k3s[7262]: E0113 18:08:59.046403    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 18:08:59 host k3s[7262]: E0113 18:08:59.811446    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.09s"
Jan 13 18:09:05 host k3s[7262]: time="2026-01-13T18:09:05+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:09:03.930686586 +0100 CET m=+1596.084677025) (total time: 1.295502643s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 10001" duration=1.295502643s
Jan 13 18:09:06 host k3s[7262]: time="2026-01-13T18:09:06+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [==================================>                ]  31.46MB/45.48MB"
Jan 13 18:09:07 host k3s[7262]: E0113 18:09:07.295993    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="6.964s"
Jan 13 18:09:08 host k3s[7262]: I0113 18:09:08.577019    7262 handler.go:285] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
Jan 13 18:09:16 host k3s[7262]: time="2026-01-13T18:09:16+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [==================================>                ]  31.46MB/45.48MB"
Jan 13 18:09:26 host k3s[7262]: time="2026-01-13T18:09:26+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [==================================>                ]  31.46MB/45.48MB"
Jan 13 18:09:28 host k3s[7262]: time="2026-01-13T18:09:28+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:09:27.277328153 +0100 CET m=+1619.431318668) (total time: 1.429018018s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? ESCAPE '^' AND kv.id > ? ORDER BY kv.id ASC LIMIT 500" duration=1.429018018s
Jan 13 18:09:33 host k3s[7262]: time="2026-01-13T18:09:33+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:09:32.406055207 +0100 CET m=+1624.560045723) (total time: 1.389886894s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? ESCAPE '^' AND kv.id > ? ORDER BY kv.id ASC LIMIT 500" duration=1.389886894s
Jan 13 18:09:38 host k3s[7262]: I0113 18:09:38.236987    7262 handler.go:285] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
Jan 13 18:09:38 host k3s[7262]: time="2026-01-13T18:09:38+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===================================>               ]  32.51MB/45.48MB"
Jan 13 18:09:38 host k3s[7262]: time="2026-01-13T18:09:38+01:00" level=warning msg="Slow SQL (started: 2026-01-13 18:09:33.794618754 +0100 CET m=+1625.948609183) (total time: 5.036218167s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=5.036218167s
Jan 13 18:09:39 host k3s[7262]: E0113 18:09:39.292107    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="16.775s"
Jan 13 18:09:47 host k3s[7262]: time="2026-01-13T18:09:47+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===================================>               ]  32.51MB/45.48MB"
Jan 13 18:09:55 host k3s[7262]: time="2026-01-13T18:09:55+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:09:53.041891254 +0100 CET m=+1645.195881709) (total time: 2.343753997s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=2.343753997s
Jan 13 18:09:56 host k3s[7262]: time="2026-01-13T18:09:56+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===================================>               ]  32.51MB/45.48MB"
Jan 13 18:09:56 host k3s[7262]: E0113 18:09:56.843239    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="11.889s"
Jan 13 18:09:57 host k3s[7262]: E0113 18:09:57.999357    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.155s"
Jan 13 18:10:01 host k3s[7262]: time="2026-01-13T18:10:01+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:10:00.143143978 +0100 CET m=+1652.297134712) (total time: 1.175457703s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? ESCAPE '^' AND kv.id > ? ORDER BY kv.id ASC LIMIT 500" duration=1.175457703s
Jan 13 18:10:03 host k3s[7262]: E0113 18:10:03.718164    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 18:10:04 host k3s[7262]: I0113 18:10:04.947626    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 18:10:06 host k3s[7262]: time="2026-01-13T18:10:06+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [====================================>              ]  33.55MB/45.48MB"
Jan 13 18:10:13 host k3s[7262]: time="2026-01-13T18:10:13+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:10:11.948755753 +0100 CET m=+1664.102746231) (total time: 1.072325981s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 10001" duration=1.072325981s
Jan 13 18:10:13 host k3s[7262]: E0113 18:10:13.241217    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="12.708s"
Jan 13 18:10:14 host k3s[7262]: E0113 18:10:14.284893    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.044s"
Jan 13 18:10:14 host k3s[7262]: W0113 18:10:14.443512    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 18:10:14 host k3s[7262]: E0113 18:10:14.443918    7262 controller.go:146] "Unhandled Error" err=<
Jan 13 18:10:14 host k3s[7262]:         Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
Jan 13 18:10:14 host k3s[7262]:         , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
Jan 13 18:10:14 host k3s[7262]:  >
Jan 13 18:10:14 host k3s[7262]: E0113 18:10:14.444007    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
Jan 13 18:10:18 host k3s[7262]: time="2026-01-13T18:10:18+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [======================================>            ]   34.6MB/45.48MB"
Jan 13 18:10:19 host k3s[7262]: E0113 18:10:19.262659    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.397s"
Jan 13 18:10:23 host k3s[7262]: time="2026-01-13T18:10:23+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:10:22.112969274 +0100 CET m=+1674.266959995) (total time: 1.301566028s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? ESCAPE '^' AND kv.id > ? ORDER BY kv.id ASC LIMIT 500" duration=1.301566028s
Jan 13 18:10:24 host k3s[7262]: E0113 18:10:24.792286    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="5.53s"
Jan 13 18:10:27 host k3s[7262]: time="2026-01-13T18:10:27+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [======================================>            ]   34.6MB/45.48MB"
Jan 13 18:10:27 host k3s[7262]: E0113 18:10:27.882679    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.512s"
Jan 13 18:10:30 host k3s[7262]: E0113 18:10:30.602680    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.225s"
Jan 13 18:10:31 host k3s[7262]: E0113 18:10:31.713382    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
Jan 13 18:10:31 host k3s[7262]: E0113 18:10:31.912615    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.309s"
Jan 13 18:10:34 host k3s[7262]: E0113 18:10:34.278340    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 18:10:35 host k3s[7262]: I0113 18:10:35.363165    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 18:10:35 host k3s[7262]: E0113 18:10:35.384334    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="3.052s"
Jan 13 18:10:36 host k3s[7262]: time="2026-01-13T18:10:36+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [======================================>            ]   34.6MB/45.48MB"
Jan 13 18:10:37 host k3s[7262]: E0113 18:10:37.128069    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.744s"
Jan 13 18:10:39 host k3s[7262]: E0113 18:10:39.589506    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.257s"
Jan 13 18:10:39 host k3s[7262]: I0113 18:10:39.907256    7262 handler.go:285] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
Jan 13 18:10:47 host k3s[7262]: time="2026-01-13T18:10:47+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=======================================>           ]  35.65MB/45.48MB"
Jan 13 18:10:47 host k3s[7262]: E0113 18:10:47.774658    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.416s"
Jan 13 18:10:50 host k3s[7262]: E0113 18:10:50.436843    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.891s"
Jan 13 18:10:52 host k3s[7262]: E0113 18:10:52.243220    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.806s"
Jan 13 18:10:57 host k3s[7262]: time="2026-01-13T18:10:57+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=======================================>           ]  35.65MB/45.48MB"
Jan 13 18:11:05 host k3s[7262]: time="2026-01-13T18:11:05+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:11:03.581751171 +0100 CET m=+1715.735742065) (total time: 1.992407621s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? ESCAPE '^' AND kv.id > ? ORDER BY kv.id ASC LIMIT 500" duration=1.992407621s
Jan 13 18:11:13 host k3s[7262]: {"level":"warn","ts":"2026-01-13T18:11:13.972084+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002995c20/kine.sock","method":"/etcdserverpb.KV/Txn","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
Jan 13 18:11:13 host k3s[7262]: E0113 18:11:11.574257    7262 controller.go:195] "Failed to update lease" err="Put \"https://127.0.0.1:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/host?timeout=10s\": context deadline exceeded"
Jan 13 18:11:13 host k3s[7262]: E0113 18:11:13.979061    7262 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 10.312136ms, panicked: false, err: context canceled, panic-reason: <nil>"
Jan 13 18:11:13 host k3s[7262]: I0113 18:11:13.966834    7262 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go@v1.34.3-k3s1/tools/cache/reflector.go:290" type="*v1.ConfigMap" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
Jan 13 18:11:13 host k3s[7262]: I0113 18:11:11.574454    7262 transport.go:356] "Warning: unable to cancel request" roundTripperType="*otelhttp.Transport"
Jan 13 18:11:13 host k3s[7262]: I0113 18:11:13.966911    7262 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go@v1.34.3-k3s1/tools/cache/reflector.go:290" type="*v1.ConfigMap" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
Jan 13 18:11:13 host k3s[7262]: I0113 18:11:13.966879    7262 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go@v1.34.3-k3s1/tools/cache/reflector.go:290" type="*v1.ConfigMap" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
Jan 13 18:11:13 host k3s[7262]: E0113 18:11:13.968710    7262 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout"
Jan 13 18:11:13 host k3s[7262]: E0113 18:11:13.980725    7262 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout"
Jan 13 18:11:13 host k3s[7262]: E0113 18:11:13.980901    7262 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout"
Jan 13 18:11:13 host k3s[7262]: time="2026-01-13T18:11:13+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:11:12.52292845 +0100 CET m=+1724.676918881) (total time: 1.467238152s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=1.467238152s
Jan 13 18:11:14 host k3s[7262]: E0113 18:11:11.499094    7262 health_controller.go:212] Metrics Controller heartbeat missed
Jan 13 18:11:14 host k3s[7262]: E0113 18:11:14.013058    7262 health_controller.go:212] Metrics Controller heartbeat missed
Jan 13 18:11:14 host k3s[7262]: time="2026-01-13T18:11:13+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=======================================>           ]  35.65MB/45.48MB"
Jan 13 18:11:14 host k3s[7262]: time="2026-01-13T18:11:14+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:11:12.46069004 +0100 CET m=+1724.614680479) (total time: 1.586263904s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=1.586263904s
Jan 13 18:11:14 host k3s[7262]: E0113 18:11:14.099297    7262 server.go:329] "Unable to authenticate the request due to an error" err="context canceled"
Jan 13 18:11:14 host k3s[7262]: E0113 18:11:14.208903    7262 timeout.go:140] "Post-timeout activity" timeElapsed="240.151272ms" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/host" result=null
Jan 13 18:11:14 host k3s[7262]: time="2026-01-13T18:11:14+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:11:13.524237384 +0100 CET m=+1725.678227813) (total time: 1.07900832s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 10001" duration=1.07900832s
Jan 13 18:11:15 host k3s[7262]: E0113 18:11:15.149974    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="14.299s"
Jan 13 18:11:17 host k3s[7262]: time="2026-01-13T18:11:17+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [========================================>          ]   36.7MB/45.48MB"
Jan 13 18:11:17 host k3s[7262]: E0113 18:11:17.444338    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.29s"
Jan 13 18:11:18 host k3s[7262]: W0113 18:11:18.324813    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 18:11:18 host k3s[7262]: E0113 18:11:18.329167    7262 controller.go:146] "Unhandled Error" err=<
Jan 13 18:11:18 host k3s[7262]:         Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
Jan 13 18:11:18 host k3s[7262]:         , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
Jan 13 18:11:18 host k3s[7262]:  >
Jan 13 18:11:18 host k3s[7262]: E0113 18:11:18.331464    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
Jan 13 18:11:18 host k3s[7262]: I0113 18:11:18.334927    7262 handler.go:285] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
Jan 13 18:11:27 host k3s[7262]: time="2026-01-13T18:11:27+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [========================================>          ]   36.7MB/45.48MB"
Jan 13 18:11:28 host k3s[7262]: time="2026-01-13T18:11:28+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:11:27.214343993 +0100 CET m=+1739.368334409) (total time: 1.712657308s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=1.712657308s
Jan 13 18:11:29 host k3s[7262]: time="2026-01-13T18:11:29+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:11:27.244782038 +0100 CET m=+1739.398772474) (total time: 1.909924126s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=1.909924126s
Jan 13 18:11:31 host k3s[7262]: E0113 18:11:31.470533    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
Jan 13 18:11:33 host k3s[7262]: E0113 18:11:33.087882    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="6.108s"
Jan 13 18:11:34 host k3s[7262]: E0113 18:11:34.110415    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.022s"
Jan 13 18:11:37 host k3s[7262]: time="2026-01-13T18:11:37+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=========================================>         ]  37.75MB/45.48MB"
Jan 13 18:11:42 host k3s[7262]: time="2026-01-13T18:11:42+01:00" level=warning msg="Slow SQL (started: 2026-01-13 18:11:36.940337468 +0100 CET m=+1749.094328310) (total time: 5.292145854s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? ESCAPE '^' AND kv.id > ? ORDER BY kv.id ASC LIMIT 500" duration=5.292145854s
Jan 13 18:11:45 host k3s[7262]: W0113 18:11:45.313551    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 18:11:46 host k3s[7262]: E0113 18:11:45.389115    7262 controller.go:102] "Unhandled Error" err=<
Jan 13 18:11:46 host k3s[7262]:         loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
Jan 13 18:11:46 host k3s[7262]:         , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
Jan 13 18:11:46 host k3s[7262]:  >
Jan 13 18:11:46 host k3s[7262]: I0113 18:11:45.416467    7262 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
Jan 13 18:11:46 host k3s[7262]: time="2026-01-13T18:11:46+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:11:42.779840873 +0100 CET m=+1754.933831290) (total time: 3.461753544s): INSERT INTO kine(name, created, deleted, create_revision, prev_revision, lease, value, old_value) values(?, ?, ?, ?, ?, ?, ?, ?)" duration=3.461753544s
Jan 13 18:11:46 host k3s[7262]: time="2026-01-13T18:11:46+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=========================================>         ]  37.75MB/45.48MB"
Jan 13 18:11:47 host k3s[7262]: time="2026-01-13T18:11:47+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:11:43.096713496 +0100 CET m=+1755.250703974) (total time: 3.963244194s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=3.963244194s
Jan 13 18:11:47 host k3s[7262]: W0113 18:11:47.436337    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 18:11:47 host k3s[7262]: E0113 18:11:47.436420    7262 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService"
Jan 13 18:11:47 host k3s[7262]: I0113 18:11:47.436453    7262 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
Jan 13 18:11:47 host k3s[7262]: time="2026-01-13T18:11:47+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:11:42.665185504 +0100 CET m=+1754.819175943) (total time: 4.909747999s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=4.909747999s
Jan 13 18:11:47 host k3s[7262]: I0113 18:11:47.215311    7262 transport.go:356] "Warning: unable to cancel request" roundTripperType="*otelhttp.Transport"
Jan 13 18:11:47 host k3s[7262]: time="2026-01-13T18:11:47+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:11:42.880696568 +0100 CET m=+1755.034686996) (total time: 4.406028457s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC ) c" duration=4.406028457s
Jan 13 18:11:47 host k3s[7262]: time="2026-01-13T18:11:47+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:11:46.099311644 +0100 CET m=+1758.253302114) (total time: 1.56552257s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? ESCAPE '^' AND kv.id > ? ORDER BY kv.id ASC LIMIT 500" duration=1.56552257s
Jan 13 18:11:47 host k3s[7262]: E0113 18:11:47.793994    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 18:11:48 host k3s[7262]: I0113 18:11:48.060755    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 18:11:51 host k3s[7262]: E0113 18:11:51.596548    7262 health_controller.go:212] Metrics Controller heartbeat missed
Jan 13 18:11:51 host k3s[7262]: time="2026-01-13T18:11:51+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:11:48.055902647 +0100 CET m=+1760.209893079) (total time: 3.641883693s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=3.641883693s
Jan 13 18:11:52 host k3s[7262]: time="2026-01-13T18:11:52+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:11:51.737215599 +0100 CET m=+1763.891206036) (total time: 1.079449697s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=1.079449697s
Jan 13 18:11:53 host k3s[7262]: E0113 18:11:53.520473    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="18.718s"
Jan 13 18:11:54 host k3s[7262]: E0113 18:11:54.532074    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.011s"
Jan 13 18:11:57 host k3s[7262]: time="2026-01-13T18:11:57+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=========================================>         ]  37.75MB/45.48MB"
Jan 13 18:12:04 host k3s[7262]: E0113 18:12:04.180064    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="7.849s"
Jan 13 18:12:09 host k3s[7262]: E0113 18:12:09.867765    7262 health_controller.go:212] Metrics Controller heartbeat missed
Jan 13 18:12:09 host k3s[7262]: time="2026-01-13T18:12:09+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=========================================>         ]  37.75MB/45.48MB"
Jan 13 18:12:09 host k3s[7262]: time="2026-01-13T18:12:09+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:12:08.234948503 +0100 CET m=+1780.388939293) (total time: 1.702183366s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? ESCAPE '^' AND kv.id > ? ORDER BY kv.id ASC LIMIT 500" duration=1.702183366s
Jan 13 18:12:15 host k3s[7262]: time="2026-01-13T18:12:15+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:12:13.052618335 +0100 CET m=+1785.206608773) (total time: 2.825303211s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=2.825303211s
Jan 13 18:12:19 host k3s[7262]: E0113 18:12:19.123620    7262 controller.go:195] "Failed to update lease" err="Put \"https://127.0.0.1:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/host?timeout=10s\": context deadline exceeded"
Jan 13 18:12:19 host k3s[7262]: time="2026-01-13T18:12:18+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=========================================>         ]  37.75MB/45.48MB"
Jan 13 18:12:19 host k3s[7262]: time="2026-01-13T18:12:19+01:00" level=warning msg="Slow SQL (started: 2026-01-13 18:12:13.087859685 +0100 CET m=+1785.241850129) (total time: 6.165788834s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=6.165788834s
Jan 13 18:12:19 host k3s[7262]: I0113 18:12:19.286811    7262 transport.go:356] "Warning: unable to cancel request" roundTripperType="*otelhttp.Transport"
Jan 13 18:12:19 host k3s[7262]: {"level":"warn","ts":"2026-01-13T18:12:19.326382+0100","logger":"etcd-client","caller":"v3@v3.6.6-k3s1/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002995c20/kine.sock","method":"/etcdserverpb.KV/Txn","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
Jan 13 18:12:19 host k3s[7262]: time="2026-01-13T18:12:19+01:00" level=warning msg="Slow SQL (started: 2026-01-13 18:12:13.252303502 +0100 CET m=+1785.406294015) (total time: 5.997255706s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=5.997255706s
Jan 13 18:12:19 host k3s[7262]: E0113 18:12:19.326313    7262 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout"
Jan 13 18:12:19 host k3s[7262]: E0113 18:12:19.407252    7262 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout"
Jan 13 18:12:19 host k3s[7262]: E0113 18:12:19.408707    7262 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout"
Jan 13 18:12:19 host k3s[7262]: E0113 18:12:19.408902    7262 timeout.go:140] "Post-timeout activity" timeElapsed="81.227274ms" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/host" result=null
Jan 13 18:12:19 host k3s[7262]: E0113 18:12:19.410381    7262 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 84.025427ms, panicked: false, err: context canceled, panic-reason: <nil>"
Jan 13 18:12:19 host k3s[7262]: E0113 18:12:19.595658    7262 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"host\": the object has been modified; please apply your changes to the latest version and try again"
Jan 13 18:12:19 host k3s[7262]: E0113 18:12:19.605839    7262 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
Jan 13 18:12:19 host k3s[7262]: I0113 18:12:19.624700    7262 garbagecollector.go:787] "failed to discover some groups" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
Jan 13 18:12:20 host k3s[7262]: E0113 18:12:20.169289    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="13.253s"
Jan 13 18:12:21 host k3s[7262]: I0113 18:12:21.061967    7262 handler.go:285] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
Jan 13 18:12:28 host k3s[7262]: time="2026-01-13T18:12:28+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [==========================================>        ]   38.8MB/45.48MB"
Jan 13 18:12:29 host k3s[7262]: time="2026-01-13T18:12:29+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:12:28.156918118 +0100 CET m=+1800.310908704) (total time: 1.119739362s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=1.119739362s
Jan 13 18:12:29 host k3s[7262]: time="2026-01-13T18:12:29+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:12:28.120863697 +0100 CET m=+1800.274854256) (total time: 1.165424883s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? ESCAPE '^' AND kv.id > ? ORDER BY kv.id ASC LIMIT 500" duration=1.165424883s
Jan 13 18:12:30 host k3s[7262]: time="2026-01-13T18:12:30+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:12:28.131801425 +0100 CET m=+1800.285791896) (total time: 2.038452778s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=2.038452778s
Jan 13 18:12:31 host k3s[7262]: I0113 18:12:31.687626    7262 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.43.0.0/16
Jan 13 18:12:32 host k3s[7262]: E0113 18:12:32.168099    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="4.214s"
Jan 13 18:12:34 host k3s[7262]: time="2026-01-13T18:12:34+01:00" level=info msg="COMPACT compactRev=157 targetCompactRev=331 currentRev=1331"
Jan 13 18:12:34 host k3s[7262]: time="2026-01-13T18:12:34+01:00" level=info msg="COMPACT deleted 29 rows from 174 revisions in 4.701371ms - compacted to 331/1331"
Jan 13 18:12:34 host k3s[7262]: time="2026-01-13T18:12:34+01:00" level=info msg="COMPACT compacted from 157 to 331 in 1 transactions over 6ms"
Jan 13 18:12:36 host k3s[7262]: time="2026-01-13T18:12:36+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [==========================================>        ]   38.8MB/45.48MB"
Jan 13 18:12:46 host k3s[7262]: time="2026-01-13T18:12:46+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===========================================>       ]  39.85MB/45.48MB"
Jan 13 18:12:56 host k3s[7262]: time="2026-01-13T18:12:56+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===========================================>       ]  39.85MB/45.48MB"
Jan 13 18:13:06 host k3s[7262]: time="2026-01-13T18:13:06+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===========================================>       ]  39.85MB/45.48MB"
Jan 13 18:13:16 host k3s[7262]: time="2026-01-13T18:13:16+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [============================================>      ]  40.89MB/45.48MB"
Jan 13 18:13:26 host k3s[7262]: time="2026-01-13T18:13:26+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [============================================>      ]  40.89MB/45.48MB"
Jan 13 18:13:36 host k3s[7262]: time="2026-01-13T18:13:36+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [==============================================>    ]  41.94MB/45.48MB"
Jan 13 18:13:46 host k3s[7262]: time="2026-01-13T18:13:46+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [==============================================>    ]  41.94MB/45.48MB"
Jan 13 18:13:56 host k3s[7262]: time="2026-01-13T18:13:56+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===============================================>   ]  42.99MB/45.48MB"
Jan 13 18:14:04 host k3s[7262]: E0113 18:14:04.267436    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.931s"
Jan 13 18:14:06 host k3s[7262]: time="2026-01-13T18:14:06+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [===============================================>   ]  42.99MB/45.48MB"
Jan 13 18:14:06 host k3s[7262]: E0113 18:14:06.890029    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.553s"
Jan 13 18:14:08 host k3s[7262]: E0113 18:14:08.310256    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.419s"
Jan 13 18:14:16 host k3s[7262]: time="2026-01-13T18:14:16+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [================================================>  ]  44.04MB/45.48MB"
Jan 13 18:14:19 host k3s[7262]: E0113 18:14:19.451245    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.117s"
Jan 13 18:14:26 host k3s[7262]: time="2026-01-13T18:14:26+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [================================================>  ]  44.04MB/45.48MB"
Jan 13 18:14:36 host k3s[7262]: time="2026-01-13T18:14:36+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Downloading [=================================================> ]  45.09MB/45.48MB"
Jan 13 18:14:37 host k3s[7262]: E0113 18:14:37.592943    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.174s"
Jan 13 18:14:46 host k3s[7262]: time="2026-01-13T18:14:46+01:00" level=info msg="Pulling image rancher/mirrored-library-traefik:3.5.1: d5e6b58dcb29: Extracting 6 s"
Jan 13 18:14:51 host k3s[7262]: time="2026-01-13T18:14:51+01:00" level=info msg="Stop pulling image rancher/mirrored-library-traefik:3.5.1: Status: Downloaded newer image for rancher/mirrored-library-traefik:3.5.1"
Jan 13 18:14:55 host k3s[7262]: E0113 18:14:55.830220    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.491s"
Jan 13 18:14:58 host k3s[7262]: I0113 18:14:58.824436    7262 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/traefik-6f5f87584-6slgz" podStartSLOduration=9.122551997 podStartE2EDuration="18m9.824397623s" podCreationTimestamp="2026-01-13 17:56:49 +0100 CET" firstStartedPulling="2026-01-13 17:56:51.548984807 +0100 CET m=+863.702975216" lastFinishedPulling="2026-01-13 18:14:52.250830422 +0100 CET m=+1944.404820842" observedRunningTime="2026-01-13 18:14:58.818322911 +0100 CET m=+1950.972313341" watchObservedRunningTime="2026-01-13 18:14:58.824397623 +0100 CET m=+1950.978388046"
Jan 13 18:15:05 host k3s[7262]: E0113 18:15:05.397740    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.019s"
Jan 13 18:15:21 host k3s[7262]: E0113 18:15:21.406026    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.075s"
Jan 13 18:15:25 host k3s[7262]: E0113 18:15:25.437262    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.768s"
Jan 13 18:15:27 host k3s[7262]: E0113 18:15:27.554199    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.211s"
Jan 13 18:15:40 host k3s[7262]: E0113 18:15:40.961913    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="8.623s"
Jan 13 18:15:46 host k3s[7262]: time="2026-01-13T18:15:46+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:15:44.507370171 +0100 CET m=+1996.661360661) (total time: 1.898285326s): SELECT SUM(pgsize) FROM dbstat" duration=1.898285326s
Jan 13 18:15:46 host k3s[7262]: time="2026-01-13T18:15:46+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:15:44.491396782 +0100 CET m=+1996.645387214) (total time: 1.927622498s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=1.927622498s
Jan 13 18:15:47 host k3s[7262]: time="2026-01-13T18:15:47+01:00" level=info msg="Slow SQL (started: 2026-01-13 18:15:44.499490466 +0100 CET m=+1996.653480896) (total time: 2.685525851s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name AS thename, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? ESCAPE '^' AND mkv.name >= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.thename ASC LIMIT 1" duration=2.685525851s
Jan 13 18:15:48 host k3s[7262]: E0113 18:15:48.188350    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="7.226s"
Jan 13 18:15:51 host k3s[7262]: W0113 18:15:51.836511    7262 handler_proxy.go:99] no RequestInfo found in the context
Jan 13 18:15:51 host k3s[7262]: E0113 18:15:51.836670    7262 controller.go:146] "Unhandled Error" err=<
Jan 13 18:15:51 host k3s[7262]:         Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
Jan 13 18:15:51 host k3s[7262]:         , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
Jan 13 18:15:51 host k3s[7262]:  >
Jan 13 18:15:51 host k3s[7262]: E0113 18:15:51.836752    7262 handler_proxy.go:143] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
Jan 13 18:15:51 host k3s[7262]: I0113 18:15:51.836808    7262 handler.go:285] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
Jan 13 18:15:52 host k3s[7262]: E0113 18:15:52.563854    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.875s"
Jan 13 18:15:53 host k3s[7262]: E0113 18:15:53.976053    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.409s"
Jan 13 18:15:57 host k3s[7262]: E0113 18:15:57.158934    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.682s"
Jan 13 18:16:05 host k3s[7262]: I0113 18:16:05.402544    7262 handler.go:285] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
Jan 13 18:16:45 host k3s[7262]: E0113 18:16:45.651990    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.315s"
Jan 13 18:16:48 host k3s[7262]: E0113 18:16:48.373841    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.042s"
Jan 13 18:16:51 host k3s[7262]: E0113 18:16:51.439411    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="3.065s"
Jan 13 18:16:54 host k3s[7262]: E0113 18:16:54.193785    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.754s"
Jan 13 18:16:58 host k3s[7262]: E0113 18:16:58.765249    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.43s"
Jan 13 18:17:00 host k3s[7262]: E0113 18:17:00.781907    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.01s"
Jan 13 18:17:05 host k3s[7262]: E0113 18:17:05.985798    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="5.203s"
Jan 13 18:17:34 host k3s[7262]: time="2026-01-13T18:17:34+01:00" level=info msg="COMPACT compactRev=331 targetCompactRev=443 currentRev=1443"
Jan 13 18:17:34 host k3s[7262]: time="2026-01-13T18:17:34+01:00" level=info msg="COMPACT deleted 38 rows from 112 revisions in 33.148236ms - compacted to 443/1443"
Jan 13 18:17:34 host k3s[7262]: time="2026-01-13T18:17:34+01:00" level=info msg="COMPACT compacted from 331 to 443 in 1 transactions over 34ms"
Jan 13 18:17:35 host k3s[7262]: E0113 18:17:35.458480    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.127s"
Jan 13 18:17:56 host k3s[7262]: E0113 18:17:56.562532    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.222s"
Jan 13 18:18:01 host k3s[7262]: E0113 18:18:01.789974    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.458s"
Jan 13 18:18:31 host k3s[7262]: E0113 18:18:31.736912    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.405s"
Jan 13 18:18:33 host k3s[7262]: E0113 18:18:33.369256    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.034s"
Jan 13 18:18:37 host k3s[7262]: E0113 18:18:37.380091    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.048s"
Jan 13 18:18:47 host k3s[7262]: E0113 18:18:47.365256    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.034s"
Jan 13 18:18:59 host k3s[7262]: E0113 18:18:59.691112    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.358s"
Jan 13 18:19:31 host k3s[7262]: E0113 18:19:31.489479    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.156s"
Jan 13 18:19:36 host k3s[7262]: E0113 18:19:36.167409    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.801s"
Jan 13 18:19:37 host k3s[7262]: E0113 18:19:37.821459    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.49s"
Jan 13 18:19:56 host k3s[7262]: E0113 18:19:56.067106    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.728s"
Jan 13 18:20:03 host k3s[7262]: E0113 18:20:03.789762    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.456s"
Jan 13 18:20:35 host k3s[7262]: E0113 18:20:35.717579    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.386s"
Jan 13 18:20:57 host k3s[7262]: E0113 18:20:57.992241    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.66s"
Jan 13 18:22:05 host k3s[7262]: E0113 18:22:05.877836    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.546s"
Jan 13 18:22:07 host k3s[7262]: E0113 18:22:07.730550    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.397s"
Jan 13 18:22:31 host k3s[7262]: I0113 18:22:31.690192    7262 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.43.0.0/16
Jan 13 18:22:34 host k3s[7262]: time="2026-01-13T18:22:34+01:00" level=info msg="COMPACT compactRev=443 targetCompactRev=533 currentRev=1533"
Jan 13 18:22:34 host k3s[7262]: time="2026-01-13T18:22:34+01:00" level=info msg="COMPACT deleted 72 rows from 90 revisions in 18.642318ms - compacted to 533/1533"
Jan 13 18:22:34 host k3s[7262]: time="2026-01-13T18:22:34+01:00" level=info msg="COMPACT compacted from 443 to 533 in 1 transactions over 19ms"
Jan 13 18:22:40 host k3s[7262]: E0113 18:22:40.099059    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.767s"
Jan 13 18:23:01 host k3s[7262]: E0113 18:23:01.991216    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.629s"
Jan 13 18:23:04 host k3s[7262]: E0113 18:23:04.193989    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.861s"
Jan 13 18:23:35 host k3s[7262]: E0113 18:23:35.392848    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="3.008s"
Jan 13 18:23:39 host k3s[7262]: E0113 18:23:39.397897    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.065s"
Jan 13 18:23:50 host k3s[7262]: I0113 18:23:50.262909    7262 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vvlcg\" (UniqueName: \"kubernetes.io/projected/0452581a-d5dc-42ab-aaa9-17928d448959-kube-api-access-vvlcg\") pod \"debug\" (UID: \"0452581a-d5dc-42ab-aaa9-17928d448959\") " pod="default/debug"
Jan 13 18:23:51 host k3s[7262]: time="2026-01-13T18:23:51+01:00" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/aee5531103305827145dbb56956db8003b19d43a2871cd9584111e9975d51f1b/resolv.conf as [nameserver 10.43.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 13 18:23:51 host k3s[7262]: map[string]interface {}{"cniVersion":"1.0.0", "forceAddress":true, "hairpinMode":true, "ipMasq":false, "ipam":map[string]interface {}{"ranges":[][]map[string]interface {}{[]map[string]interface {}{map[string]interface {}{"subnet":"10.42.0.0/24"}}}, "routes":[]types.Route{types.Route{Dst:net.IPNet{IP:net.IP{0xa, 0x2a, 0x0, 0x0}, Mask:net.IPMask{0xff, 0xff, 0x0, 0x0}}, GW:net.IP(nil), MTU:0, AdvMSS:0, Priority:0, Table:(*int)(nil), Scope:(*int)(nil)}}, "type":"host-local"}, "isDefaultGateway":true, "isGateway":true, "mtu":(*uint)(0xc000102650), "name":"cbr0", "type":"bridge"}
Jan 13 18:23:51 host k3s[7262]: delegateAdd: netconf sent to delegate plugin:
Jan 13 18:24:07 host k3s[7262]: {"cniVersion":"1.0.0","forceAddress":true,"hairpinMode":true,"ipMasq":false,"ipam":{"ranges":[[{"subnet":"10.42.0.0/24"}]],"routes":[{"dst":"10.42.0.0/16"}],"type":"host-local"},"isDefaultGateway":true,"isGateway":true,"mtu":1450,"name":"cbr0","type":"bridge"}time="2026-01-13T18:24:07+01:00" level=info msg="Pulling image busybox:latest: 0f4360cf3c3e: Download complete "
Jan 13 18:24:17 host k3s[7262]: time="2026-01-13T18:24:17+01:00" level=info msg="Pulling image busybox:latest: 0f4360cf3c3e: Download complete "
Jan 13 18:24:27 host k3s[7262]: time="2026-01-13T18:24:27+01:00" level=info msg="Pulling image busybox:latest: e59838ecfec5: Downloading [=======================>                           ]  1.049MB/2.214MB"
Jan 13 18:24:37 host k3s[7262]: time="2026-01-13T18:24:37+01:00" level=info msg="Pulling image busybox:latest: e59838ecfec5: Downloading [=======================>                           ]  1.049MB/2.214MB"
Jan 13 18:24:42 host k3s[7262]: time="2026-01-13T18:24:42+01:00" level=info msg="Stop pulling image busybox:latest: Status: Downloaded newer image for busybox:latest"
Jan 13 18:24:44 host k3s[7262]: I0113 18:24:44.983066    7262 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/debug" podStartSLOduration=4.369601104 podStartE2EDuration="54.983027866s" podCreationTimestamp="2026-01-13 18:23:50 +0100 CET" firstStartedPulling="2026-01-13 18:23:52.066114732 +0100 CET m=+2484.220105132" lastFinishedPulling="2026-01-13 18:24:42.679541499 +0100 CET m=+2534.833531894" observedRunningTime="2026-01-13 18:24:44.98137993 +0100 CET m=+2537.135370352" watchObservedRunningTime="2026-01-13 18:24:44.983027866 +0100 CET m=+2537.137018292"
Jan 13 18:24:59 host k3s[7262]: E0113 18:24:59.597582    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.265s"
Jan 13 18:27:21 host k3s[7262]: E0113 18:27:21.455905    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.116s"
Jan 13 18:27:34 host k3s[7262]: time="2026-01-13T18:27:34+01:00" level=info msg="COMPACT compactRev=533 targetCompactRev=634 currentRev=1634"
Jan 13 18:27:34 host k3s[7262]: time="2026-01-13T18:27:34+01:00" level=info msg="COMPACT deleted 96 rows from 101 revisions in 13.016124ms - compacted to 634/1634"
Jan 13 18:27:34 host k3s[7262]: time="2026-01-13T18:27:34+01:00" level=info msg="COMPACT compacted from 533 to 634 in 1 transactions over 14ms"
Jan 13 18:29:29 host k3s[7262]: E0113 18:29:29.770246    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.401s"
Jan 13 18:29:39 host k3s[7262]: E0113 18:29:39.663841    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.33s"
Jan 13 18:29:51 host k3s[7262]: E0113 18:29:51.644276    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.312s"
Jan 13 18:29:56 host k3s[7262]: E0113 18:29:56.566966    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.193s"
Jan 13 18:30:11 host k3s[7262]: E0113 18:30:11.562308    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.23s"
Jan 13 18:30:40 host k3s[7262]: E0113 18:30:40.250118    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.919s"
Jan 13 18:30:49 host k3s[7262]: E0113 18:30:49.424275    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.093s"
Jan 13 18:30:56 host k3s[7262]: E0113 18:30:56.004629    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.659s"
Jan 13 18:31:05 host k3s[7262]: E0113 18:31:05.372426    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.037s"
Jan 13 18:31:07 host k3s[7262]: E0113 18:31:07.723469    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.389s"
Jan 13 18:32:17 host k3s[7262]: E0113 18:32:17.618977    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.287s"
Jan 13 18:32:31 host k3s[7262]: I0113 18:32:31.692898    7262 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.43.0.0/16
Jan 13 18:32:34 host k3s[7262]: time="2026-01-13T18:32:34+01:00" level=info msg="COMPACT compactRev=634 targetCompactRev=724 currentRev=1724"
Jan 13 18:32:34 host k3s[7262]: time="2026-01-13T18:32:34+01:00" level=info msg="COMPACT deleted 84 rows from 90 revisions in 24.785449ms - compacted to 724/1724"
Jan 13 18:32:34 host k3s[7262]: time="2026-01-13T18:32:34+01:00" level=info msg="COMPACT compacted from 634 to 724 in 1 transactions over 25ms"
Jan 13 18:33:34 host k3s[7262]: E0113 18:33:34.512435    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="4.178s"
Jan 13 18:36:25 host k3s[7262]: E0113 18:36:25.762931    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.421s"
Jan 13 18:37:19 host k3s[7262]: E0113 18:37:19.923951    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="5.577s"
Jan 13 18:37:27 host k3s[7262]: E0113 18:37:27.469780    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.138s"
Jan 13 18:37:33 host k3s[7262]: E0113 18:37:33.988682    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.657s"
Jan 13 18:37:34 host k3s[7262]: time="2026-01-13T18:37:34+01:00" level=info msg="COMPACT compactRev=724 targetCompactRev=814 currentRev=1814"
Jan 13 18:37:34 host k3s[7262]: time="2026-01-13T18:37:34+01:00" level=info msg="COMPACT deleted 57 rows from 90 revisions in 18.512708ms - compacted to 814/1814"
Jan 13 18:37:34 host k3s[7262]: time="2026-01-13T18:37:34+01:00" level=info msg="COMPACT compacted from 724 to 814 in 1 transactions over 19ms"
Jan 13 18:38:07 host k3s[7262]: E0113 18:38:07.768205    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.437s"
Jan 13 18:38:15 host k3s[7262]: E0113 18:38:15.344394    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.009s"
Jan 13 18:38:20 host k3s[7262]: E0113 18:38:20.905155    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.529s"
Jan 13 18:38:24 host k3s[7262]: E0113 18:38:24.566381    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.219s"
Jan 13 18:38:25 host k3s[7262]: E0113 18:38:25.856575    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.289s"
Jan 13 18:38:28 host k3s[7262]: E0113 18:38:28.340934    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.99s"
Jan 13 18:38:29 host k3s[7262]: E0113 18:38:29.626174    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.279s"
Jan 13 18:38:38 host k3s[7262]: E0113 18:38:38.334717    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="7.899s"
Jan 13 18:38:43 host k3s[7262]: E0113 18:38:43.735873    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.385s"
Jan 13 18:38:49 host k3s[7262]: E0113 18:38:49.902058    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="5.566s"
Jan 13 18:38:59 host k3s[7262]: E0113 18:38:59.158991    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="9.256s"
Jan 13 18:39:00 host k3s[7262]: E0113 18:39:00.405101    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.246s"
Jan 13 18:39:01 host k3s[7262]: E0113 18:39:01.767298    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.362s"
Jan 13 18:39:03 host k3s[7262]: E0113 18:39:03.491120    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.159s"
Jan 13 18:39:10 host k3s[7262]: E0113 18:39:10.621778    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.277s"
Jan 13 18:39:13 host k3s[7262]: E0113 18:39:13.840340    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.508s"
Jan 13 18:39:18 host k3s[7262]: E0113 18:39:18.796460    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="4.418s"
Jan 13 18:39:24 host k3s[7262]: E0113 18:39:24.519077    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.128s"
Jan 13 18:39:30 host k3s[7262]: E0113 18:39:30.122121    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="5.603s"
Jan 13 18:40:51 host k3s[7262]: E0113 18:40:51.906507    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.57s"
Jan 13 18:40:54 host k3s[7262]: E0113 18:40:54.415356    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.082s"
Jan 13 18:41:28 host k3s[7262]: E0113 18:41:28.927526    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.58s"
Jan 13 18:41:30 host k3s[7262]: E0113 18:41:30.046386    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.119s"
Jan 13 18:41:38 host k3s[7262]: E0113 18:41:38.792191    7262 kubelet.go:2618] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.422s"
Jan 13 18:42:31 host k3s[7262]: I0113 18:42:31.696056    7262 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.43.0.0/16
Jan 13 18:42:34 host k3s[7262]: time="2026-01-13T18:42:34+01:00" level=info msg="COMPACT compactRev=814 targetCompactRev=905 currentRev=1905"
Jan 13 18:42:35 host k3s[7262]: time="2026-01-13T18:42:35+01:00" level=info msg="COMPACT deleted 82 rows from 91 revisions in 415.473517ms - compacted to 905/1905"
Jan 13 18:42:35 host k3s[7262]: time="2026-01-13T18:42:35+01:00" level=info msg="COMPACT compacted from 814 to 905 in 1 transactions over 416ms"